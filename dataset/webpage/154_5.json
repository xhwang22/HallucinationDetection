{"Fei-Ping Hsu's research interests include computer graphics, computer vision, robotics, and medical imaging.": [{"url": "https://link.springer.com/article/10.1007/s41095-022-0271-y", "page_content": "Itti, L.; Koch, C.; Niebur, E. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence Vol. 20, No. 11, 1254\u20131259, 1998.\n\nHayhoe, M.; Ballard, D. Eye movements in natural behavior. Trends in Cognitive Sciences Vol. 9, No. 4, 188\u2013194, 2005\n\nRensink, R. A. The dynamic representation of scenes. Visual Cognition Vol. 7, Nos. 1\u20133, 17\u201342, 2000.\n\nCorbetta, M.; Shulman, G. L. Control of goal-directed and stimulus-driven attention in the brain. Nature Reviews Neuroscience Vol. 3, No. 3, 201\u2013215, 2002.\n\nHu, J.; Shen, L.; Albanie, S.; Sun, G.; Wu, E. H. Squeeze-and-excitation networks. IEEE Transactions on Pattern Analysis and Machine Intelligence Vol. 42, No. 8, 2011\u20132023, 2020.\n\nWoo, S.; Park, J.; Lee, J.; Kweon, I. S. CBAM: Convolutional block attention module. In: Computer Vision \u2014 ECCV 2018. Lecture Notes in Computer Science, Vol. 11211. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham, 3\u201319, 2018.\n\nDai, J. F.; Qi, H. Z.; Xiong, Y. W.; Li, Y.; Zhang, G. D.; Hu, H.; Wei, Y. Deformable convolutional networks. In: Proceedings of the IEEE International Conference on Computer Vision, 764\u2013773, 2017.\n\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; Zagoruyko, S. End-to-end object detection with transformers. In: Computer Vision \u2014 ECCV 2020. Lecture Notes in Computer Science, Vol. 12346. Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J. M. Eds. Springer Cham, 213\u2013229, 2020.\n\nYuan, Y.; Wang, J. OCNet: Object context network for scene parsing. arXiv preprint arXiv:1809.00916, 2018.\n\nFu, J.; Liu, J.; Tian, H. J.; Li, Y.; Bao, Y. J.; Fang, Z. W.; Lu, H. Dual attention network for scene segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3141\u20133149, 2019.\n\nYang, J. L.; Ren, P. R.; Zhang, D. Q.; Chen, D.; Wen, F.; Li, H. D.; Hua, G. Neural aggregation network for video face recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5216\u20135225, 2017.\n\nWang, Q. C.; Wu, T. Y.; Zheng, H.; Guo, G. D. Hierarchical pyramid diverse attention networks for face recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8323\u20138332, 2020.\n\nLi, W.; Zhu, X. T.; Gong, S. G. Harmonious attention network for person re-identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2285\u20132294, 2018.\n\nChen, B. H.; Deng, W. H.; Hu, J. N. Mixed high-order attention network for person re-identification. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 371\u2013381, 2019.\n\nWang, X. L.; Girshick, R.; Gupta, A.; He, K. M. Non-local neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7794\u20137803, 2018.\n\nDu, W. B.; Wang, Y. L.; Qiao, Y. Recurrent spatial-temporal attention network for action recognition in videos. IEEE Transactions on Image Processing Vol. 27, No. 3, 1347\u20131360, 2018.\n\nPeng, Y. X.; He, X. T.; Zhao, J. J. Object-part attention model for fine-grained image classification. IEEE Transactions on Image Processing Vol. 27, No. 3, 1487\u20131500, 2018.\n\nHe, P.; Huang, W. L.; He, T.; Zhu, Q. L.; Qiao, Y.; Li, X. L. Single shot text detector with regional attention. In: Proceedings of the IEEE International Conference on Computer Vision, 3066\u20133074, 2017.\n\nOktay, O.; Schlemper, J.; Folgoc, L. L.; Lee, M.; Heinrich, M.; Misawa, K.; Mori, K.; McDonagh, S.; Hammerla, N. Y.; Kainz, B.; et al. Attention U-Net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018.\n\nGuan, Q.; Huang, Y.; Zhong, Z.; Zheng, Z.; Zheng, L.; Yang, Y. Diagnose like a radiologist: Attention guided convolutional neural network for thorax disease classification. arXiv preprint arXiv:1801.09927, 2018.\n\nGregor, K.; Danihelka, I.; Graves, A.; Wierstra, D. DRAW: A recurrent neural network for image generation. In: Proceedings of the 32nd International Conference on Machine Learning, 1462\u20131471, 2015.\n\nZhang, H.; Goodfellow, I. J.; Metaxas, D. N.; Odena, A. Self-attention generative adversarial networks. In: Proceedings of the 36th International Conference on Machine Learning, 7354\u20137363, 2019.\n\nChu, X.; Yang, W.; Ouyang, W. L.; Ma, C.; Yuille, A. L.; Wang, X. G. Multi-context attention for human pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5669\u20135678, 2017.\n\nDai, T.; Cai, J. R.; Zhang, Y. B.; Xia, S. T.; Zhang, L. Second-order attention network for single image super-resolution. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11057\u201311066, 2019.\n\nZhang, Y. L.; Li, K. P.; Li, K.; Wang, L. C.; Zhong, B. N.; Fu, Y. Image super-resolution using very deep residual channel attention networks. In: Computer Vision \u2014 ECCV 2018. Lecture Notes in Computer Science, Vol. 11211. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham, 294\u2013310, 2018.\n\nXie, S. N.; Liu, S. N.; Chen, Z. Y.; Tu, Z. W. Attentional ShapeContextNet for point cloud recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4606\u20134615, 2018.\n\nGuo, M. H.; Cai, J. X.; Liu, Z. N.; Mu, T. J.; Martin, R. R.; Hu, S. M. PCT: Point cloud transformer. Computational Visual Media Vol. 7, No. 2, 187\u2013199, 2021.\n\nSu, W. J.; Zhu, X. Z.; Cao, Y.; Li, B.; Lu, L. W.; Wei, F. R.; Dai, J. L-BERT: Pre-training of generic visual-linguistic representations. In: Proceedings of the International Conference on Learning Representations, 2020.\n\nXu, T.; Zhang, P. C.; Huang, Q. Y.; Zhang, H.; Gan, Z.; Huang, X. L.; He, X. AttnGAN: Finegrained text to image generation with attentional generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1316\u20131324, 2018.\n\nWu, Y. X.; He, K. M. Group normalization. International Journal of Computer Vision Vol. 128, No. 3, 742\u2013755, 2020.\n\nMnih, V.; Heess, N.; Graves, A.; Kavukcuoglu, K. Recurrent models of visual attention. In: Proceedings of the 27th International Conference on Neural Information Processing Systems, Vol. 2, 2204\u20132212, 2014.\n\nJaderberg, M.; Simonyan, K.; Zisserman, A.; Kavukcuoglu, K. Spatial transformer networks. In: Proceedings of the 28th International Conference on Neural Information Processing Systems, Vol. 2, 2017\u20132025, 2015.\n\nVaswani, A.; Shazeer, N. M.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; Polosukhin, I. Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing System, 6000\u20136010, 2017.\n\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. An image is worth 16\u00d716 words: Transformers for image recognition at scale. In: Proceedings of the 9th International Conference on Learning Representations, 2021.\n\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhutdinov, R.;. Zemel, R.; Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention. In: Proceedings of the 32nd International Conference on Machine Learning, 2048\u20132057, 2015.\n\nZhu, X. Z.; Hu, H.; Lin, S.; Dai, J. F. Deformable ConvNets V2: More deformable, better results. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9300\u20139308, 2019.\n\nWang, Q. L.; Wu, B. G.; Zhu, P. F.; Li, P. H.; Zuo, W. M.; Hu, Q. H. ECA-net: Efficient channel attention for deep convolutional neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11531\u201311539, 2020.\n\nDevlin, J.; Chang, M. W.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nYang, Z. L.; Dai, Z. H.; Yang, Y. M.; Carbonell, J. G.; Salakhutdinov, R.; Le, Q. V. XLNet: Generalized autoregressive pretraining for language understanding. In: Proceedings of the 33rd Conference on Neural Information Processing Systems, 2019.\n\nLi, X.; Zhong, Z. S.; Wu, J. L.; Yang, Y. B.; Lin, Z. C.; Liu, H. Expectation-maximization attention networks for semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 9166\u20139175, 2019.\n\nHuang, Z. L.; Wang, X. G.; Huang, L. C.; Huang, C.; Wei, Y. C.; Liu, W. Y. CCNet: Criss-cross attention for semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence doi: https://doi.org/10.1109/TPAMI.2020.3007032, 2020.\n\nGeng, Z.; Guo, M.-H.; Chen, H.; Li, X.; Wei, K.; Lin, Z. Is attention better than matrix decomposition? In: Proceedings of the International Conference on Learning Representations, 2021.\n\nRamachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Levskaya, A.; Shlens, J. Stand-alone self-attention in vision models. In: Proceedings of the 33rd Conference on Neural Information Processing Systems, 2019.\n\nYuan, L.; Chen, Y.; Wang, T.; Yu, W.; Shi, Y.; Jiang, Z.-H.; Tay, F. E.; Feng, J.; Yan, S. Tokens-to-Token ViT: Training vision transformers from scratch on ImageNet. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 558\u2013567, 2021.\n\nWang, W. H.; Xie, E. Z.; Li, X.; Fan, D. P.; Song, K. T.; Liang, D.; Lu, T.; Luo, P.; Shao, L. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Visio, 568\u2013578, 2021.\n\nLiu, Z.; Lin, Y. T.; Cao, Y.; Hu, H.; Guo, B. N. Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 10012\u201310022, 2021.\n\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.; Zhang, L. CvT: Introducing convolutions to vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 22\u201331, 2021.\n\nYuan, L.; Hou, Q. B.; Jiang, Z. H.; Feng, J. S.; Yan, S. C. VOLO: Vision outlooker for visual recognition. arXiv preprint arXiv:2106.13112, 2021.\n\nDai, Z. H.; Liu, H. X.; Le, Q. V.; Tan, M. X. CoAtNet: Marrying convolution and attention for all data sizes. arXiv preprint arXiv:2106.04803, 2021.\n\nChen, L.; Zhang, H. W.; Xiao, J.; Nie, L. Q.; Shao, J.; Liu, W.; Chua, T. SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6298\u20136306, 2017.\n\nNair, V.; Hinton, G. E. Rectified linear units improve restricted Boltzmann machines. In: Proceedings of the 27th International Conference on Machine Learning, 807\u2013814, 2010.\n\nIoffe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: Proceedings of the 32nd International Conference on International Conference on Machine Learning, Vol. 37, 448\u2013456, 2015.\n\nZhang, H.; Dana, K.; Shi, J. P.; Zhang, Z. Y.; Wang, X. G.; Tyagi, A.; Agrawal, A. Context encoding for semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7151\u20137160, 2018.\n\nGao, Z. L.; Xie, J. T.; Wang, Q. L.; Li, P. H. Global second-order pooling convolutional networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3019\u20133028, 2019.\n\nLee, H.; Kim, H. E.; Nam, H. SRM: A style-based recalibration module for convolutional neural networks. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 1854\u20131862, 2019.\n\nYang, Z. X.; Zhu, L. C.; Wu, Y.; Yang, Y. Gated channel transformation for visual recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11791\u201311800, 2020.\n\nQin, Z. Q.; Zhang, P. Y.; Wu, F.; Li, X. FcaNet: Frequency channel attention networks. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 783\u2013792, 2021.\n\nDiba, A. L.; Fayyaz, M.; Sharma, V.; Arzani, M. M.; Yousefzadeh, R.; Gall, J.; van Gool, L. Spatiotemporal channel correlation networks for action classification. In: Computer Vision - ECCV 2018. Lecture Notes in Computer Science, Vol. 11208. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springe Cham, 299\u2013315, 2018.\n\nChen, Z. R.; Li, Y.; Bengio, S.; Si, S. You look twice: GaterNet for dynamic filter selection in CNNs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9164\u20139172, 2019.\n\nShi, H. Y.; Lin, G. S.; Wang, H.; Hung, T. Y.; Wang, Z. H. SpSequenceNet: Semantic segmentation network on 4D point clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4573\u20134582, 2020.\n\nHu, J.; Shen, L.; Albanie, S.; Sun, G.; Vedaldi, A. Gather-excite: Exploiting feature context in convolutional neural networks. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems, 9423\u20139433, 2018.\n\nYan, X.; Zheng, C. D.; Li, Z.; Wang, S.; Cui, S. G. PointASNL: Robust point clouds processing using nonlocal neural networks with adaptive sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5588\u20135597, 2020.\n\nHu, H.; Gu, J. Y.; Zhang, Z.; Dai, J. F.; Wei, Y. C. Relation networks for object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3588\u20133597, 2018.\n\nZhang, H.; Zhang, H.; Wang, C. G.; Xie, J. Y. Co-occurrent features in semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 548\u2013557, 2019.\n\nBello, I.; Zoph, B.; Le, Q.; Vaswani, A.; Shlens, J. Attention augmented convolutional networks. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 3285\u20133294, 2019.\n\nZhu, X. Z.; Cheng, D. Z.; Zhang, Z.; Lin, S.; Dai, J. F. An empirical study of spatial attention mechanisms in deep networks. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 6687\u20136696, 2019.\n\nLi, X.; Yang, Y. B.; Zhao, Q. J.; Shen, T. C.; Lin, Z. C.; Liu, H. Spatial pyramid based graph reasoning for semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8947\u20138956, 2020.\n\nZhu, Z.; Xu, M. D.; Bai, S.; Huang, T. T.; Bai, X. Asymmetric non-local neural networks for semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 593\u2013602, 2019.\n\nCao, Y.; Xu, J. R.; Lin, S.; Wei, F. Y.; Hu, H. GCNet: Non-local networks meet squeeze-excitation networks and beyond. In: Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop, 1971\u20131980, 2019.\n\nChen, Y.; Kalantidis, Y.; Li, J.; Yan, S.; Feng, J. A2-nets: Double attention networks. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems, 350\u2013359, 2018.\n\nChen, Y. P.; Rohrbach, M.; Yan, Z. C.; Yan, S. C.; Feng, J. S.; Kalantidis, Y. Graph-based global reasoning networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 433\u2013442, 2019.\n\nZhang, S. Y.; Yan, S. P.; He, X. M. LatentGNN: Learning efficient non-local relations for visual recognition. In: Proceedings of the 36th International Conference on Machine Learning, 7374\u20137383, 2019.\n\nYuan, Y.; Chen, X.; Chen, X.; Wang, J. Segmentation transformer: Object-contextual representations for semantic segmentation. arXiv preprint arXiv: 1909.11065, 2019.\n\nYin, M. H.; Yao, Z. L.; Cao, Y.; Li, X.; Zhang, Z.; Lin, S.; Hu, H. Disentangled non-local neural networks. In: Computer Vision \u2014 ECCV 2020. Lecture Notes in Computer Science, Vol. 12360. Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J. M. Eds. Springer Cham, 191\u2013207, 2020.\n\nGuo, M. H.; Liu, Z. N.; Mu, T. J.; Hu, S. M. Beyond self-attention: External attention using two linear layers for visual tasks. arXiv preprint arXiv:2105.02358, 2021.\n\nHu, H.; Zhang, Z.; Xie, Z. D.; Lin, S. Local relation networks for image recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 3463\u20133472, 2019.\n\nZhao, H. S.; Jia, J. Y.; Koltun, V. Exploring self-attention for image recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10073\u201310082, 2020.\n\nChen, M.; Radford, A.; Child, R.; Wu, J.; Jun, H.; Luan, D.; Sutskever, I. Generative pretraining from pixels. In: Proceedings of the 37th International Conference on Machine Learning, 1691\u20131703, 2020.\n\nChen, H. T.; Wang, Y. H.; Guo, T. Y.; Xu, C.; Deng, Y. P.; Liu, Z. H.; Ma, S.; Xu, C.; Xu, C.; Gao, W. Pretrained image processing transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12294\u201312305, 2021.\n\nZhao, H.; Jiang, L.; Jia, J.; Torr, P.; Koltun, V. Point transformer. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 16259\u201316268, 2021.\n\nZheng, S. X.; Lu, J. C.; Zhao, H. S.; Zhu, X. T.; Luo, Z. K.; Wang, Y. B.; Fu, Y.; Feng, J.; Xiang, T.; Torr, P. H.; et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6877\u20136886, 2021.\n\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; Wang, Y. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.\n\nLiu, S. L.; Zhang, L.; Yang, X.; Su, H.; Zhu, J. Query2Label: A simple transformer way to multilabel classification. arXiv preprint arXiv:2107.10834, 2021.\n\nChen, X. L.; Xie, S. N.; He, K. M. An empirical study of training self-supervised visual transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 9640\u20139649, 2021.\n\nBao, H. B.; Dong, L.; Wei, F. R. BEiT: BERT pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.\n\nXie, E. Z.; Wang, W. H.; Yu, Z. D.; Anandkumar, A.; \u00c1lvarez, J.; Luo, P. SegFormer: Simple and efficient design for semantic segmentation with transformers. arXiv preprint arXiv:2105.15203, 2021.\n\nZhao, H.; Zhang, Y.; Liu, S.; Shi, J.; Loy, C. C.; Lin, D.; Jia, J. PSANet: Point-wise spatial attention network for scene parsing. In: Computer Vision \u2014 ECCV 2018. Lecture Notes in Computer Science, Vol. 11213. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham, 270\u2013286, 2018.\n\nBa, J.; Mnih, V.; Kavukcuoglu, K. Multiple object recognition with visual attention. arXiv preprint arXiv:1412.7755, 2014.\n\nSharma, S.; Kiros, R.; Salakhutdinov, R. Action recognition using visual attention. arXiv preprint arXiv:1511.04119, 2015.\n\nGirdhar, R.; Ramanan, D. Attentional pooling for action recognition. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, 33\u201344, 2017.\n\nLi, Z. Y.; Gavrilyuk, K.; Gavves, E.; Jain, M.; Snoek, C. G. M. VideoLSTM convolves, attends and flows for action recognition. Computer Vision and Image Understanding Vol. 166, 41\u201350, 2018.\n\nYue, K. Y.; Sun, M.; Yuan, Y. C.; Zhou, F.; Ding, E. R.; Xu, F. X. Compact generalized non-local network. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems, 6511\u20136520, 2018.\n\nLiu, X. H.; Han, Z. Z.; Wen, X.; Liu, Y. S.; Zwicker, M. L2G auto-encoder: Understanding point clouds by local-to-global reconstruction with hierarchical self-attention. In: Proceedings of the 27th ACM International Conference on Multimedia, 989\u2013997, 2019.\n\nPaigwar, A.; Erkent, O.; Wolf, C.; Laugier, C. Attentional PointNet for 3D-object detection in point clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 1297\u20131306, 2019.\n\nWen, X.; Han, Z. Z.; Youk, G.; Liu, Y. S. CF-SIS: Semantic-instance segmentation of 3D point clouds by context fusion with self-attention. In: Proceedings of the 28th ACM International Conference on Multimedia, 1661\u20131669, 2020.\n\nYang, J. C.; Zhang, Q.; Ni, B. B.; Li, L. G.; Liu, J. X.; Zhou, M. D.; Tian, Q. Modeling point clouds with self-attention and gumbel subset sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3318\u20133327, 2019.\n\nXu, J.; Zhao, R.; Zhu, F.; Wang, H. M.; Ouyang, W. L. Attention-aware compositional network for person re-identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2119\u20132128, 2018.\n\nLiu, H.; Feng, J. S.; Qi, M. B.; Jiang, J. G.; Yan, S. C. End-to-end comparative attention networks for person re-identification. IEEE Transactions on Image Processing Vol. 26, No. 7, 3492\u20133506, 2017.\n\nZheng, Z. D.; Zheng, L.; Yang, Y. Pedestrian alignment network for large-scale person re-identification. IEEE Transactions on Circuits and Systems for Video Technology Vol. 29, No. 10, 3037\u20133045, 2019.\n\nLi, K. P.; Wu, Z. Y.; Peng, K. C.; Ernst, J.; Fu, Y. Tell me where to look: Guided attention inference network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9215\u20139223, 2018.\n\nZhang, Z. Z.; Lan, C. L.; Zeng, W. J.; Jin, X.; Chen, Z. B. Relation-aware global attention for person re-identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3183\u20133192, 2020.\n\nZhao, B.; Wu, X.; Feng, J. S.; Peng, Q.; Yan, S. C. Diversified visual attention networks for fine-grained object classification. IEEE Transactions on Multimedia Vol. 19, No. 6, 1245\u20131256, 2017.\n\nBryan, B.; Gong, Y.; Zhang, Y. Z.; Poellabauer, C. Second-order non-local attention networks for person re-identification. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 3759\u20133768, 2019.\n\nZheng, H. L.; Fu, J. L.; Mei, T.; Luo, J. B. Learning multi-attention convolutional neural network for finegrained image recognition. In: Proceedings of the IEEE International Conference on Computer Vision, 5219\u20135227, 2017.\n\nFu, J. L.; Zheng, H. L.; Mei, T. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4476\u20134484, 2017.\n\nLiu, S.; Li, F.; Zhang, H.; Yang, X.; Qi, X.; Su, H.; Zhu, J.; Zhang, L. DAB-DETR: Dynamic anchor boxes are better queries for DETR. arXiv preprint arXiv:2201.12329, 2022.\n\nYang, G. Y.; Li, X. L.; Martin, R.; Hu, S. M. Sampling equivariant self-attention networks for object detection in aerial images. arXiv preprint arXiv:2111.03420, 2021.\n\nZheng, H. L.; Fu, J. L.; Zha, Z. J.; Luo, J. B. Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5007\u20135016, 2019.\n\nLee, J.; Lee, Y.; Kim, J.; Kosiorek, A. R.; Choi, S.; Teh Y. W. Set transformer: A framework for attention-based permutation-invariant neural networks. In: Proceedings of the 36th International Conference on Machine Learning, 3744\u20133753, 2019.\n\nXu, S. J.; Cheng, Y.; Gu, K.; Yang, Y.; Chang, S. Y.; Zhou, P. Jointly attentive spatial-temporal pooling networks for video-based person re-identification. In: Proceedings of the IEEE International Conference on Computer Vision, 4743\u20134752, 2017.\n\nZhang, R. M.; Li, J. Y.; Sun, H. B.; Ge, Y. Y.; Luo, P.; Wang, X. G.; Lin, L. SCAN: Self-and-collaborative attention network for video person re-identification. IEEE Transactions on Image Processing Vol. 28, No. 10, 4870\u20134882, 2019.\n\nChen, D. P.; Li, H. S.; Xiao, T.; Yi, S.; Wang, X. G. Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1169\u20131178, 2018.\n\nSrivastava, R. K.; Greff, K.; Schmidhuber, J. Training very deep networks. In: Proceedings of the 28th International Conference on Neural Information Processing Systems, Vol. 2, 2377\u20132385, 2015.\n\nLi, X.; Wang, W. H.; Hu, X. L.; Yang, J. Selective kernel networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 510\u2013519, 2019.\n\nZhang, H.; Wu, C.; Zhang, Z.; Zhu, Y.; Lin, H.; Zhang, Z.; Sun, Y.; He, T.; Mueller, J.; Manmatha, R.; et al. ResNeSt: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020.\n\nChen, Y. P.; Dai, X. Y.; Liu, M. C.; Chen, D. D.; Yuan, L.; Liu, Z. C. Dynamic convolution: attention over convolution kernels. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11027\u201311036, 2020.\n\nPark, J.; Woo, S.; Lee, J.-Y.; Kweon, I. S. BAM: Bottleneck attention module. arXiv preprint arXiv:1807.06514, 2018.\n\nYang, L.; Zhang, R.-Y.; Li, L.; Xie, X. SimAM: A simple, parameter-free attention module for convolutional neural networks. In: Proceedings of the 38th International Conference on Machine Learning, 11863\u201311874, 2021.\n\nWang, F.; Jiang, M. Q.; Qian, C.; Yang, S.; Li, C.; Zhang, H. G.; Wang, X.; Tang, X. Residual attention network for image classification. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6450\u20136458, 2017.\n\nGuo, M.-H.; Lu, C.-Z.; Liu, Z.-N.; Cheng, M.-M.; Hu, S.-M. Visual attention network. arXiv preprint arXiv:2202.09741, 2022.\n\nLiu, J. J.; Hou, Q. B.; Cheng, M. M.; Wang, C. H.; Feng, J. S. Improving convolutional networks with self-calibrated convolutions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10093\u201310102, 2020.\n\nMisra, D.; Nalamada, T.; Arasanipalai, A. U.; Hou, Q. B. Rotate to attend: Convolutional triplet attention module. In: Proceedings of the IEEE Winter Conference on Applications of Computer Vision, 3138\u20133147, 2021.\n\nLinsley,.; Shiebler, D.; Eberhardt, S.; Serre, T. Learning what and where to attend. In: Proceedings of the 7th International Conference on Learning Representations, 2019.\n\nRoy, A. G.; Navab, N.; Wachinger, C. Recalibrating fully convolutional networks with spatial and channel \u201csqueeze and excitation\u201d blocks. IEEE Transactions on Medical Imaging Vol. 38, No. 2, 540\u2013549, 2019.\n\nHou, Q. B.; Zhang, L.; Cheng, M. M.; Feng, J. S. Strip pooling: Rethinking spatial pooling for scene parsing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4002\u20134011, 2020.\n\nYou, H. X.; Feng, Y. F.; Ji, R. R.; Gao, Y. PVNet: A joint convolutional network of point cloud and multiview for 3D shape recognition. In: Proceedings of the 26th ACM International Conference on Multimedia, 1310\u20131318, 2018.\n\nXie, Q.; Lai, Y. K.; Wu, J.; Wang, Z. T.; Zhang, Y. M.; Xu, K.; Wang, J. MLCVNet: Multi-level context VoteNet for 3D object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10444\u201310453, 2020.\n\nWang, C.; Zhang, Q.; Huang, C.; Liu, W.; Wang, X. Mancs: A multi-task attentional network with curriculum sampling for person re-identification. In: Computer Vision \u2014 ECCV 2018. Lecture Notes in Computer Science, Vol. 11208. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham, 384\u2013400, 2018.\n\nChen, T. L.; Ding, S. J.; Xie, J. Y.; Yuan, Y.; Chen, W. Y.; Yang, Y.; Ren, Z.; Wang, Z. ABD-net: Attentive but diverse person re-identification. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 8350\u20138360, 2019.\n\nHou, Q. B.; Zhou, D. Q.; Feng, J. S. Coordinate attention for efficient mobile network design. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13708\u201313717, 2021.\n\nSong, S.; Lan, C.; Xing, J.; Zeng, W.; Liu, J. An end-to-end spatio-temporal attention model for human action recognition from skeleton data. In: Proceedings of the 31st AAAI Conference on Artificial Intelligence, 4263\u20134270, 2017.\n\nFu, Y.; Wang, X. Y.; Wei, Y. C.; Huang, T. STA: Spatial-temporal attention for large-scale video-based person re-identification. Proceedings of the AAAI Conference on Artificial Intelligence Vol. 33, 8287\u20138294, 2019.\n\nGao, L. L.; Li, X. P.; Song, J. K.; Shen, H. T. Hierarchical LSTMs with adaptive attention for visual captioning. IEEE Transactions on Pattern Analysis and Machine Intelligence Vol. 42, No. 5, 1112\u20131131, 2020.\n\nYan, C. G.; Tu, Y. B.; Wang, X. Z.; Zhang, Y. B.; Hao, X. H.; Zhang, Y. D.; Dai, Q. STAT: Spatial-temporal attention mechanism for video captioning. IEEE Transactions on Multimedia Vol. 22, No. 1, 229\u2013241, 2020.\n\nMeng, L. L.; Zhao, B.; Chang, B.; Huang, G.; Sun, W.; Tung, F.; Sigal, L. Interpretable spatiotemporal attention for video action recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop, 1513\u20131522, 2019.\n\nHe, B.; Yang, X. T.; Wu, Z. X.; Chen, H.; Shrivastava, A. GTA: Global temporal attention for video action understanding. arXiv preprint arXiv:2012.08510, 2020.\n\nLi, S.; Bak, S.; Carr, P.; Wang, X. G. Diversity regularized spatiotemporal attention for video-based person re-identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 369\u2013378, 2018.\n\nZhang, Z. Z.; Lan, C. L.; Zeng, W. J.; Chen, Z. B. Multi-granularity reference-aided attentive feature aggregation for video-based person re-identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10404\u201310413, 2020.\n\nHo, H. I.; Kim, J.; Wee, D. READ: Reciprocal attention discriminator for image-to-video re-identification. In: Computer Vision - ECCV 2020. Lecture Notes in Computer Science, Vol. 12359. Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J. M. Eds. Springer Cham, 335\u2013350, 2020.\n\nLiu, R.; Deng, H. M.; Huang, Y. Y.; Shi, X. Y.; Li, H. S. Decoupled spatial-temporal transformer for video inpainting. arXiv preprint arXiv:2104.06637, 2021.\n\nChaudhari, S.; Mithal, V.; Polatkan, G.; Ramanath, R. An attentive survey of attention models. ACM Transactions on Intelligent Systems and Technology Vol. 12, No. 5, Article No. 53, 2021.\n\nXu, Y. F.; Wei, H. P.; Lin, M. X.; Deng, Y. Y.; Sheng, K. K.; Zhang, M. D.; Tang, F.; Dong, W.; Huang, F.; Xu, C. Transformers in computational visual media: A survey. Computational Visual Media Vol. 8, No. 1, 33\u201362, 2022.\n\nHan, K.; Wang, Y.; Chen, H.; Chen, X.; Guo, J.; Liu, Z.; Tang, Y.; Xiao, A.; Xu, C.; Xu, Y.; et al. A survey on visual transformer. arXiv preprint arXiv:2012.12556, 2020.\n\nKhan, S.; Naseer, M.; Hayat, M.; Zamir, S. W.; Khan, F. S.; Shah, M. Transformers in vision: A survey. ACM Computing Surveyshttps://doi.org/10.1145/3505244, 2022.\n\nWang, F.; Tax, D. M. J. Survey on the attention based RNN model and its applications in computer vision. arXiv preprint arXiv:1601.06823, 2016.\n\nHe, K. M.; Zhang, X. Y.; Ren, S. Q.; Sun, J. Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770\u2013778, 2016.\n\nFang, P. F.; Zhou, J. M.; Roy, S.; Petersson, L.; Harandi, M. Bilinear attention networks for person retrieval. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 8029\u20138038, 2019.\n\nHochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Computation Vol. 9, No. 8, 1735\u20131780, 1997.\n\nSutton, R. S.; McAllester, D. A.; Singh, S. P.; Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In: Proceedings of the 12th International Conference on Neural Information Processing Systems, 1057\u20131063, 1999.\n\nBahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n\nLin, Z. H.; Feng, M. W.; Santos, C. N. D.; Yu, M.; Bengio, Y. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n\nDai, Z. H.; Yang, Z. L.; Yang, Y. M.; Carbonell, J.; Le, Q.; Salakhutdinov, R. Transformer-XL: Attentive language models beyond a fixed-length context. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2978\u20132988, 2019.\n\nChoromanski, K.; Likhosherstov, V.; Dohan, D.; Song, X. Y.; Gane, A.; Sarlos, T.; Hawkins, P.; Davis, J.; Mohiuddin, A.; Kaiser, L.; et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.\n\nZhu, X. Z.; Su, W. J.; Lu, L. W.; Li, B.; Wang, X. G.; Dai, J. F. Deformable DETR: Deformable transformers for end-to-end object detection. In: Proceedings of the International Conference on Learning Representations, 2021.\n\nLiu, W.; Rabinovich, A.; Berg, A. C. ParseNet: Looking wider to see better. arXiv preprint arXiv:1506.04579, 2015.\n\nPeng, C.; Zhang, X. Y.; Yu, G.; Luo, G. M.; Sun, J. Large kernel matters\u2014Improve semantic segmentation by global convolutional network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1743\u20131751, 2017.\n\nZhao, H. S.; Shi, J. P.; Qi, X. J.; Wang, X. G.; Jia, J. Y. Pyramid scene parsing network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6230\u20136239, 2017.\n\nHe, K. M.; Zhang, X. Y.; Ren, S. Q.; Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. In: Computer Vision \u2014 ECCV 2014. Lecture Notes in Computer Science, Vol. 8691. Fleet, D.; Pajdla, T.; Schiele, B.; Tuytelaars, T. Eds. Springer Cham, 346\u2013361, 2014.\n\nTolstikhin, I.; Houlsby, N.; Kolesnikov, A.; Beyer, L.; Zhai, X. H.; Unterthiner, T.; Yung, J.; Steiner, A.; Keysers, D.; Uszkoreit, J.; et al. MLP-mixer: An all-MLP architecture for vision. In: Proceedings of the 35th Conference on Neural Information Processing Systems, 2021.\n\nTouvron, H.; Bojanowski, P.; Caron, M.; Cord, M.; El-Nouby, A.; Grave, E.; Izacard, G.; Joulin, A.; Synnaeve, G.; Verbeek, J.; et al. ResMLP: Feedforward networks for image classification with data-efficient training. arXiv preprint arXiv: 2105.03404, 2021.\n\nShaw, P.; Uszkoreit, J.; Vaswani, A. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.\n\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. Language models are few-shot learners. In: Proceedings of the 34th Conference on Neural Information Processing Systems, 2020.\n\nBa, J. L.; Kiros, J. R.; Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\nHendrycks, D.; Gimpel, K. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016.\n\nSun, C.; Shrivastava, A.; Singh, S.; Gupta, A. Revisiting unreasonable effectiveness of data in deep learning era. In: Proceedings of the IEEE International Conference on Computer Vision, 843\u2013852, 2017.\n\nDeng, J.; Dong, W.; Socher, R.; Li, L. J.; Kai, L.; Li, F. F. ImageNet: A large-scale hierarchical image database. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 248\u2013255, 2009.\n\nZhou, D. Q.; Kang, B. Y.; Jin, X. J.; Yang, L. J.; Lian, X. C.; Jiang, Z. H.; Hou, Q. B.; Feng, J. S. DeepViT: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; J\u00e9gou, H. Going deeper with image transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 32\u201342, 2021.\n\nLiu, R.; Deng, H. M.; Huang, Y. Y.; Shi, X. Y.; Lu, L. W.; Sun, W. X.; Wang, X.; Dai, J.; Li, H. FuseFormer: Fusing fine-grained information in transformers for video inpainting. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 14040\u201314049, 2021.\n\nHe, K. M.; Chen, X. L.; Xie, S. N.; Li, Y. H.; Doll\u00e1r, P.; Girshick, R. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\n\nGuo, M. H.; Liu, Z. N.; Mu, T. J.; Liang, D.; Martin, R. R.; Hu, S. M. Can attention enable MLPs to catch up with CNNs? Computational Visual Media Vol. 7, No. 3, 283\u2013288, 2021.\n\nLi, J. N.; Zhang, S. L.; Wang, J. D.; Gao, W.; Tian, Q. Global-local temporal representations for video person re-identification. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 3957\u20133966, 2019.\n\nLiu, Z. Y.; Wang, L. M.; Wu, W.; Qian, C.; Lu, T. TAM: Temporal adaptive module for video recognition. arXiv preprint arXiv:2005.06803, 2020.\n\nYang, B.; Bender, G.; Le, Q. V.; Ngiam, J. CondConv: Conditionally parameterized convolutions for efficient inference. In: Proceedings of the 33rd International Conference on Neural Information Processing Systems, Article No. 117, 1307\u20131318, 2019.\n\nSpillmann, L.; Dresp-Langley, B.; Tseng, C. H. Beyond the classical receptive field: The effect of contextual stimuli. Journal of Vision Vol. 15, No. 9, 7, 2015.\n\nXie, S. N.; Girshick, R.; Doll\u00e1r, P.; Tu, Z. W.; He, K. M. Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5987\u20135995, 2017.\n\nWebb, B. S.; Dhruv, N. T.; Solomon, S. G.; Tailby, C.; Lennie, P. Early and late mechanisms of surround suppression in striate cortex of macaque. Journal of Neuroscience Vol. 25, No. 50, 11666\u201311675, 2005.\n\nYang, J. R.; Zheng, W. S.; Yang, Q. Z.; Chen, Y. C.; Tian, Q. Spatial-temporal graph convolutional network for video-based person re-identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3286\u20133296, 2020.\n\nSzegedy, C.; Liu, W.; Jia, Y. Q.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going deeper with convolutions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1\u20139, 2015.\n\nCaron, M.; Touvron, H.; Misra, I.; J\u00e9gou, H.; Mairal, J.; Bojanowski, P.; Joulin, A. Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 9650\u20139660, 2021.\n\nQian, N. On the momentum term in gradient descent learning algorithms. Neural Networks Vol. 12, No. 1, 145\u2013151, 1999.\n\nKingma, D. P.; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nLoshchilov, I.; Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nChen, X. N.; Hsieh, C. J.; Gong, B. Q. When vision transformers outperform ResNets without pretraining or strong data augmentations. arXiv preprint arXiv:2106.01548, 2021."}, {"url": "https://www.nature.com/articles/s41746-020-00376-2", "page_content": "As medical AI advances into the clinic144, it will simultaneously have the power to do great good for society, and to potentially exacerbate long-standing inequalities and perpetuate errors in medicine. If done properly and ethically, medical AI can become a flywheel for more equitable care\u2014the more it is used, the more data it acquires, the more accurate and general it becomes. The key is in understanding the data that the models are built on and the environment in which they are deployed. Here, we present four key considerations when applying ML technologies in healthcare: assessment of data, planning for model limitations, community participation, and trust building.\n\nData quality largely determines model quality; identifying inequities in the data and taking them into account will lead towards more equitable healthcare. Procuring the right datasets may depend on running human-in-the-loop programs or broad-reaching data collection techniques. There are a number of methods that aim to remove bias in data. Individual-level bias can be addressed via expert discussion145 and labeling adjudication146. Population-level bias can be addressed via missing data supplements and distributional shifts. International multi-institutional evaluation is a robust method to determine generalizability of models across diverse populations, medical equipment, resource settings, and practice patterns. In addition, using multi-task learning147 to train models to perform a variety of tasks rather than one narrowly defined task, such as multi-cancer detection from histopathology images148, makes them more generally useful and often more robust.\n\nTransparent reporting can reveal potential weaknesses and help address model limitations. Guardrails to protect against possible worst-case scenarios\u2014minority, dismissal, or automation bias\u2014must be put in place. It is insufficient to report and be satisfied with strong performance measures on general datasets when delivering care for patients\u2014there should be an understanding of the specific instances in which the model fails. One technique is to assess demographic performance in combination with saliency maps149, to visualize what the model pays attention to, and check for potential biases. For instance, when using deep learning to develop a differential diagnosis for skin diseases95, researchers examined the model performance based on Fitzpatrick skin types and other demographic information to determine patient types for which there were insufficient examples, and inform future data collection. Further, they used saliency masks to verify the model was informed by skin abnormalities and not skin type. See Fig. 4.\n\nFig. 4: Bias in deployment. a Example graphic of biased training data in dermatology. AIs trained primarily on lighter skin tones may not generalize as well when tested on darker skin157. Models require diverse training datasets for maximal generalizability (e.g.95). b Gradient Masks project the model\u2019s attention onto the original input image, allowing practitioners to visually confirm regions that most influence predictions. Panel was reproduced from ref. 95 with permission. Full size image\n\nA known limitation of ML is its performance on out-of-distribution data\u2013data samples that are unlike any seen during model training. Progress has been made on out-of-distribution detection150 and developing confidence intervals to help detect anomalies. Additionally, methods are developing to understand the uncertainty151 around model outputs. This is especially critical when implementing patient-specific predictions that impact safety.\n\nCommunity participation\u2014from patients, physicians, computer scientists, and other relevant stakeholders\u2014is paramount to successful deployment. This has helped identify structural drivers of racial bias in health diagnostics\u2014particularly in discovering bias in datasets and identifying demographics for which models fail152. User-centered evaluations are a valuable tool in ensuring a system\u2019s usability and fit into the real world. What\u2019s the best way to present a model\u2019s output to facilitate clinical decision making? How should a mobile app system be deployed in resource-constrained environments, such as areas with intermittent connectivity? For example, when launching ML-powered diabetic retinopathy models in Thailand and India, researchers noticed that model performance was impacted by socioeconomic factors38, and determined that where a model is most useful may not be where the model was generated. Ophthalmology models may need to be deployed in endocrinology care, as opposed to eye centers, due to access issues in the specific local environment. Another effective tool to build physician trust in AI results is side-by-side deployment of ML models with existing workflows (e.g manual grading16). See Fig. 5. Without question, AI models will require rigorous evaluation through clinical trials, to gauge safety and effectiveness. Excitingly, AI and CV can also help support clinical trials153,154 through a number of applications\u2014including patient selection, tumor tracking, adverse event detection, etc\u2014creating an ecosystem in which AI can help design safe AI.\n\nFig. 5: Clinical Deployment. An example workflow showing the positive compounding effect of AI-enhanced workflows, and the resultant trust that can be built. AI predictions provide immediate value to physicians, and improve over time as bigger datasets are collected. Full size image\n\nTrust for AI in healthcare is fundamental to its adoption155 both by clinical teams and by patients. The foundation of clinical trust will come in large part from rigorous prospective trials that validate AI algorithms in real-world clinical environments. These environments incorporate human and social responses, which can be hard to predict and control, but for which AI technologies must account for. Whereas the randomness and human element of clinical environments are impossible to capture in retrospective studies, prospective trials that best reflect clinical practice will shift the conversation towards measurable benefits in real deployments. Here, AI interpretability will be paramount\u2014predictive models will need the ability to describe why specific factors about the patient or environment lead them to their predictions.\n\nIn addition to clinical trust, patient trust\u2014particularly around privacy concerns\u2014must be earned. One significant area of need is next-generation regulations that account for advances in privacy-preserving techniques. ML typically does not require traditional identifiers to produce useful results, but there are meaningful signals in data that can be considered sensitive. To unlock insights from these sensitive data types, the evolution of privacy-preserving techniques must continue, and further advances need to be made in fields such as federated learning and federated analytics.\n\nEach technological wave affords us a chance to reshape our future. In this case, artificial intelligence, deep learning, and computer vision represent an opportunity to make healthcare far more accessible, equitable, accurate, and inclusive than it has ever been."}, {"url": "https://link.springer.com/article/10.1007/s41095-021-0229-5", "page_content": "Charles, R. Q.; Hao, S.; Mo, K. C.; Guibas, L. J. PointNet: Deep learning on point sets for 3D classification and segmentation. IN: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 77\u201385, 2017.\n\nTchapmi, L. P.; Choy, C. B.; Armeni, I.; Gwak, J.; Savarese, S. SEGCloud: Semantic segmentation of 3D point clouds. In: Proceedings of the International Conference on 3D Vision, 537\u2013547, 2017.\n\nLi, Y.; Bu, R.; Sun, M.; Wu, W.; Di, X.; Chen, B. PointCNN: Convolution on x-transformed points. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems, 828\u2013838, 2018.\n\nAtzmon, M.; Maron, H.; Lipman, Y. Point convolutional neural networks by extension operators. ACM Transactions on Graphics Vol. 37, No. 4, Article No. 71, 2018.\n\nWu, W. X.; Qi, Z.; Fuxin, L. PointConv: Deep convolutional networks on 3D point clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9613\u20139622, 2019.\n\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; Polosukhin, I. Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing, 6000\u20136010, 2017.\n\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nWu, B.; Xu, C.; Dai, X.; Wan, A.; Zhang, P.; Tomizuka, M.; Keutzer, K.; Vajda, P. Visual transformers: Token-based image representation and processing for computer vision. arXiv preprint arXiv:2006.03677, 2020.\n\nBruna, J.; Zaremba, W.; Szlam, A.; LeCun, Y. Spectral networks and locally connected networks on graphs. In: Proceedings of the International Conference on Learning Representations, 2014.\n\nHu, S.-M.; Liang, D.; Yang, G.-Y.; Yang, G.-W.; Zhou, W.-Y. Jittor: A novel deep learning framework with meta-operators and unified graph execution. Science China Information Sciences Vol. 63, No. 12, Article No. 222103, 2020.\n\nBahdanau, D.; Cho, K. H.; Bengio, Y. Neural machine translation by jointly learning to align and translate. In: Proceedings of the 3rd International Conference on Learning Representations, 2015.\n\nLin, Z.; Feng, M.; dos Santos, C. N.; Yu, M.; Xiang, B.; Zhou, B.; Bengio, Y. A structured self-attentive sentence embedding. In: Proceedings of the International Conference on Learning Representations, 2017.\n\nDevlin, J.; Chang, M.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol. 1, 4171\u20134186, 2019.\n\nYang, Z.; Dai, Z.; Yang, Y.; Carbonell, J. G.; Salakhutdinov, R.; Le, Q. V. XLNet: Generalized autoregressive pretraining for language understanding. In: Proceedings of the 33rd Conference on Neural Information Processing Systems, 5754\u20135764, 2019.\n\nDai, Z. H.; Yang, Z. L.; Yang, Y. M.; Carbonell, J.; Le, Q.; Salakhutdinov, R. Transformer-XL: Attentive language models beyond a fixed-length context. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2978\u20132988, 2019.\n\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; Kang, J. BioBERT: A pre-trained biomedical language representation model for biomedical text mining. Bioinformatics Vol. 36, No. 4, 1234\u20131240, 2020.\n\nWang, F.; Jiang, M. Q.; Qian, C.; Yang, S.; Li, C.; Zhang, H. G.; Wang, X.; Tang, X. Residual attention network for image classification. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6450\u20136458, 2017.\n\nHu, J.; Shen, L.; Sun, G. Squeeze-and-excitation networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7132\u20137141, 2018.\n\nZhang, H.; Goodfellow, I. J.; Metaxas, D. N.; Odena, A. Self-attention generative adversarial networks. In: Proceedings of the International Conference on Machine Learning, 7354\u20137363, 2019.\n\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; Zagoruyko, S. End-to-end object detection with transformers. In: Computer Vision \u2014 ECCV 2020. Lecture Notes in Computer Science, Vol. 12346. Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J. M. Eds. Springer Cham, 213\u2013229, 2020.\n\nQi, C. R.; Yi, L.; Su, H.; Guibas, L. J. PointNet++: Deep hierarchical feature learning on point sets in a metric space. In: Proceedings of the 31st Conference on Neural Information Processing Systems, 5099\u20135108, 2017.\n\nHermosilla, P.; Ritschel, T.; V\u00e1zquez, P. P.; Vinacua, \u00c0.; Ropinski, T. Monte Carlo convolution for learning on non-uniformly sampled point clouds. ACM Transactions on Graphics Vol. 37, No. 6, Article No. 235, 2018.\n\nTatarchenko, M.; Park, J.; Koltun, V.; Zhou, Q. Y. Tangent convolutions for dense prediction in 3D. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3887\u20133896, 2018.\n\nLandrieu, L.; Simonovsky, M. Large-scale point cloud semantic segmentation with superpoint graphs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4558\u20134567, 2018.\n\nYang, Y. Q.; Liu, S. L.; Pan, H.; Liu, Y.; Tong, X. PFCNN: Convolutional neural networks on 3D surfaces using parallel frames. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13575\u201313584, 2020.\n\nWang, Y.; Sun, Y.; Liu, Z.; Sarma, S. E.; Bronstein, M. M.; Solomon, J. M. Dynamic graph CNN for learning on point clouds. ACM Transactions on Graphics Vol. 38, No. 5, Article No. 146, 2019.\n\nYan, X.; Zheng, C. D.; Li, Z.; Wang, S.; Cui, S. G. PointASNL: Robust point clouds processing using nonlocal neural networks with adaptive sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5588\u20135597, 2020.\n\nHertz, A.; Hanocka, R.; Giryes, R.; Cohen-Or, D. PointGMM: A neural GMM network for point clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12051\u201312060, 2020.\n\nWang, Y.; Solomon, J. Deep closest point: Learning representations for point cloud registration. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 3522\u20133531, 2019.\n\nWu, Z.; Song, S.; Khosla, A.; Yu, F.; Zhang, L.; Tang, X.; Xiao, J. 3D ShapeNets: A deep representation for volumetric shapes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1912\u20131920, 2015.\n\nYi, L.; Kim, V. G.; Ceylan, D.; Shen, I. C.; Yan, M. Y.; Su, H.; Lu, C.; Huang, Q.; Sheffer, A.; Guibas, L. A scalable active framework for region annotation in 3D shape collections. ACM Transactions on Graphics Vol. 35, No. 6, Article No. 210, 2016.\n\nXie, S. N.; Liu, S. N.; Chen, Z. Y.; Tu, Z. W. Attentional ShapeContextNet for point cloud recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4606\u20134615, 2018.\n\nLi, J. X.; Chen, B. M.; Lee, G. H. SO-net: Self-organizing network for point cloud analysis. In: Proceeding of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9397\u20139406, 2018.\n\nKlokov, R.; Lempitsky, V. Escape from cells: Deep kdnetworks for the recognition of 3D point cloud models. In: Proceeding of the IEEE International Conference on Computer Vision, 863\u2013872, 2017.\n\nLe, T.; Duan, Y. PointGrid: A deep network for 3D shape understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9204\u20139214, 2018.\n\nZhao, H.; Jiang, L.; Fu, C.; Jia, J. PointWeb: Enhancing local neighborhood features for point cloud processing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5560\u20135568, 2019.\n\nKomarichev, A.; Zhong, Z. C.; Hua, J. A-CNN: Annularly convolutional neural networks on point clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7413\u20137422, 2019.\n\nLiu, X. H.; Han, Z. Z.; Liu, Y. S.; Zwicker, M. Point2Sequence: Learning the shape representation of 3D point clouds with an attention-based sequence to sequence network. In: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, 8778\u20138785, 2019.\n\nThomas, H.; Qi, C. R.; Deschaud, J. E.; Marcotegui, B.; Goulette, F.; Guibas, L. KPConv: Flexible and deformable convolution for point clouds. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 6410\u20136419, 2019."}, {"url": "http://oei.hust.edu.cn/info/1100/5743.htm", "page_content": "\u4e2a\u4eba\u7b80\u4ecb\uff1a\n\n\u5149\u7535\u4fe1\u606f\u5b66\u9662\u201c\u751f\u7269\u5149\u5b50\u5b66\u4e0e\u5fae\u6d41\u63a7\u6280\u672f\u5b9e\u9a8c\u5ba4\u201d\u8d1f\u8d23\u4eba(PI)\n\n\u79d1\u6280\u90e8863\u8ba1\u5212\u8bfe\u9898\u8d1f\u8d23\u4eba\uff1b\n\n\n\n\u534e\u4e2d\u79d1\u6280\u5927\u5b66\u9e1f\u5de2\u8ba1\u5212\u5b66\u8005\n\n\u804c\u79f0\uff1a\u6559\u6388\uff08\u5149\u7535\u4fe1\u606f\u5b66\u9662/\u521b\u65b0\u7814\u7a76\u9662\uff09\uff1b\u7814\u7a76\u5458\uff08\u534e\u4e2d\u79d1\u6280\u5927\u5b66\u6df1\u5733\u7814\u7a76\u9662\uff09\uff1b\n\n\u7814\u7a76\u65b9\u5411\uff1a\u751f\u7269\u6210\u50cf\u6280\u672f\u3001\u5fae\u6d41\u63a7\u751f\u7269\u82af\u7247\u6280\u672f\u3001\u96c6\u6210\u5f0f\u5149\u673a\u7535\u5668\u4ef6\u6280\u672f\n\n\u8054\u7cfb\u65b9\u5f0f\uff1a027-87559243\uff08\u529e\u516c\uff09\uff1b\n\nfeipeng@hust.edu.cn\uff08\u90ae\u7bb1\uff09\n\n\u5b9e\u9a8c\u5ba4\u5730\u5740\uff1a\u534e\u4e2d\u79d1\u6280\u5927\u5b66\u5357\u4e94\u697c407 (\u529e\u516c\u5ba4)\n\n\u534e\u4e2d\u79d1\u6280\u5927\u5b66\u5357\u4e94\u697c405\uff08\u5de5\u4f5c\u95f4\uff09\n\n\u534e\u4e2d\u79d1\u6280\u5927\u5b66\u5357\u4e94\u697c414,415\uff08\u5b9e\u9a8c\u5ba4\uff09\n\n\u4e00\u3001 \u7814\u7a76\u65b9\u5411\n\n\u751f\u7269\u5149\u5b50\u5b66\u4e0e\u5fae\u6d41\u63a7\u6280\u672f\u8bfe\u9898\u7ec4\u81f4\u529b\u4e8e\u53d1\u5c55\u9762\u5411\u751f\u547d\u79d1\u5b66\u7814\u7a76\u7684\u65b0\u6280\u672f\u3002\u8bfe\u9898\u7ec4\u5f53\u524d\u7684\u7814\u7a76\u5174\u8da3\u96c6\u4e2d\u5728\u751f\u7269\u6210\u50cf\u6280\u672f\uff0c\u5fae\u6d41\u63a7\u751f\u7269\u82af\u7247\u6280\u672f\u53ca\u96c6\u6210\u5f0f\u5149\u673a\u7535\u5668\u4ef6\u6280\u672f\u4e09\u4e2a\u65b9\u9762\u3002\u6211\u4eec\u4fa7\u91cd\u53d1\u5c55\u9ad8\u6548\u3001\u65b0\u578b\u7684\u5149\u5b66\u663e\u5fae\u6210\u50cf\u6280\u672f\u53ca\u5176\u5e94\u7528\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u8d85\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u5149\u7247\u6210\u50cf\u6280\u672f\u7684\u7814\u53d1\uff0c\u9762\u5411\u5927\u6570\u636e\u65f6\u4ee3\u7684\u56fe\u50cf\u5e76\u884c\u8ba1\u7b97\u7684\u7814\u53d1\uff0c\u4ee5\u53ca\u5c16\u7aef\u6210\u50cf\u6280\u672f\u5728\u53d1\u80b2\u751f\u7269\u5b66\u3001\u7ec4\u7ec7\u5de5\u7a0b\u3001\u518d\u751f\u533b\u5b66\u7b49\u591a\u7ec6\u80de\u751f\u7269\u4f53\u7814\u7a76\u9886\u57df\u7684\u5e94\u7528\u3002\u6211\u4eec\u540c\u65f6\u5927\u529b\u53d1\u5c55\u5927\u89c4\u6a21\u96c6\u6210\u5fae\u6d41\u63a7\u82af\u7247\u6280\u672f\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u5728\u751f\u547d\u79d1\u5b66\u5173\u952e\u95ee\u9898\u7684\u7814\u7a76\u4e2d\uff0c\u5305\u62ec\u7ec6\u80de\u57f9\u517b\u53ca\u5fae\u73af\u5883\u63a7\u5236\uff0c\u4ee5\u53ca\u5728\u5355\u7ec6\u80de\u5c42\u6b21\u4e0a\u89c2\u5bdf\u751f\u547d\u8fc7\u7a0b\u7684\u968f\u673a\u6027\u3002\u6211\u4eec\u8fd8\u5c06\u5927\u529b\u53d1\u5c55\u524d\u6cbf\u7684\u5149\u5b66\u6210\u50cf\u6280\u672f\u4e0e\u5fae\u673a\u7535\u7cfb\u7edf\u7684\u96c6\u6210\u548c\u5668\u4ef6\u5c0f\u578b\u5316\uff0c\u7528\u4e8e\u5f00\u5c55\u5bf9\u6d3b\u4f53\u52a8\u7269\u7684\u975e\u4ecb\u5165\u6027\u89c2\u5bdf\u7814\u7a76\uff0c\u4ee5\u53ca\u670d\u52a1\u4e8e\u4eba\u7684\u4fbf\u643a\u5f0f\u533b\u7597\u548c\u5065\u5eb7\u8bca\u65ad\u3002\n\n\u4e8c\u3001\u8bfe\u9898\u7ec4\u7b80\u4ecb\n\n\u5b66\u9662\u751f\u7269\u5149\u5b50\u5b66\u4e0e\u5fae\u6d41\u63a7\u6280\u672f\u8bfe\u9898\u7ec4\u65b0\u8fd1\u6210\u7acb\u4e8e2015\u5e748\u6708\u6210\u7acb\uff0c\u8d1f\u8d23\u4eba\u8d39\u9e4f\u6559\u6388\u4e3b\u6301\u79d1\u6280\u90e8\u201c863\u201d\u8ba1\u5212\u8bfe\u9898\uff08\u5fae\u7eb3\u7cfb\u7edf\u4e09\u7ef4\u5f02\u8d28\u96c6\u6210\u5316\u6280\u672f\uff0c2015AA042602\uff0c729\u4e07\uff09\uff0c\u5177\u5907\u56e2\u961f\u7ba1\u7406\u548c\u9879\u76ee\u8fd0\u4f5c\u7ecf\u9a8c\u3002\u8bfe\u9898\u7ec4\u76ee\u524d\u6b63\u6301\u7eed\u53d1\u5c55\u4e2d\uff0c\u9010\u6b65\u5f62\u6210\u4e00\u652f\u5305\u542b\u535a\u58eb\u540e\uff0c\u535a\u58eb\u751f\u3001\u7855\u58eb\u751f\u3001\u672c\u79d1\u751f\u5728\u5185\u7684\u5b8c\u6574\u79d1\u7814\u68af\u961f\uff0c\u961f\u4f0d\u5e74\u8f7b\u4e14\u4e13\u4e1a\u914d\u7f6e\u5408\u7406\uff08\u5149\u7535\u3001\u673a\u68b0\u81ea\u52a8\u5316\u3001\u751f\u7269\u533b\u5b66\u80cc\u666f\u5747\u6709\u6210\u5458\u53c2\u4e0e\uff09\u3002\u8bfe\u9898\u7ec4\u5728\u5149\u7535\u5b66\u9662\u5357\u4e94\u697c\u7ec4\u5efa\u5b9e\u9a8c\u5ba4\uff0c\u8f9f\u670950\u5e73\u7c73\u7528\u4e8e\u751f\u533b\u5149\u5b50\u5b66\u7814\u7a76\u7684\u6d01\u51c0\u5b9e\u9a8c\u7a7a\u95f4\uff0c\u914d\u5907\u5965\u6797\u5df4\u65af\u663e\u5fae\u955c\uff0c\u6ee8\u677e\u63a2\u6d4b\u5668\uff0c\u5b89\u9053\u5c14\u76f8\u673a\uff0c\u5b89\u6377\u4f26\u963b\u6297\u5206\u6790\u4eea\uff0c\u8bd5\u6837\u5236\u4f5c\u673a\u7b49\u591a\u5957\u5149\u7535\u8bbe\u5907\u548c\u9644\u4ef6\u3002\u540c\u65f6\u81ea\u4e3b\u642d\u5efa\u6709\u4e24\u5957\u524d\u6cbf\u7684\u5149\u7247\u8367\u5149\u663e\u5fae\u7cfb\u7edf\uff0c\u5e76\u914d\u5957\u56fe\u5f62\u5de5\u4f5c\u7ad9\uff0c\u5b58\u50a8\u670d\u52a1\u5668\uff0c\u6570\u636e\u91c7\u96c6\u7535\u8111\u591a\u53f0\u3002\u8bfe\u9898\u7ec4\u8fd8\u5efa\u670930\u5e73\u5343\u7ea7\u751f\u7269\u6d01\u51c0\u95f4\uff0c\u5185\u914d\u6709\u7ec6\u80de\u57f9\u517b\u7bb1\uff0c\u5fae\u751f\u7269\u57f9\u517b\u7bb1\uff0c\u7d2b\u5916\u5de5\u4f5c\u53f0\uff0c\u7b49\u79bb\u5b50\u6e05\u6d17\u673a\uff0c\u79bb\u5fc3\u673a\u7b49\u591a\u5957\u8bbe\u5907\uff0c\u7528\u4e8e\u5f00\u5c55\u7ec6\u80de\u57f9\u517b\u7b49\u751f\u7269\u5b66\u64cd\u4f5c\u548c\u5fae\u6d41\u63a7\u82af\u7247\u3001\u5fae\u673a\u7535\u5668\u4ef6\u7684\u5236\u5907\u3002\u5b9e\u9a8c\u5ba4\u83b7\u56fd\u5bb6985\u5de5\u7a0b\u7ecf\u8d39\u652f\u6301\uff0c\u5e76\u4e0e\u56fd\u5bb6\u5149\u7535\u5b9e\u9a8c\u5ba4\u751f\u7269\u533b\u5b66\u5149\u5b50\u5b66\u529f\u80fd\u5b9e\u9a8c\u5ba4\u6df1\u5ea6\u5171\u4eab\u79d1\u6280\u8d44\u6e90\u3002\u8bfe\u9898\u7ec4\u5c06\u7ee7\u7eed\u79c9\u627f\u5149\u7535\u3001\u751f\u7269\u3001\u6750\u6599\u3001\u8ba1\u7b97\u673a\u7b49\u524d\u6cbf\u5b66\u79d1\u590d\u5408\uff0c\u4ea4\u53c9\u7684\u7279\u70b9\uff0c\u671d\u9ad8\u7cbe\u5c16\u79d1\u5b66\u6280\u672f\u53d1\u5c55\u7684\u65b9\u5411\u7ee7\u7eed\u8fc8\u8fdb\u3002\n\n\u5e74\u8f7b\u7684\u56e2\u961f\u4eba\u5458\u914d\u7f6e\n\n\u65b0\u5efa\u7684\u5b9e\u9a8c\u5ba4\u7a7a\u95f4\u53ca\u591a\u5957\u5149\u5b66\u8bbe\u5907\n\n\u4e09\u3001\u6559\u5b66\u60c5\u51b5\n\n\u8bfe\u9898\u7ec4\u8d1f\u8d23\u4eba\u540c\u65f6\u5c06\u627f\u62c5\u6559\u5b66\u4efb\u52a1\u3002\u56e0\u56de\u56fd\u4e0d\u4e45\uff0c\u672c\u5b66\u671f\u6682\u672a\u6388\u8bfe\u3002\u672c\u5b66\u671f\u6559\u5b66\u65b9\u9762\u6307\u5bfc6\u540d\u540c\u5b66\u672c\u79d1\u751f\u6bd5\u4e1a\u8bbe\u8ba1\uff1b\u62c5\u4efb\u542f\u660e\u5b66\u9662\u5bfc\u5e08\uff0c\u6307\u5bfc\u542f\u660e\u5b66\u9662\u7279\u4f18\u751f\u4e00\u540d\uff1b\u62c5\u4efb\u4e24\u540d\u5927\u73e9\u73ed\u65b0\u751f\u5bfc\u5e08\uff1b\u6307\u5bfc\u4e00\u7ec4\u5b66\u9662\u672c\u79d1\u751f\u56e2\u961f\u53c2\u52a0\u5168\u56fd\u5149\u7535\u5927\u8d5b\u3002\u4e0b\u5b66\u671f\u8d77\u5c06\u52a0\u5165\u5149\u7535\u5de5\u7a0b\u7cfb\u201c\u5149\u7535\u4fe1\u53f7\u5904\u7406\u201d\u6559\u5b66\u7ec4\uff0c\u5f00\u59cb\u6388\u8bfe\u3002\n\n\u56db\u3001\u8bfe\u9898\u7ec4\u79d1\u7814\u6210\u679c\n\n\u8bfe\u9898\u7ec4\u6210\u7acb\u8fd1\u4e00\u5e74\u6765\uff0c\u5728\u5149\u7247\u663e\u5fae\u6210\u50cf\uff0c\u663e\u5fae\u56fe\u50cf\u91cd\u6784\uff0c\u4fbf\u643a\u8d85\u5206\u8fa8\u6210\u50cf\uff0c\u4f20\u611f\u5668\u4e09\u7ef4\u5f02\u8d28\u96c6\u6210\u65b9\u9762\u5df2\u53d6\u5f97\u4e00\u5b9a\u6210\u679c\u3002\u9996\u6b21\u5c06\u8d85\u5206\u8fa8\u5149\u7247\u663e\u5fae\u6210\u50cf\u7528\u4e8e\u5fc3\u810f\u53d1\u80b2\u75c5\u4f8b\u7684\u7814\u7a76\uff1b\u9996\u521b\u4fbf\u643a\u7684\u65e0\u7ebf\u8d85\u5206\u8fa8\u663e\u5fae\u955c\u7528\u4e8e\u5b9e\u65f6\u3001\u539f\u4f4d\u5730\u89c2\u5bdf\u7ec6\u80de\u836f\u7269\u53cd\u5e94\u7b49\u751f\u7269\u73b0\u8c61\uff1b\u53d1\u660e\u4e00\u79cd\u5c0f\u578b\u5316\u7684\u5149\u7247\u7167\u660e\u88c5\u7f6e\uff0c\u53ef\u4e0e\u4e3b\u6d41\u7684\u5bbd\u573a\u8367\u5149\u663e\u5fae\u955c\u7ed3\u5408\uff0c\u5b9e\u65bd\u9ad8\u7ea7\u7684\u5149\u7247\u4e09\u7ef4\u6210\u50cf\uff1b\u5b8c\u6210\u6781\u5927\u6df1\u5f84\u6bd4\u7684TSV\u5236\u5907\u548c\u7535\u9540\uff0c\u4ee5\u53ca\u538b\u529b\u3001\u6e29\u6e7f\u4f20\u611f\u5668\u7684\u4e09\u7ef4\u5f02\u8d28\u96c6\u6210\u3002\n\n\u76f8\u5173\u6210\u679c\u65b0\u8fd1\u4e8e2015\u5e74\u5e95\u81f32016\u5e74\u521d\u5728\u56fd\u9645\u9876\u7ea7\u533b\u5b66\u671f\u520aJournal of Clinical Investigation\uff08\u534e\u79d1\u5927\u7b2c\u4e8c\u7bc7\uff0c\u672c\u90e8\u7b2c\u4e00\u7bc7\uff0c\u5f71\u54cd\u56e0\u5b5013.215\uff09, Nature\u51fa\u7248\u96c6\u56e2\u520a\u7269Scientific Reports, \u5149\u5b66\u7c7b\u9ad8\u6c34\u5e73\u671f\u520aOptics Express\uff0cBiomedical Optics Express\u4e0a\u53d1\u8868\uff0c\u53e6\u6709\u4e24\u9879\u5149\u5b66\u6210\u50cf\u548c\u4f20\u611f\u5668\u6210\u679c\u4ea6\u6b63\u5728\u6295\u7a3f\u4e2d\u3002\u8bfe\u9898\u7ec4\u8fd8\u7533\u8bf7\u4e86\u5149\u7247\u6280\u672f\u76f8\u5173\u4e13\u52293\u9879\uff0c\u6388\u6743\u5b9e\u7528\u65b0\u578b1\u9879\u3002\u8bfe\u9898\u7ec4\u76ee\u524d\u6b63\u4e0e\u56fd\u5185\u5916\u591a\u4e2a\u751f\u7269\u533b\u5b66\u7814\u7a76\u7ec4\u5f00\u5c55\u5408\u4f5c\u4e2d\u3002\n\n\u4e94\u3001\u8d39\u9e4f\u6559\u6388\u5b66\u4e60\u5de5\u4f5c\u7ecf\u5386\n\n2006 - 2007\u5e74\uff0c\u5317\u4eac\u5927\u5b66\uff0c\u6750\u6599\u79d1\u5b66\u4e0e\u5de5\u7a0b\u7cfb\uff0c\u7814\u7a76\u751f\uff1b\n\n2007 - 2009\u5e74\uff0c\u4f50\u6cbb\u4e9a\u7406\u5de5\u5b66\u9662\uff0c\u6750\u6599\u79d1\u5b66\u4e0e\u5de5\u7a0b\u7cfb\uff0c\u8054\u5408\u57f9\u517b\u535a\u58eb\u751f\uff1b\n\n2009 - 2012\u5e74 \u5317\u4eac\u5927\u5b66\uff0c\u6750\u6599\u79d1\u5b66\u4e0e\u5de5\u7a0b\u7cfb\uff0c\u535a\u58eb\u751f\uff1b\n\n2012 - 2014\u5e74\uff0c\u52a0\u5dde\u5927\u5b66\u6d1b\u6749\u77f6\u5206\u6821\uff0c\u673a\u68b0\u4e0e\u822a\u5929\u5de5\u7a0b\u7cfb\uff0c\u535a\u58eb\u540e\u7814\u7a76\u5458\uff1b\n\n2014 - 2015\u5e74\uff0c\u52a0\u5dde\u5927\u5b66\u6d1b\u6749\u77f6\u5206\u6821\uff0c\u533b\u5b66\u9662\uff0c\u7814\u7a76\u79d1\u5b66\u5bb6\uff1b\n\n2014 - \u73b0\u5728\uff0c\u534e\u4e2d\u79d1\u6280\u5927\u5b66\uff0c\u5149\u5b66\u4e0e\u7535\u5b50\u4fe1\u606f\u5b66\u9662\uff0c\u6559\u6388\u3002\n\n\u516d\u3001\u4ee3\u8868\u6027\u8bba\u8457\u53ca\u4e13\u5229\n\n\u4ee3\u8868\u6027\u8bba\u6587\n\n(1)Peng Fei, Ping-Hung Yeh, Jun Zhou, Sheng Xu, Yifan Gao, Jinhui Song, Yudong Gu, Yanyi Huang and Zhong Lin Wang(*), Piezoelectric PotentialGated Field-Effect TransistorBased on a Free-Standing ZnO Wire, Nano Letters, 2009, 9, 3435-3439.\n\n(2) Jun Zhou, Peng Fei(#), Yifan Gao, Yudong Gu, Jin Liu, Gang Bao and Z.L. Wang(*), Mechanical-Electrical Triggers and Sensors Using Piezoelectric Micowires/Nanowires, Nano Letters,2008, 8(9), 2725\u20132730. (#co-first author)\n\n(3)Jun Zhou, Peng Fei, Yudong Gu, Wenjie Mai, Yifan Gao, Rusen Yang, Gang Bao, and Z.L. Wang(*), Piezoelectric-Potential-Controlled Polarity-Reversible Schottky Diodes and Switches of ZnO Wires,Nano Letters,2008, (8),11. 3973-3977.\n\n(4)Jin Liu, Peng Fei, Jinhui Song, Xudong Wang, Changshi Lao, R. Tummala and Z.L. Wang(*),Carrier Density and Schottky Barrier on the Performance of DC Nanogenerator, Nano Letters,2008, 8, 328-332.\n\n(5)Peng Fei\uff0cZi He\uff0cChunhong Zheng\uff0cTao Chen\uff0cYongfan Men\uff0cYanyi Huang(*)\uff0cDiscretely Tunable Optofluidic Compound Microlenses\uff0cLab on a Chip\uff0c2011\uff0c11 (3)\uff1a2835-2837\n\n(6)Peng Fei\uff0cZitian Chen\uff0cYonfanmen\uff0cAng Li\uff0cYiran Shen\uff0cYanyi Huang(*)\uff0cA Compact Optofluidic Cytometer with Integrated Liquid-core/PDMS-cladding Waveguides\uff0cLab on a Chip\uff0c2012\uff0c12 (5)\uff1a3700-3706\n\n(7)Peng Fei\uff0cZhilong Yu\uff0cXu Wang\uff0cPeter J. Lu\uff0cYusi Fu\uff0cZi He\uff0cJingweiXiong\uff0cYanyi Huang(*)\uff0cHigh Dynamic Range Optical Projection Tomography (HDR-OPT)\uff0cOptics Express\uff0c2012\uff0c20 (8)\uff1a8824-8836\n\n(8 ) Peng Fei(*)\uff0cKenneth TU\uff0cJuhyun Lee\uff0cLarge Scale, High Resolution Imaging with a Simple and RobustSuper-resolution Light Sheet Microscopy\uff0cAnnual Conference of Biomedical Engineering Society 2014\uff0cSan Antonio, 2014.10.22-2014.10.26\n\n(9) Juhyun Lee\uff0cPeng Fei\uff0cNelson Jen\uff0cTyler Beebe\uff0cChih-ming Ho\uff0cAlison\n\nMarsden\uff0cNeil Chi\uff0cTzung Hsiai(*)\uff0cNovel 4-Dimensional Optical Technique to Elucidate Hemodynamic Shear Forces and Initiation of Trabeculation During Cardiac Morphogenesis\uff0cCirculation\uff0c2014\uff0c130 (2)\uff1aA12438-A12438.\n\n(10) Masaya Hagiwara,Peng Fei(#), Chih-Ming Ho(*)\uff0cIn vitro reconstruction of branched tubular structures from lung epithelial cells in high cell concentration gradient environment\uff0cScientific Reports\uff0c2015, (3)\uff1a7603-7608. (#co-first author)\n\n(11) Di Jin\uff0cDennis Wong\uff0cJunxiang Li\uff0cZhang Luo\uff0cYiran Guo\uff0cBifengLiu\uff0cQiong Wu\uff0cChih-Ming Ho\uff0cPeng Fei(*)\uff0cCompact Wireless Microscope for In-Situ Time Course Study of Large Scale Cell Dynamics within an Incubator\uff0cScientific Reports\uff0c2015, 5:18483\n\n(12) Zeyi Guan\uff0cJuhyun Lee\uff0cHao Jiang\uff0cSiyan Dong\uff0cNelso Jen\uff0cTzung Hsiai\uff0cChih-Ming Ho\uff0cPeng Fei(*)\uff0cCompact plane illumination plugin device to enable light sheet fluorescence imaging of multicellular organisms on an inverted wide-field microscope\uff0cBiomedical OpticsExpress\uff0c2015\uff0c7 (1)\uff1a194-208\n\n(13) Peng Fei\uff0cJuhyun Lee\uff0cRen\u00e9 R. Sevag Packard\uff0cKonstantina-Ioanna Sereti\uff0cHao Xu\uff0cJianguo Ma\uff0cYichen Ding\uff0cHanul Kang\uff0cHarrison Chen\uff0cKevin Sung\uff0cRajan Kulkarni\uff0cReza Ardehali\uff0cC.-C. Jay Kuo\uff0cXiaolei Xu\uff0cChih-MingHo\uff0cTzung K. Hsiai(*)\uff0cCardiac Light-Sheet Fluorescent Microscopy for Multi-Scale and Rapid Imaging of Architecture and Function\uff0cScientific Reports\uff0c2016, 6\uff1a22489\n\n(14) Juhyun Lee\uff0cPeng Fei(#)\uff0cRen\u00e9 R. Sevag Packard\uff0cHanul Kang\uff0cHao Xu\uff0cKyung In Baek\uff0cNelson Jen\uff0cJunjie Chen\uff0cHilary Yen\uff0cC-C Jay Kuo\uff0cNeil C. Chi\uff0cChih-Ming Ho\uff0cRongsong Li\uff0cTzung K. Hsiai(*)\uff0c4-Dimensional Light-sheet Microscopy to Elucidate Shear Stress Modulation of Cardiac Trabeculation, Journal of Clinical Investigation,2016, doi:10.1172/JCI83496 (#co-first author)\n\n\u4ee3\u8868\u6027\u4e13\u5229\n\n(1) Zhongling Wang, Peng Fei, Transverse Force, Pressure and Vibration Sensors using Piezoelectric Nanostructures\uff0cUS20110006286. US patent\n\n(2) \u9ec4\u5ca9\u8c0a\uff0c\u8d39\u9e4f\uff0c\u8d6b\u6ecb\uff0c\u90d1\u6625\u7ea2\uff0c\u95e8\u6d8c\u5e06\uff0c\u57fa\u4e8e\u5fae\u6d41\u63a7\u5b66\u7684\u6570\u5b57\u53ef\u8c03\u5f0f\u5fae\u955c\u82af\u7247\u53ca\u5176\u5236\u5907\u65b9\u6cd5\uff0c\u4e2d\u56fd\uff0cCN102841443A\u3002\u53d1\u660e\u4e13\u5229\n\n(3) \u8d39\u9e4f\uff0c\u5173\u6cfd\u4e00\uff0c\u8463\u601d\u708e\uff0c\u4e25\u521a\uff0c\u4e00\u79cd\u8367\u5149\u6210\u50cf\u88c5\u7f6e\uff0c\u4e2d\u56fd\uff0cCN201420530078X\u3002\u5b9e\u7528\u65b0\u578b\n\n(4)\u8d39\u9e4f\uff0c\u5173\u6cfd\u4e00\uff0c\u8463\u601d\u708e\uff0c\u4e25\u521a\uff0c\u4e00\u79cd\u8367\u5149\u6210\u50cf\u88c5\u7f6e\uff0c\u4e2d\u56fd\uff0cCN2013104679416\u3002\u4e13\u5229\n\n(5) Peng Fei\uff0cDennis Wong\uff0cKennth Tu\uff0cJuhyun Lee\uff0cA Three Dimensional Voxel Super Resovled Lighet-Sheet Microscopy\uff0cUnited States\uff0cUC 2014-967. US provisional patent\n\n(6) Peng Fei\uff0cJuhyun Lee\uff0cSiyan Dong\uff0cZeyi Guan\uff0cJonathan C. Law\uff0cA Kind of Compact Device to Enable Light Sheet Imaging on an Inverted Wide-Field Microscope\uff0cUnited States\uff0cUC 2014-9AH-0.US provisional patent\n\n(7)\u8d39\u9e4f\uff0c\u5173\u6cfd\u4e00\uff0c\u8463\u601d\u708e\uff0c\u4e00\u79cd\u4f7f\u7528\u5149\u6d41\u63a7\u6280\u672f\u7684\u81ea\u52a8\u9ad8\u901a\u91cf\u5207\u7247\u5149\u626b\u63cf\u6210\u50cf\u65b9\u6cd5\uff0c\u4e2d\u56fd\uff0cCN201410278052.5\u3002\u4e13\u5229\n\n(8) \u8d39\u9e4f\uff0c\u5173\u6cfd\u4e00\uff0c\u8463\u601d\u708e\uff0c\u865e\u4e4b\u9f99\uff0c\u4e00\u79cd\u4e09\u7ef4\u8d85\u5206\u8fa8\u663e\u5fae\u6210\u50cf\u65b9\u6cd5\uff0c\u4e2d\u56fd\uff0cCN201410269145.1\u3002\u4e13\u5229\n\n\u4e03\u3001\u62df\u62db\u6536\u7814\u7a76\u751f\u4e13\u4e1a\n\n1\u3001\u5149\u7535\u4fe1\u606f\uff0c\u5149\u5b66\u5de5\u7a0b\u4e13\u4e1a\uff1a\u638c\u63e1\u51e0\u4f55\u5149\u5b66\uff0c\u5085\u91cc\u53f6\u5149\u5b66\uff0c\u4fe1\u53f7\u5904\u7406\u7b49\u65b9\u9762\u57fa\u7840\u77e5\u8bc6\uff1b\u5bf9\u7cbe\u5bc6\u5149\u5b66\u7cfb\u7edf\u642d\u5efa\u6709\u6d53\u539a\u5174\u8da3\u6216\u5177\u5907\u7ecf\u9a8c\u8005\u5c24\u4f73\u3002\n\n2\u3001\u7535\u5b50\u3001\u8ba1\u7b97\u673a\u76f8\u5173\u4e13\u4e1a\uff1a\u638c\u63e1\u6570\u5b57\u56fe\u50cf\u5904\u7406\u6216\u7535\u5b50\u7ebf\u8def\u65b9\u9762\u57fa\u7840\u77e5\u8bc6\uff1b\u80fd\u719f\u7ec3\u4f7f\u7528Labview, Matlab\uff0cVC\uff0cCUDA\u7b49\u7f16\u7a0b\u624b\u6bb5\u4e2d\u81f3\u5c11\u4e00\u79cd\u8fdb\u884c\u7f16\u7a0b\u3002\n\n3\u3001\u751f\u7269\u3001\u533b\u5b66\u76f8\u5173\u4e13\u4e1a\uff1a\u638c\u63e1\u7ec6\u80de\u751f\u7269\u5b66\uff0c\u5206\u5b50\u751f\u7269\u5b66\uff0c\u53d1\u80b2\u751f\u7269\u5b66\u65b9\u9762\u57fa\u7840\u77e5\u8bc6\uff1b\u5bf9\u63ed\u793a\u751f\u547d\u79d1\u5b66\u7684\u5965\u79d8\u5177\u6709\u5174\u8da3\uff1b\u5df2\u5177\u5907\u7ec6\u80de\u751f\u7269\u5b66\uff0c\u5206\u5b50\u751f\u7269\u5b66\u5b9e\u9a8c\u64cd\u4f5c\u7ecf\u9a8c\u8005\u5c24\u4f73\u3002\n\n\u516b\u3001\u8d39\u9e4f\u6559\u6388\u5bc4\u8bed\n\n\u79d1\u5b66\u7814\u7a76\u662f\u4e00\u7c7b\u88ab\u5c42\u51fa\u4e0d\u7a77\u7684\u95ee\u9898\u6240\u9a71\u52a8\u7684\u4e00\u5c0f\u64ae\u6781\u5ba2\u94bb\u725b\u89d2\u5c16\u7684\u6d3b\u52a8\u3002\u53ea\u6709\u5f53\u4f60\u5177\u5907\u70ed\u60c5\u548c\u5174\u8da3\uff0c\u624d\u4f1a\u6709\u53d1\u73b0\u95ee\u9898\u7684\u773c\u775b\uff1b\u53ea\u6709\u5f53\u4f60\u5177\u5907\u624e\u5b9e\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u624d\u4f1a\u6709\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002\u540c\u5b66\u4eec\uff0c\u521d\u5165\u79d1\u7814\u9886\u57df\uff0c\u6682\u4e14\u773c\u9ad8\u624b\u4f4e\u6ca1\u5173\u7cfb\uff0c\u5173\u952e\u7684\u662f\u6001\u5ea6\u7aef\u6b63\uff0c\u51e1\u4e8b\u5c3d\u529b\uff1b\u505a\u4ec0\u4e48\u4e8b\u60c5\u4e5f\u4e0d\u90a3\u4e48\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u548c\u8c01\u4e00\u8d77\u505a\u3002\u90a3\u4e48\uff0c\u6211\u7684\u8bfe\u9898\u7ec4\u5c06\u968f\u65f6\u6b22\u8fce\u6709\u6fc0\u60c5\u3001\u6709\u6001\u5ea6\u3001\u6709\u672c\u4e8b\u7684\u540c\u5b66\u52a0\u5165\u3002\u751f\u7269\u5149\u5b50\u5b66\u4e0e\u5fae\u6d41\u63a7\u6280\u672f\u5b9e\u9a8c\u5ba4\u5c06\u4e3a\u4f60\u63d0\u4f9b\u878d\u6d3d\u7684\u56e2\u961f\u6c1b\u56f4\uff0c\u5148\u8fdb\u7684\u79d1\u7814\u7406\u5ff5\u548c\u786c\u4ef6\u3002\u8ba9\u6211\u4eec\u4e00\u8d77\u5feb\u4e50\u5730\uff0c\u6295\u5165\u5730\u73a9\u79d1\u7814\u3002"}, {"url": "https://link.springer.com/article/10.1007/s41095-020-0184-6", "page_content": "Cai, S.; Zuo, W.; Zhang, L. Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization. In: Proceedings of the IEEE International Conference on Computer Vision, 511\u2013520, 2017.\n\nCui, Y.; Song, Y.; Sun, C.; Howard, A.; Belongie, S. J. Large scale fine-grained categorization and domain-specific transfer learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4109\u20134118, 2018.\n\nWang, Y.; Morariu, V. I.; Davis, L. S. Learning a discriminative filter bank within a CNN for fine-grained recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4148\u20134157, 2018.\n\nYang, Z.; Luo, T. G.; Wang, D.; Hu, Z. Q.; Gao, J.; Wang, L. W. Learning to navigate for fine-grained classification. In: Computer Vision \u2014 ECCV 2018. Lecture Notes in Computer Science Vol. 11218. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham, 438\u2013454, 2018.\n\nKhosla, A.; Jayadevaprakash, N.; Yao, B.; Li, F.-F. Novel dataset for fine-grained image categorization. In: Proceedings of the 1st Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition, 2011.\n\nKrizhevsky, A.; Sutskever, I.; Hinton, G. E. ImageNet classification with deep convolutional neural networks. In: Proceedings of the 25th International Conference on Neural Information Processing Systems, Vol. 1, 1097\u20131105, 2012.\n\nChen, L.; Yang, M. Semi-supervised dictionary learning with label propagation for image classification. Computational Visual Media Vol. 3, No. 1, 83\u201394, 2017.\n\nChen, K. X.; Wu, X. J. Component SPD matrices: A low-dimensional discriminative data descriptor for image set classification. Computational Visual Media Vol. 4, No. 3, 245\u2013252, 2018.\n\nRen, J. Y.; Wu, X. J. Vectorial approximations of infinite-dimensional covariance descriptors for image classification. Computational Visual Media Vol. 3, No. 4, 379\u2013385, 2017.\n\nWah, C.; Branson, S.; Welinder, P.; Perona, P.; Belongie, S. The Caltech-UCSD Birds-200\u20132011 Dataset. Computation & Neural Systems Technical Report, CNS-TR-2011-001. California Institute of Technology, 2011.\n\nLiu, J.; Kanazawa, A.; Jacobs, D.; Belhumeur, P. Dog breed classification using part localization. In: Proceedings of the 12th European Conference on Computer Vision, Vol. Part I, 172\u2013185, 2012.\n\nBerg, T.; Belhumeur, P. N. POOF: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 955\u2013962, 2013.\n\nBranson, S.; Horn, G. V.; Belongie, S.; Perona, P. Bird species categorization using pose normalized deep convolutional nets. arXiv preprint arXiv:1406.2952, 2014.\n\nZhang, N.; Donahue, J.; Girshick, R.; Darrell, T. Part-based R-CNNs for fine-grained category detection. In: Computer Vision-ECCV 2014. Lecture Notes in Computer Science Vol. 8689. Fleet, D.; Pajdla, T.; Schiele, B.; Tuytelaars, T. Eds. Springer Cham, 834\u2013849, 2014.\n\nLin, D.; Shen, X.; Lu, C.; Jia, J. Deep LAC: Deep localization, alignment and classification for fine-grained recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1666\u20131674, 2015.\n\nLam, M.; Mahasseni, B.; Todorovic, S. Fine-grained recognition as HSnet search for informative image parts. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6497\u20136506, 2017.\n\nChen, Y.; Bai, Y.; Zhang, W.; Mei, T. Destruction and construction learning for finegrained image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5157\u20135166, 2019.\n\nGe, W. F.; Lin, X. R.; Yu, Y. Z. Weakly supervised complementary parts models for fine-grained image classification from the bottom up. arXiv preprint arXiv:1903.02827, 2019.\n\nDu, R. Y.; Chang, D. L.; Bhunia, A. K.; Xie, J. Y.; Ma, Z. Y.; Song, Y. Z.; Guo, J. Fine-grained visual classification via progressive multi-granularity training of jigsaw patches. arXiv preprint arXiv:2003.03836, 2020.\n\nZheng, H.; Fu, J.; Mei, T.; Luo, J. Learning multi-attention convolutional neural network for fine-grained image recognition. In: Proceedings of the IEEE International Conference on Computer Vision, 5219\u20135227, 2017.\n\nFu, J.; Zheng, H.; Mei, T. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4476\u20134484, 2017.\n\nZheng, H.; Fu, J.; Zha, Z.; Luo, J.; Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5012\u20135021, 2019.\n\nZhang, F.; Li, M.; Zhai, G.; Liu, Y. Three-branch and multi-scale learning for fine-grained image recognition (TBMSL-Net). arXiv preprint arXiv:2003.09150, 2020.\n\nSun, G. L.; Cholakkal, H.; Khan, S.; Khan, F. S.; Shao, L. Fine-grained recognition: Accounting for subtle differences between similar classes. arXiv preprint arXiv:1912.06842, 2019.\n\nLin, T.-Y.; RoyChowdhury, A.; Maji, S. Bilinear CNN models for fine-grained visual recognition. In: Proceedings of the IEEE international conference on computer vision, 1449\u20131457, 2015.\n\nGao, Y.; Beijbom, O.; Zhang, N.; Darrell, T. Compact bilinear pooling. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 317\u2013326, 2016.\n\nYu, C.; Zhao, X.; Zheng, Q.; Zhang, P.; You, X. Hierarchical bilinear pooling for fine-grained visual recognition. In: Computer Vision-ECCV 2018. Lecture Notes in Computer Science Vol. 11220. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham, 595\u2013610, 2018.\n\nWang, Y.; Choi, J.; Morariu, V. I.; Davis, L. S. Mining discriminative triplets of patches for fine-grained classification. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1163\u20131172, 2016.\n\nZhang, X.; Zhou, F.; Lin, Y.; Zhang, S. Embedding label structures for finegrained feature representation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1114\u20131123, 2016.\n\nDubey, A.; Gupta, O.; Raskar, R.; Naik, N. Maximum-entropy fine grained classification. arXiv preprint arXiv:1809.05934, 2018.\n\nQian, Q.; Jin, R.; Zhu, S.; Lin, Y. Fine-grained visual categorization via multi-stage metric learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3716\u20133724, 2015.\n\nSun, M.; Yuan, Y.; Zhou, F.; Ding, E. Multi-attention multi-class constraint for fine-grained image recognition. In: Computer Vision-ECCV 2018. Lecture Notes in Computer Science Vol. 11220. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham, 834\u2013850, 2018.\n\nDubey, A.; Gupta, O.; Guo, P.; Raskar, R.; Farrell, R.; Naik, N. Pairwise confusion for fine-grained visual classification. In: Computer Vision-ECCV 2018. Lecture Notes in Computer Science Vol. 11216. Ferrari, V.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham, 71\u201388, 2018.\n\nZhuang, P.; Wang, Y.; Qiao, Y. Learning attentive pairwise interaction for fine-grained classification. arXiv preprint arXiv:2002.10191, 2020.\n\nXu, Z.; Huang, S.; Zhang, Y.; Tao, D. Augmenting strong supervision using web data for finegrained categorization. In: Proceedings of the IEEE International Conference on Computer Vision, 2524\u20132532, 2015.\n\nNiu, L.; Veeraraghavan, A.; Sabharwal, A. Finegrained classification using heterogeneous web data and auxiliary categories. arXiv preprint arXiv:1811.07567, 2018.\n\nTorralba, A.; Efros, A. A. Unbiased look at dataset bias. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1521\u20131528, 2011.\n\nHu, T.; Qi, H. G.; Huang, Q. M.; Lu, Y. See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification. arXiv preprint arXiv:1901.09891, 2019.\n\nKrause, J.; Stark, M.; Deng, J.; L. Fei-Fei. 3D object representations for fine-grained categorization. In: Proceedings of the IEEE International Conference on Computer Vision Workshops, 554\u2013561, 2013.\n\nMaji, S.; Rahtu, E.; Kannala, J.; Blaschko, M.; Vedaldi, A. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n\nNilsback, M.; Zisserman, A. Automated flower classification over a large number of classes. In: Proceedings of the 6th Indian Conference on Computer Vision, Graphics & Image Processing, 722\u2013729, 2008.\n\nDeng, J.; Dong, W.; Socher, R.; Li, L.; Li, K.; Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 248\u2013255, 2009.\n\nEveringham, M.; van Gool, L.; Williams, C. K. I.; Winn, J.; Zisserman, A. The pascal visual object classes (VOC) challenge. International Journal of Computer-Vision Vol. 88, No. 2, 303\u2013338, 2010.\n\nLin, T.; Maire, M.; Belongie, S.; Bourdev, L.; Girshick, R.; Hays, J.; Perona, P.; Ramanan, D.; Zitnick, C. L.; Doll\u00e1r, P. Microsoft COCO: Common objects in context. arXiv preprint arXiv:1405.0312, 2014.\n\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; Simoncelli, E. P. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing Vol. 13, No. 4, 600\u2013612, 2004.\n\nRussell, B. C.; Torralba, A.; Murphy, K. P.; Freeman, W. T. LabelMe: A database and web-based tool for image annotation. International Journal of Computer-Vision Vol. 77, Nos. 1\u20133, 157\u2013173, 2008."}, {"url": "https://link.springer.com/article/10.1007/s41095-021-0240-x", "page_content": "Meng-Hao Guo is a Ph.D. candidate supervised by Prof. Shi-Min Hu in the Department of Computer Science and Technology at Tsinghua University, Beijing, China. His research interests include computer graphics, computer vision, and machine learning.\n\nZheng-Ning Liu received his bachelor degree in computer science from Tsinghua University in 2017. He is currently a Ph.D. candidate in computer science at Tsinghua University. His research interests include 3D computer vision, 3D reconstruction, and computer graphics.\n\nTai-Jiang Mu is currently an assistant researcher at Tsinghua University, where he received his B.S. and Ph.D. degrees in computer science in 2011 and 2016, respectively. His research interests include computer vision, robotics, and computer graphics.\n\nDun Liang is a Ph.D. candidate in computer science at Tsinghua University, where he received his B.S. degree in 2016. His research interests include computer graphics, visual media learning, and high-performance computing.\n\nRalph R. Martin received his Ph.D. degree from Cambridge University in 1983. He is an emeritus professor of Cardiff University with over 300 papers and 14 books in geometry processing, computer graphics, and computer vision. He is a sometime Fellow of the Learned Society of Wales, the Institute of Mathematics and its Applications, and the British Computer Society. He is currently an Associate Editor-in-Chief of Computational Visual Media.\n\nShi-Min Hu is currently a professor in computer science at Tsinghua University. He received his Ph.D. degree from Zhejiang University in 1996. His research interests include geometry processing, image & video processing, rendering, computer animation, and CAD. He has published more than 100 papers in journals and refereed conferences. He is Editor-in-Chief of Computational Visual Media, and on the editorial boards of several journals, including Computer Aided Design and Computer & Graphics."}, {"url": "https://profiles.stanford.edu/fei-fei-li", "page_content": "Klionsky, D. J., Abdalla, F. C., Abeliovich, H., Abraham, R. T., Acevedo-Arozena, A., Adeli, K., Agholme, L., Agnello, M., Agostinis, P., Aguirre-Ghiso, J. A., Ahn, H. J., Ait-Mohamed, O., Ait-Si-Ali, S., Akematsu, T., Akira, S., Al-Younes, H. M., Al-Zeer, M. A., Albert, M. L., Albin, R. L., Alegre-Abarrategui, J., Aleo, M. F., Alirezaei, M., Almasan, A., Almonte-Becerril, M., Amano, A., Amaravadi, R., Amarnath, S., Amer, A. O., Andrieu-Abadie, N., Anantharam, V., Ann, D. K., Anoopkumar-Dukie, S., Aoki, H., Apostolova, N., Arancia, G., Aris, J. P., Asanuma, K., Asare, N. Y., Ashida, H., Askanas, V., Askew, D. S., Auberger, P., Baba, M., Backues, S. K., Baehrecke, E. H., Bahr, B. A., Bai, X., Bailly, Y., Baiocchi, R., Baldini, G., Balduini, W., Ballabio, A., Bamber, B. A., Bampton, E. T., Banhegyi, G., Bartholomew, C. R., Bassham, D. C., Bast, R. C., Batoko, H., Bay, B., Beau, I., Bechet, D. M., Begley, T. J., Behl, C., Behrends, C., Bekri, S., Bellaire, B., Bendall, L. J., Benetti, L., Berliocchi, L., Bernardi, H., Bernassola, F., Besteiro, S., Bhatia-Kissova, I., Bi, X., Biard-Piechaczyk, M., Blum, J. S., Boise, L. H., Bonaldo, P., Boone, D. L., Bornhauser, B. C., Bortoluci, K. R., Bossis, I., Bost, F., Bourquin, J., Boya, P., Boyer-Guittaut, M., Bozhkov, P. V., Brady, N. R., Brancolini, C., Brech, A., Brenman, J. E., Brennand, A., Bresnick, E. H., Brest, P., Bridges, D., Bristol, M. L., Brookes, P. S., Brown, E. J., Brumell, J. H., Brunetti-Pierri, N., Brunk, U. T., Bulman, D. E., Bultman, S. J., Bultynck, G., Burbulla, L. F., Bursch, W., Butchar, J. P., Buzgariu, W., Bydlowski, S. P., Cadwell, K., Cahova, M., Cai, D., Cai, J., Cai, Q., Calabretta, B., Calvo-Garrido, J., Camougrand, N., Campanella, M., Campos-Salinas, J., Candi, E., Cao, L., Caplan, A. B., Carding, S. R., Cardoso, S. M., Carew, J. S., Carlin, C. R., Carmignac, V., Carneiro, L. A., Carra, S., Caruso, R. A., Casari, G., Casas, C., Castino, R., Cebollero, E., Cecconi, F., Celli, J., Chaachouay, H., Chae, H., Chai, C., Chan, D. C., Chan, E. Y., Chang, R. C., Che, C., Chen, C., Chen, G., Chen, G., Chen, M., Chen, Q., Chen, S. S., Chen, W., Chen, X., Chen, X., Chen, X., Chen, Y., Chen, Y., Chen, Y., Chen, Y., Chen, Z., Cheng, A., Cheng, C. H., Cheng, Y., Cheong, H., Cheong, J., Cherry, S., Chess-Williams, R., Cheung, Z. H., Chevet, E., Chiang, H., Chiarelli, R., Chiba, T., Chin, L., Chiou, S., Chisari, F. V., Cho, C. H., Cho, D., Choi, A. M., Choi, D., Choi, K. S., Choi, M. E., Chouaib, S., Choubey, D., Choubey, V., Chu, C. T., Chuang, T., Chueh, S., Chun, T., Chwae, Y., Chye, M., Ciarcia, R., Ciriolo, M. R., Clague, M. J., Clark, R. S., Clarke, P. G., Clarke, R., Codogno, P., Coller, H. A., Colombo, M. I., Comincini, S., Condello, M., Condorelli, F., Cookson, M. R., Coppens, G. H., Corbalan, R., Cossart, P., Costelli, P., Costes, S., Coto-Montes, A., Couve, E., Coxon, F. P., Cregg, J. M., Crespo, J. L., Cronje, M. J., Cuervo, A. M., Cullen, J. J., Czaja, M. J., D'Amelio, M., Darfeuille-Michaud, A., Davids, L. M., Davies, F. E., De Felici, M., de Groot, J. F., de Haan, C. A., De Martino, L., De Milito, A., De Tata, V., Debnath, J., Degterev, A., Dehay, B., Delbridge, L. M., Demarchi, F., Deng, Y. Z., Dengjel, J., Dent, P., Denton, D., Deretic, V., Desai, S. D., Devenish, R. J., Di Gioacchino, M., Di Paolo, G., Di Pietro, C., Diaz-Araya, G., Diaz-Laviada, I., Diaz-Meco, M. T., Diaz-Nido, J., Dikic, I., Dinesh-Kumar, S. P., Ding, W., Distelhorst, C. W., Diwan, A., Djavaheri-Mergny, M., Dokudovskaya, S., Dong, Z., Dorsey, F. C., Dosenko, V., Dowling, J. J., Doxsey, S., Dreux, M., Drew, M. E., Duan, Q., Duchosal, M. A., Duff, K., Dugail, I., Durbeej, M., Duszenko, M., Edelstein, C. L., Edinger, A. L., Egea, G., Eichinger, L., Eissa, N. T., Ekmekcioglu, S., El-Deiry, W. S., Elazar, Z., Elgendy, M., Ellerby, L. M., Eng, K. E., Engelbrecht, A., Engelender, S., Erenpreisa, J., Escalante, R., Esclatine, A., Eskelinen, E., Espert, L., Espina, V., Fan, H., Fan, J., Fan, Q., Fan, Z., Fang, S., Fang, Y., Fanto, M., Fanzani, A., Farkas, T., Farre, J., Faure, M., Fechheimer, M., Feng, C. G., Feng, J., Feng, Q., Feng, Y., Fesues, L., Feuer, R., Figueiredo-Pereira, M. E., Fimia, G. M., Fingar, D. C., Finkbeiner, S., Finkel, T., Finley, K. D., Fiorito, F., Fisher, E. A., Fisher, P. B., Flajolet, M., Florez-McClure, M. L., Florio, S., Fon, E. A., Fornai, F., Fortunato, F., Fotedar, R., Fowler, D. H., Fox, H. S., Franco, R., Frankel, L. B., Fransen, M., Fuentes, J. M., Fueyo, J., Fujii, J., Fujisaki, K., Fujita, E., Fukuda, M., Furukawa, R. H., Gaestel, M., Gailly, P., Gajewska, M., Galliot, B., Galy, V., Ganesh, S., Ganetzky, B., Ganley, I. G., Gao, F., Gao, G. F., Gao, J., Garcia, L., Garcia-Manero, G., Garcia-Marcos, M., Garmyn, M., Gartel, A. L., Gatti, E., Gautel, M., Gawriluk, T. R., Gegg, M. E., Geng, J., Germain, M., Gestwicki, J. E., Gewirtz, D. A., Ghavami, S., Ghosh, P., Giammarioli, A. M., Giatromanolaki, A. N., Gibson, S. B., Gilkerson, R. W., Ginger, M. L., Ginsberg, H. N., Golab, J., Goligorsky, M. S., Golstein, P., Gomez-Manzano, C., Goncu, E., Gongora, C., Gonzalez, C. D., Gonzalez, R., Gonzalez-Estevez, C., Gonzalez-Polo, R. A., Gonzalez-Rey, E., Gorbunov, N. V., Gorski, S., Goruppi, S., Gottlieb, R. A., Gozuacik, D., Granato, G. E., Grant, G. D., Green, K. N., Gregorc, A., Gros, F., Grose, C., Grunt, T. W., Gual, P., Guan, J., Guan, K., Guichard, S. M., Gukovskaya, A. S., Gukovsky, I., Gunst, J., Gustafsson, A. B., Halayko, A. J., Hale, A. N., Halonen, S. K., Hamasaki, M., Han, F., Han, T., Hancock, M. K., Hansen, M., Harada, H., Harada, M., Hardt, S. E., Harper, J. W., Harris, A. L., Harris, J., Harris, S. D., Hashimoto, M., Haspel, J. A., Hayashi, S., Hazelhurst, L. A., He, C., He, Y., Hebert, M., Heidenreich, K. A., Helfrich, M. H., Helgason, G. V., Henske, E. P., Herman, B., Herman, P. K., Hetz, C., Hilfiker, S., Hill, J. A., Hocking, L. J., Hofman, P., Hofmann, T. G., Hoehfeld, J., Holyoake, T. L., Hong, M., Hood, D. A., Hotamisligil, G. S., Houwerzijl, E. J., Hoyer-Hansen, M., Hu, B., Hu, C. A., Hu, H., Hua, Y., Huang, C., Huang, J., Huang, S., Huang, W., Huber, T. B., Huh, W., Hung, T., Hupp, T. R., Hur, G. M., Hurley, J. B., Hussain, S. N., Hussey, P. J., Hwang, J. j., Hwang, S., Ichihara, A., Ilkhanizadeh, S., Inoki, K., Into, T., Iovane, V., Iovanna, J. L., Ip, N. Y., Isaka, Y., Ishida, H., Isidoro, C., Isobe, K., Iwasaki, A., Izquierdo, M., Izumi, Y., Jaakkola, P. M., Jaattela, M., Jackson, G. R., Jackson, W. T., Janji, B., Jendrach, M., Jeon, J., Jeung, E., Jiang, H., Jiang, H., Jiang, J. X., Jiang, M., Jiang, Q., Jiang, X., Jiang, X., Jimenez, A., Jin, M., Jin, S., Joe, C. O., Johansen, T., Johnson, D. E., Johnson, G. V., Jones, N. L., Joseph, B., Joseph, S. K., Joubert, A. M., Juhasz, G., Juillerat-Jeanneret, L., Jung, C. H., Jung, Y., Kaarniranta, K., Kaasik, A., Kabuta, T., Kadowaki, M., Kagedal, K., Kamada, Y., Kaminskyy, V. O., Kampinga, H. H., Kanamori, H., Kang, C., Kang, K. B., Kang, K. I., Kang, R., Kang, Y., Kanki, T., Kanneganti, T., Kanno, H., Kanthasamy, A. G., Kanthasamy, A., Karantza, V., Kaushal, G. P., Kaushik, S., Kawazoe, Y., Ke, P., Kehrl, J. H., Kelekar, A., Kerkhoff, C., Kessel, D. H., Khalil, H., Kiel, J. A., Kiger, A. A., Kihara, A., Kim, D. R., Kim, D., Kim, D., Kim, E., Kim, H., Kim, J., Kim, J. H., Kim, J. C., Kim, J. K., Kim, P. K., Kim, S. W., Kim, Y., Kim, Y., Kimchi, A., Kimmelman, A. C., King, J. S., Kinsella, T. J., Kirkin, V., Kirshenbaum, L. A., Kitamoto, K., Kitazato, K., Klein, L., Klimecki, W. T., Klucken, J., Knecht, E., Ko, B. C., Koch, J. C., Koga, H., Koh, J., Koh, Y. H., Koike, M., Komatsu, M., Kominami, E., Kong, H. J., Kong, W., Korolchuk, V. I., Kotake, Y., Koukourakis, M. I., Flores, J. B., Kovacs, A. L., Kraft, C., Krainc, D., Kraemer, H., Kretz-Remy, C., Krichevsky, A. M., Kroemer, G., Krueger, R., Krut, O., Ktistakis, N. T., Kuan, C., Kucharczyk, R., Kumar, A., Kumar, R., Kumar, S., Kundu, M., Kung, H., Kurz, T., Kwon, H. J., La Spada, A. R., Lafont, F., Lamark, T., Landry, J., Lane, J. D., Lapaquette, P., Laporte, J. F., Laszlo, L., Lavandero, S., Lavoie, J. N., Layfield, R., Lazo, P. A., Le, W., Le Cam, L., Ledbetter, D. J., Lee, A. J., Lee, B., Lee, G. M., Lee, J., Lee, J., Lee, M., Lee, M., Lee, S. H., Leeuwenburgh, C., Legembre, P., Legouis, R., Lehmann, M., Lei, H., Lei, Q., Leib, D. A., Leiro, J., Lemasters, J. J., Lemoine, A., Lesniak, M. S., Lev, D., Levenson, V. V., Levine, B., Levy, E., Li, F., Li, J., Li, L., Li, S., Li, W., Li, X., Li, Y., Li, Y., Liang, C., Liang, Q., Liao, Y., Liberski, P. P., Lieberman, A., Lim, H. J., Lim, K., Lim, K., Lin, C., Lin, F., Lin, J., Lin, J. D., Lin, K., Lin, W., Lin, W., Lin, Y., Linden, R., Lingor, P., Lippincott-Schwartz, J., Lisanti, M. P., Liton, P. B., Liu, B., Liu, C., Liu, K., Liu, L., Liu, Q. A., Liu, W., Liu, Y., Liu, Y., Lockshin, R. A., Lok, C., Lonial, S., Loos, B., Lopez-Berestein, G., Lopez-Otin, C., Lossi, L., Lotze, M. T., Low, P., Lu, B., Lu, B., Lu, B., Lu, Z., Luciano, F., Lukacs, N. W., Lund, A. H., Lynch-Day, M. A., Ma, Y., Macian, F., MacKeigan, J. P., Macleod, K. F., Madeo, F., Maiuri, L., Maiuri, M. C., Malagoli, D., Malicdan, M. C., Malorni, W., Man, N., Mandelkow, E., Manon, S., Manov, I., Mao, K., Mao, X., Mao, Z., Marambaud, P., Marazziti, D., Marcel, Y. L., Marchbank, K., Marchetti, P., Marciniak, S. J., Marcondes, M., Mardi, M., Marfe, G., Marino, G., Markaki, M., Marten, M. R., Martin, S. J., Martinand-Mari, C., Martinet, W., Martinez-Vicente, M., Masini, M., Matarrese, P., Matsuo, S., Matteoni, R., Mayer, A., Mazure, N. M., Mcconkey, D. J., McConnell, M. J., McDermott, C., McDonald, C., McInerney, G. M., McKenna, S. L., McLaughlin, B., McLean, P. J., McMaster, C. R., McQuibban, G. A., Meijer, A. J., Meisler, M. H., Melendez, A., Melia, T. J., Melino, G., Mena, M. A., Menendez, J. A., Menna-Barreto, R. F., Menon, M. B., Menzies, F. M., Mercer, C. A., Merighi, A., Merry, D. E., Meschini, S., Meyer, C. G., Meyer, T. F., Miao, C., Miao, J., Michels, P. A., Michiels, C., Mijaljica, D., Milojkovic, A., Minucci, S., Miracco, C., Miranti, C. K., Mitroulis, I., Miyazawa, K., Mizushima, N., Mograbi, B., Mohseni, S., Molero, X., Mollereau, B., Mollinedo, F., Momoi, T., Monastyrska, I., Monick, M. M., Monteiro, M. J., Moore, M. N., Mora, R., Moreau, K., Moreira, P. I., Moriyasu, Y., Moscat, J., Mostowy, S., Mottram, J. C., Motyl, T., Moussa, C. E., Mueller, S., Muenger, K., Muenz, C., Murphy, L. O., Murphy, M. E., Musaro, A., Mysorekar, I., Nagata, E., Nagata, K., Nahimana, A., Nair, U., Nakagawa, T., Nakahira, K., Nakano, H., Nakataogawa, H., Nanjundan, M., Naqvi, N. I., Narendra, D. P., Narita, M., Navarro, M., Nawrocki, S. T., Nazarko, T. Y., Nemchenko, A., Netea, M. G., Neufeld, T. P., Ney, P. A., Nezis, I. P., Huu Phuc Nguyen, H. P., Nie, D., Nishino, I., Nislow, C., Nixon, R. A., Noda, T., Noegel, A. A., Nogalska, A., Noguchi, S., Notterpek, L., Novak, I., Nozaki, T., Nukina, N., Nuernberger, T., Nyfeler, B., Obara, K., Oberley, T. D., Oddo, S., Ogawa, M., Ohashi, T., Okamoto, K., Oleinick, N. L., Oliver, F. J., Olsen, L. J., Olsson, S., Opota, O., Osborne, T. F., Ostrander, G. K., Otsu, K., Ou, J. J., Ouimet, M., Overholtzer, M., Ozpolat, B., Paganetti, P., Pagnini, U., Pallet, N., Palmer, G. E., Palumbo, C., Pan, T., Panaretakis, T., Pandey, U. B., Papackova, Z., Papassideri, I., Paris, I., Park, J., Park, O. K., Parys, J. B., Parzych, K. R., Patschan, S., Patterson, C., Pattingre, S., Pawelek, J. M., Peng, J., Perlmutter, D. H., Perrotta, I., Perry, G., Pervaiz, S., Peter, M., Peters, G. J., Petersen, M., Petrovski, G., Phang, J. M., Piacentini, M., Pierre, P., Pierrefite-Carle, V., Pierron, G., Pinkas-Kramarski, R., Piras, A., Piri, N., Platanias, L. C., Poeggeler, S., Poirot, M., Poletti, A., Poues, C., Pozuelo-Rubio, M., Praetorius-Ibba, M., Prasad, A., Prescott, M., Priault, M., Produit-Zengaffinen, N., Progulske-Fox, A., Proikas-Cezanne, T., Przedborski, S., Przyklenk, K., Puertollano, R., Puyal, J., Qian, S., Qin, L., Qin, Z., Quaggin, S. E., Raben, N., Rabinowich, H., Rabkin, S. W., Rahman, I., Rami, A., Ramm, G., Randall, G., Randow, F., Rao, V. A., Rathmell, J. C., Ravikumar, B., Ray, S. K., Reed, B. H., Reed, J. C., Reggiori, F., Regnier-Vigouroux, A., Reichert, A. S., Reiners, J. J., Reiter, R. J., Ren, J., Revuelta, J. L., Rhodes, C. J., Ritis, K., Rizzo, E., Robbins, J., Roberge, M., Roca, H., Roccheri, M. C., Rocchi, S., Rodemann, H. P., de Cordoba, S. R., Rohrer, B., Roninson, I. B., Rosen, K., Rost-Roszkowska, M. M., Rouis, M., Rouschop, K. M., Rovetta, F., Rubin, B. P., Rubinsztein, D. C., Ruckdeschel, K., Rucker, E. B., Rudich, A., Rudolf, E., Ruiz-Opazo, N., Russo, R., Rusten, T. E., Ryan, K. M., Ryter, S. W., Sabatini, D. M., Sadoshima, J., Saha, T., Saitoh, T., Sakagami, H., Sakai, Y., Salekdeh, G. H., Salomoni, P., Salvaterra, P. M., Salvesen, G., Salvioli, R., Sanchez, A. M., Sanchez-Alcazar, J. A., Sanchez-Prieto, R., Sandri, M., Sankar, U., Sansanwal, P., Santambrogio, L., Saran, S., Sarkar, S., Sarwal, M., Sasakawa, C., Sasnauskiene, A., Sass, M., Sato, K., Sato, M., Schapira, A. H., Scharl, M., Schaetzl, H. M., Scheper, W., Schiaffino, S., Schneider, C., Schneider, M. E., Schneider-Stock, R., Schoenlein, P. V., Schorderet, D. F., Schueller, C., Schwartz, G. K., Scorrano, L., Sealy, L., Seglen, P. O., Segura-Aguilar, J., Seiliez, I., Seleverstov, O., Sell, C., Seo, J. B., Separovic, D., Setaluri, V., Setoguchi, T., Settembre, C., Shacka, J. J., Shanmugam, M., Shapiro, I. M., Shaulian, E., Shaw, R. J., Shelhamer, J. H., Shen, H., Shen, W., Sheng, Z., Shi, Y., Shibuya, K., Shidoji, Y., Shieh, J., Shih, C., Shimada, Y., Shimizu, S., Shintani, T., Shirihai, O. S., Shore, G. C., Sibirny, A. A., Sidhu, S. B., Sikorska, B., Silva-Zacarin, E. C., Simmons, A., Simon, A. K., Simon, H., Simone, C., Simonsen, A., Sinclair, D. A., Singh, R., Sinha, D., Sinicrope, F. A., Sirko, A., Siu, P. M., Sivridis, E., Skop, V., Skulachev, V. P., Slack, R. S., Smaili, S. S., Smith, D. R., Soengas, M. S., Soldati, T., Song, X., Sood, A. K., Soong, T. W., Sotgia, F., Spector, S. A., Spies, C. D., Springer, W., Srinivasula, S. M., Stefanis, L., Steffan, J. S., Stendel, R., Stenmark, H., Stephanou, A., Stern, S. T., Sternberg, C., Stork, B., Stralfors, P., Subauste, C. S., Sui, X., Sulzer, D., Sun, J., Sun, S., Sun, Z., Sung, J. J., Suzuki, K., Suzuki, T., Swanson, M. S., Swanton, C., Sweeney, S. T., Sy, L., Szabadkai, G., Tabas, I., Taegtmeyer, H., Tafani, M., Takacs-Vellai, K., Takano, Y., Takegawa, K., Takemura, G., Takeshita, F., Talbot, N. J., Tan, K. S., Tanaka, K., Tanaka, K., Tang, D., Tang, D., Tanida, I., Tannous, B. A., Tavernarakis, N., Taylor, G. S., Taylor, G. A., Taylor, J. P., Terada, L. S., Terman, A., Tettamanti, G., Thevissen, K., Thompson, C. B., Thorburn, A., Thumm, M., Tian, F., Tian, Y., Tocchini-Valentini, G., Tolkovsky, A. M., Tomino, Y., Toenges, L., Tooze, S. A., Tournier, C., Tower, J., Towns, R., Trajkovic, V., Travassos, L. H., Tsai, T., Tschan, M. P., Tsubata, T., Tsung, A., Turk, B., Turner, L. S., Tyagi, S. C., Uchiyama, Y., Ueno, T., Umekawa, M., Umemiya-Shirafuji, R., Unni, V. K., Vaccaro, M. I., Valente, E. M., Van den Berghe, G., van der Klei, I. J., van Doorn, W. G., van Dyk, L. F., van Egmond, M., van Grunsven, L. A., Vandenabeele, P., Vandenberghe, W. P., Vanhorebeek, I., Vaquero, E. C., Velasco, G., Vellai, T., Vicencio, J. M., Vierstra, R. D., Vila, M., Vindis, C., Viola, G., Viscomi, M. T., Voitsekhovskaja, O. V., von Haefen, C., Votruba, M., Wada, K., Wade-Martins, R., Walker, C. L., Walsh, C. M., Walter, J., Wan, X., Wang, A., Wang, C., Wang, D., Wang, F., Wang, F., Wang, G., Wang, H., Wang, H., Wang, H., Wang, J., Wang, K., Wang, M., Wang, R. C., Wang, X., Wang, X., Wang, Y., Wang, Y., Wang, Z., Wang, Z. C., Wang, Z., Wansink, D. G., Ward, D. M., Watada, H., Waters, S. L., Webster, P., Wei, L., Weihl, C. C., Weiss, W. A., Welford, S. M., Wen, L., Whitehouse, C. A., Whitton, J. L., Whitworth, A. J., Wileman, T., Wiley, J. W., Wilkinson, S., Willbold, D., Williams, R. L., Williamson, P. R., Wouters, B. G., Wu, C., Wu, D., Wu, W. K., Wyttenbach, A., Xavier, R. J., Xi, Z., Xia, P., Xiao, G., Xie, Z., Xie, Z., Xu, D., Xu, J., Xu, L., Xu, X., Yamamoto, A., Yamamoto, A., Yamashina, S., Yamashita, M., Yan, X., Yanagida, M., Yang, D., Yang, E., Yang, J., Yang, S. Y., Yang, W., Yang, W. Y., Yang, Z., Yao, M., Yao, T., Yeganeh, B., Yen, W., Yin, J., Yin, X., Yoo, O., Yoon, G., Yoon, S., Yorimitsu, T., Yoshikawa, Y., Yoshimori, T., Yoshimoto, K., You, H. J., Youle, R. J., Younes, A., Yu, L., Yu, L., Yu, S., Yu, W. H., Yuan, Z., Yue, Z., Yun, C., Yuzaki, M., Zabirnyk, O., Silva-Zacarin, E., Zacks, D., Zacksenhaus, E., Zaffaroni, N., Zakeri, Z., Zeh, H. J., Zeitlin, S. O., Zhang, H., Zhang, H., Zhang, J., Zhang, J., Zhang, L., Zhang, L., Zhang, M., Zhang, X. D., Zhao, M., Zhao, Y., Zhao, Y., Zhao, Z. J., Zheng, X., Zhivotovsky, B., Zhong, Q., Zhou, C., Zhu, C., Zhu, W., Zhu, X., Zhu, X., Zhu, Y., Zoladek, T., Zong, W., Zorzano, A., Zschocke, J., Zuckerbraun, B.\n\nAbstract\n\nIn 2008 we published the first set of guidelines for standardizing research in autophagy. Since then, research on this topic has continued to accelerate, and many new scientists have entered the field. Our knowledge base and relevant new technologies have also been expanding. Accordingly, it is important to update these guidelines for monitoring autophagy in different organisms. Various reviews have described the range of assays that have been used for this purpose. Nevertheless, there continues to be confusion regarding acceptable methods to measure autophagy, especially in multicellular eukaryotes. A key point that needs to be emphasized is that there is a difference between measurements that monitor the numbers or volume of autophagic elements (e.g., autophagosomes or autolysosomes) at any stage of the autophagic process vs. those that measure flux through the autophagy pathway (i.e., the complete process); thus, a block in macroautophagy that results in autophagosome accumulation needs to be differentiated from stimuli that result in increased autophagic activity, defined as increased autophagy induction coupled with increased delivery to, and degradation within, lysosomes (in most higher eukaryotes and some protists such as Dictyostelium) or the vacuole (in plants and fungi). In other words, it is especially important that investigators new to the field understand that the appearance of more autophagosomes does not necessarily equate with more autophagy. In fact, in many cases, autophagosomes accumulate because of a block in trafficking to lysosomes without a concomitant change in autophagosome biogenesis, whereas an increase in autolysosomes may reflect a reduction in degradative activity. Here, we present a set of guidelines for the selection and interpretation of methods for use by investigators who aim to examine macroautophagy and related processes, as well as for reviewers who need to provide realistic and reasonable critiques of papers that are focused on these processes. These guidelines are not meant to be a formulaic set of rules, because the appropriate assays depend in part on the question being asked and the system being used. In addition, we emphasize that no individual assay is guaranteed to be the most appropriate one in every situation, and we strongly recommend the use of multiple assays to monitor autophagy. In these guidelines, we consider these various methods of assessing autophagy and what information can, or cannot, be obtained from them. Finally, by discussing the merits and limits of particular autophagy assays, we hope to encourage technical innovation in the field.\n\nView details for DOI 10.4161/auto.19496\n\nView details for Web of Science ID 000305403400002\n\nView details for PubMedID 22966490\n\nView details for PubMedCentralID PMC3404883"}, {"url": "https://prabook.com/web/fei-ping.hsu/2433004", "page_content": "Fei-Ping Hsu\n\npianist Christian pastor\n\n1952 (age 49)\n\nThe family realized his tremendous talent and encouraged him to study music When he was eight years old, he had a rare opportunity to meet Professor Ji-sen Fan, the head of the piano department of the Shanghai Conservatory, and was recognized as a child prodigy. By the time he was twelve-years-old, he already played the complete Chopin Etudes and had performed with the Shanghai Philharmonic. Feiping was invited to perform for the Queen Elisabeth of Belgium, who was so impressed with his ability that she personally invited Feiping to study and perform in Europe under her sponsorship. Unfortunately, he was not allowed to accept this invitation because of the advent of the Cultural Revolution during the 1960s. However, he survived the labor camps and became well known in China and toured extensively throughout the Far East as a soloist with the Central Philharmonic which was the foremost leading orchestra. He also garnered top prizes in other international competitions including the Gina Bachauer Memorial Piano Scholarship Competition, the University of Maryland International Competition, and the Paloma O\u2019Shea International Competition (Spain). Mr. Hsu made his New York recital debut at Alice Tully Hall in 1983 and performed throughout the United States, including notable appearances at Carnegie Hall, Lincoln Center, and the Kennedy Center in Washington, District of Columbia He also frequently toured Europe, South America, and the Far East. Mr. Hsu was especially noted for his larger-than-life virtuosic style of Horowitzian pianism. In 1979, Fei-Ping Hsu received permission to study in the United States. Among others he attended the Juilliard School of Music under the tutelage of Sascha Gorodnitzki. He had given numerous concerts in Germany, France, Andorra, Switzerland, and Italy. He has appeared as a soloist with major orchestra including the Montreal Symphony Orchestra under the baton of Maestro Charles Dutoit, the Moscow Philharmonic in Russia, the Finnish Tempere Symphony Orchestra in Europe, as well as numerous orchestra in the United States performing in various concert halls and music festivals. Mr. Hsu was also actively engaged in his native Asia, where his collaborations include appearances with the China National Symphony Orchestra in Beijing, the Shanghai Symphony Orchestra, the Shanghai Radio Symphony Orchestra and the Kyushu Symphony Orchestra in Japan. Since the early 1970s Mr. Hsu had recorded for major labels such as the Columbia Records, the Radio Corporation of America Victor Records, the Hugo Records, the Republic of Ireland Productions, and the Master of Arts Recordings in Japan. His life was cut short in a road accident while on a concert tour in China on November 27, 2001 and is remembered by many to this day."}]}