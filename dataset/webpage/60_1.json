{"Pattern members are Andrew Thiboldeaux (vocals, guitar, drums, percussion, keyboards) and Chris Ward (vocals, bass, guitar, keyboards).": [{"url": "https://patternismovement.bandcamp.com/album/suckling-untitled-how-does-it-feel", "page_content": "Digital Album Streaming + Download Includes high-quality download in MP3, FLAC and more. Paying supporters also get unlimited streaming via the free Bandcamp app. Purchasable with gift card Buy Digital Album name your price Send as Gift\n\nPattern Is Movement \"Suckling / Untitled (How Does It Feel?)\" Limited Edition 12\" Record/Vinyl + Digital Album Limited to 500 hand-numbered copies, this 12\" is pressed on transparent green vinyl and is packaged in a full-color 20pt. matte-finish jacket. Features \"Suckling\" from their 2013 self-titled album PLUS the band's classic version of D'Angelo's \"Untitled (How Does It Feel?)\". Includes a high quality digital download (MP3, FLAC). Featuring art by Adam Heathcott of Hometapes.\n\n\n\nIncludes unlimited streaming of Suckling / Untitled (How Does It Feel?) via the free Bandcamp app, plus high-quality download in MP3, FLAC and more. Sold Out\n\n\n\nShare / Embed\n\nabout\n\nFounded in Philadelphia by Andrew Thiboldeaux and Christopher Ward, Pattern Is Movement and its twelve year history map a vast territory of internal exploration and external output. As the band recorded a series of albums over the past decade \u2014 The (Im)possibility of Longing, Stowaway, and All Together \u2014 they also shifted in membership and in stylistic focus, eventually solidifying into a powerful duo: Thiboldeaux on Rhodes, synth, bass, and vocals, and Ward on drums. Deeply soulful and natively genre-defying, they toured extensively, joining bands like St. Vincent, The Roots, and Shudder to Think, before delving into their fourth album and, along with it, the brightest and darkest corners of composition, orchestration, and collaboration. Their first reveal is a limited edition 12\" and digital single, featuring the new song \"Suckling\" backed with the long-awaited studio recording of fan favorite \"Untitled (How Does It Feel?),\" Pattern Is Movement's sweltering cover of D'Angelo.\n\ncredits\n\nreleased October 8, 2013\n\n\n\nPattern Is Movement is\n\nAndrew Thiboldeaux - vocals, keyboards, bass, percussion\n\nChristopher Ward - drums, percussion\n\n\n\nProduced by Pattern Is Movement and David Downham\n\nRecorded at The Gradwell House\n\nEngineered by David Downham\n\n\n\nSuckling (3:20)\n\nWritten and performed by Pattern Is Movement\n\nTrumpet by Josh Anderson\n\nMixed at The Gradwell House and Sigma Sound\n\nMixed by David Downham with additional mixing by Christopher Ward\n\nMastered by Tony Dawsey at Masterdisk\n\n\n\nUntitled (How Does It Feel?) (5:13)\n\nWritten by D'Angelo and Raphael Sadiq\n\nPerformed by Pattern Is Movement\n\nMixed by David Downham at The Gradwell House\n\nMastered by David Downham at The Gradwell House\n\n\n\nArt and layout by Adam Heathcott, Hometapes.\n\nlicense\n\nall rights reserved"}, {"url": "https://www.washingtonpost.com/archive/local/2004/04/01/live/4e627fb9-51b0-4e7d-8d57-72b988ed2e8d/", "page_content": "Pattern Is Movement is a five-piece band from Philadelphia that seems to spend a lot of time thinking about what it wants to convey.\n\nBy the band's own admission, its sound could be described as indie rock, post rock or the intriguing term \"math rock.\" That's the heading for a cerebral, angular offshoot of the alternative scene that embraces pop melody but just as often employs obtuse lyrics, abrupt rhythm changes and unusual instrumental textures.\n\nThink of bands such as Pavement or Joan of Arc or the esoteric roster of the Delaware-based Jade Tree label.\n\nThe quintet's Web site is www.patternismovement.com and offers extrapolations on the music heard on its full-length CD, \"The (Im)possibility of Longing,\" a concept album of sorts that was released in February. The site offers short narratives tied to some of the song lyrics, music samples and a gallery of arty photographs.\n\nBefore the CD was released, the band earned a spot at MACrock (the Mid-Atlantic College Radio Conference) by submitting a generic disc of the material. As Andrew Thiboldeaux (vocals, guitar, keyboards, electronics) said, it was simply \"a Memorex CD-R of the new record, with our name markered on it. We didn't even have the artwork yet!\"\n\nThiboldeaux performed solo and with various ensembles before hooking up musically with Wade Hampton (bass), Chris Ward (drums), Corey Duncan (guitar) and former Arlingtonian Dan McClain (guitar, keyboards, lyrics), all of whom also had band experience.\n\nThe band's stated goal, expressed in its press guide, is lofty but sincere: \"We strive to contribute honest and intentional art to our community.\" They're willing to put some hard labor into it, too.\n\nThiboldeaux said some of the members will participate in a marathon in Vancouver, B.C., on May 3 in support of the Wellness Community, a national organization that provides free support services for cancer patients and their families. The band accept donations in support of the charity at the show.\n\nCatch a preview Sunday at 6 p.m. on WMUC-88.1-FM, the University of Maryland's radio station. Pattern Is Movement will visit for talk and music before heading to the Galaxy Hut show.\n\n-- MARIANNE MEYER\n\nGalaxy Hut is at 2711 Wilson Blvd., Arlington. Admission is free and limited to those 21 and over with valid ID. For more information, visit www.galaxyhut.com or call 703-525-8646.\n\nE-mail \"Live!\" suggestions to mariannemeyer@comcast.net."}, {"url": "https://www.jambase.com/article/the-roots-jam-10-01-brooklyn", "page_content": "The Roots :: 10.01.09 :: Brooklyn Bowl :: Brooklyn, NY\n\nThe Roots Jam | 10.01 | Brooklyn\n\nOver the past year, The Roots have been playing weekly midnight jam sessions at Highline Ballroom in NYC. These sessions produced some amazing collaborations, fusing hip hop, jazz, rock, soul, and funk. Since March, The Roots have been joined onstage by many well known musicians, creating a unique event and adding another accomplishment to their long and varied career.\n\nThe latest \u201cRoots Jam\u201d was held for the first time at Brooklyn Bowl, a bowling alley, concert hall, and restaurant with a floor that can fit about 600 people. The Bowl is owned by Peter Shapiro, best known for running The Wetlands at the ripe age of 24. Shapiro is also the creator of the Jammy Awards and just recently became the owner of Relix Magazine. All throughout this show you could spot Shapiro running around doing whatever he could to make the night perfect. Once the band came on he was right in the mix with the rest of the crowd, enjoying the show.\n\nThe opener was a group called Pattern Is Movement, a band that has played many previous jam sessions. A two-man act featuring Andrew Thiboldeaux (vocals, keys) and Chris Ward (drums), Pattern Is Movement were joined by The Roots for their only two songs of the night. With a voice comparable to Cee-Lo, Thiboldeaux belted out \u201cCrazy In Love,\u201d a Beyonc\u00e9 cover that had some of the crowd scratching their heads at first. People eventually started to catch on, unable to resist the pure energy exuded by the band. This was followed by a cover of D\u2019Angelo\u2019s \u201cHow Does It Feel?\u201d Pattern left the stage with a worked crowd ready for the rest of the jam.\n\nThe Roots Jam | 10.01 | Brooklyn\n\nWithout a break, The Roots remained onstage and transitioned into some smooth guitar fromas Questlove \u2018s heavy drum beat kept everything moving into an unreleased song. Roots\u2019 frontman Black Thought came out with a vengeance, setting the mood with his rhymes and charismatic body language. At times referred to as \u201cthe Grateful Dead of hip hop,\u201d The Roots, much like the Dead, have a few unsung heroes in their lineup. Keyboardistand bassistreally bring a lot to the table, especially when it comes to performing live. Poyser\u2019s jazz chops work great when improvising and Biddle\u2019s bass is as steady as they come, often quoting phrases from classic soul and rap songs.\n\nThe Roots were then joined by a horn section for \u201cCriminal,\u201d an original off of 2008\u2019s Rising Down. The horns consisted of \u201cMoist\u201d Paula Henderson (baritone sax), Chelsea Baratz (tenor sax), and Jonathan Powell (trumpet). Soon, they were also joined by members of Baja and The Dry Eye Crew as a reggae beat was laid down and once again the music shifted gears.\n\nThe Roots have shared the stage with many big names yet they also give their favorite underground artists a shot at the spotlight. One such artist is Brooklyn\u2019s own Jahdan Blakkamoore who came on as a guest emcee. Later, Ursula Rucker, a Philadelphia native and spoken word artist whose music is more poetic than lyrical, joined them. The band slowed it down as Rucker spoke her mind. Although she used strong language while onstage, she did not come across as vulgar, just a powerful person with strong beliefs.\n\nWhen Rucker left the stage, The Roots played the title track off their upcoming album, How I Got Over, now slated for release February 2010 on Def Jam Records. In the middle of the song the band was joined by Talib Kweli, and they segued right into his hit \u201cGet By.\u201d Kweli is a local Brooklyn hero, and got a huge response from the audience. In fact, Kweli grew up not too far from the Brooklyn Bowl itself. He was definitely a surprise for the audience, as it is always a mystery if a big name will appear at a jam.\n\nThe Roots Jam | 10.01 | Brooklyn\n\nAnother surprise of the night was when Reverend Vince Anderson , a raspy voiced piano/organ player, belted out Johnny Cash\u2019s \u201cHe Turned The Water Into Wine.\u201d The horn section once again joined in to add some southern New Orleans inspired soul. Anderson\u2019s voice mixed with all the instruments onstage creating a controlled chaos that blew everyone away.\n\nTo close the show, The Roots invited Mayer Hawthorne and his band The County up. Hawthorne has worked as a songwriter, producer, DJ, and audio engineer, among other things. Hawthorn dresses in nerd chic/indie rock fashion and looks a lot like Rivers Cuomo of Weezer. Hawthorne\u2019s vocals on his own \u201cMaybe So, Maybe No\u201d were tremendous. During Hawthorne\u2019s \u201cThe Ills,\u201d Black Thought jumped in and laid down a verse from \u201cIn The Music.\u201d It is safe to say that he stole the second half of the show. Maybe it was because it was the end of the night and everyone had a few beers in their system, but the bottom line is that for a nerdy, hipster kid, the guy can hold his own alongside The Roots.\n\nThe Wetlands was known for helping to launch the careers of many of the biggest bands today. Introducing audiences to new artists and genres is what Shapiro does best, and bringing a new variety of bands to Williamsburg will surely add some diversity to the neighborhood. The music in the area is currently dominated by whatever happens to be trendy on the big music blogs that month. Having an old school rock club will surely make for interesting times. This Roots Jam was a great change of pace and opened a lot of people\u2019s eyes and ears to another world. With the music scene coming together to support this new venue, the neighborhood may rediscover some great American music.\n\nThe Roots tour dates are available here.\n\nContinue reading for more images of The Roots Jam at Brooklyn Bowl\u2026\n\nJamBase | New York\n\nGo See Live Music!"}, {"url": "https://www.williamssoundstudio.com/pages/instrument-mixing-order.php", "page_content": "What Order Should You Mix Instruments in Music? When I first started mixing music at home, I wondered if there was a correct order to mix instruments. After reading and watching many mixes by pro-mixers, I finally settled on a set order for each genre.\n\nSo, in what order is it recommended to mix instruments? Most music genres begin with the drums and vocals - get these to sit in the same acoustic space. Next move on to the bass guitar and get it to sit nicely with the kick drum. Then by order of importance with the song, mix the instruments one by one.\n\nMixing Order of the Top 9 Music Genres\n\nThe lists below are suggested as a guide or starting point for those of us who have not mixed a particular genre before and are unexpectedly presented with the task. The guiding principle for any mix order is \u2018musical importance.\u2019 One shaker sound in a Punk Song is much less critical than the vocals, for example.\n\nAlso, decide if you will mix into a processed mix bus or a dry mixed bus. At WSS, we send all our tracks into a preset, standard mix bus, but this is a matter of preference.\n\nFigure 1: Mixing Instruments\n\nLet's get to the mixing orders for each music genre:\n\nMixing Order for Rock and Pop\n\nDrums \u2013 see the detailed explanation for drums further on in the article. Lead Vocals Bass Guitar Rhythm Guitar Lead Guitar Keys \u2013 Piano, Synthesizer, Hammond Organ Backing Vocals Percussion Reverb and other effects\n\nNote: Rhythm Guitar and Lead Guitar can be mixed in the reverse order, in certain parts of Rock and Pop songs.\n\nMixing Order for R&B and Hip-hop\n\nDrums Bass Vocals Lead Vocals Backing Vocals Keys \u2013 Piano, Organ Guitar Percussion Reverb and other effects\n\nMixing Order for Blues\n\nDrums Lead Vocals Lead Guitar Rhythm Guitar Backing Vocals Keys \u2013 Piano, Hammond Organ Percussion Reverb and other effects\n\nMixing Order for Classical Music\n\nSpaced Microphone Array (Decca Tree) Overhead Microphones Violins Cellos Violas Bass Instruments Woodwind Horns / French Horns Brass Percussion Stereo Room Microphones\n\nNote: If the music was recorded in a large space, mix the Decca Tree first, but if not mix the spaced microphones at the end.\n\nMixing Order for Country Music\n\nDrums Lead Vocals Backing Vocals Rhythm Guitar Keys \u2013 Piano, Organ Lead Instruments \u2013 Lead Guitar, Pedal Steel Guitar or Debro Other Lead Instruments\n\nFiddle\n\nAccordion\n\nHarp\n\nBanjo\n\nHarmonica\n\nMandolin\n\nZither\n\nReverb and other effects\n\nMixing Order for Electronic & EDM\n\nElectronic Drums Bass Lead Synth Lead Guitar (if there is one) Saxophone (if there is one) Samples and Chops Backing Vocals Synth Pads Reverb and other effects\n\nMixing Order for Folk Music and Bluegrass\n\nLead Vocals Rhythm (Mood) Instruments \u2013 Acoustic Guitar, Mandolin, Banjo Bass Instruments \u2013 Bass Guitar, Jug Percussive Instruments \u2013 Drums, Percussion Supporting Instruments\n\nFlute\n\nAccordion\n\nDobro\n\nHarmonica\n\nHarp\n\nPercussion \u2013 Washboard, Spoons\n\nReverb and other effects\n\nMixing Order for Jazz\n\nRoom Microphones Drums Bass \u2013 Double Bass, Bass Guitar Keys \u2013 Piano, Organ Guitar Trumpet Trombone Saxophone\n\n\n\nNote: If the Jazz has vocals, mix the vocal inbetween the drums and bass.\n\nMixing Order for Reggae\n\nDrums \u2013 Reggae and roots drums tend to sound very dry Lead Vocals Backing Vocals Bass Guitar Lead Instruments \u2013 Guitar, Clarinet Keys \u2013 Piano, Organ, Xylophone (not strictly keys I know) Percussion \u2013 Bongo, Guiro, Cowbell, Shaker, Eggs, Chimes, The Kweeka Reverb and other effects\n\n\n\nSome points to note: A mix will most likely not be complete until you have cycled through the suggested order a few times. Some instruments may only be relevant for a short section of a song. In this case, mix that section of the song in your own modified mix order. Sometimes it is necessary, and indeed ok, to mix some percussion instruments with the drums. For example, it is ok to mix hand drums with acoustic drums if they are a big part of the rhythm.\n\nMixing Order For Drums\n\nMost of the lists above prioritize mixing drums first. Mixing drum parts has an order of its own, and is explained below.\n\nAcoustic Drums\n\nIt is recommended to mix all your drum sounds into a single bus (the drum bus) so that the volume of the whole drum kit can be raised or lowered with one fader.\n\nHere is the suggested drum mix order:\n\nDrum Overheads Kick Drum Out Kick Drum In Snare Bottom Snare Top Hi-hat Floor Tom Rack Toms Room Microphones Parallel Compression (NYC Compression) Reverbs\n\n\n\nNotes: If you augment your drum sounds with samples, mix the sample at the same time as the original sound source. For example, combine and mix a recorded snare sound and a sampled overdub snare at the same time.\n\n\n\nOn mono room microphone recordings try sending the mono signal to a stereo reverb as a separate auxiliary track and blend in the two.\n\nElectronic Drums\n\neKick eSnare eHi-Hat eToms Percussion\n\nMixing Vocals\n\nIn every case, mix lead vocals before backing vocals \u2013 lead vocals are always the main thrust of a song. Mixing vocals in the verses and choruses require different treatment. For a lot of music, the chorus is the part of the song which should stand out. How you treat all instruments in verses and choruses is essential but more so for the vocals.\n\nIf your vocals sound thin and lack body, try some double-take vocals (if your vocalist is still available), or add a doubler like The Waves Doubler or Soundtoys\u2019s Microshift \u2013 both of which work very well for thickening a weak vocal recording.\n\nFigure 2: Vocal Mixing with Waves Doubler and Soundtoys Microshift\n\nRelated questions\n\nWhat are the typical mixing levels for each instrument? To get instruments to sit in their own space in the mix, you need to make sure that one track does not mask another. Avoid frequency masking by filtering less important frequencies from instruments that do not need those frequencies, for example, filter low and low-mid frequencies from an acoustic guitar in a rock mix. Also, use panning and reverb to place instruments around the soundstage.\n\nHow do you balance a mix? First, hardpan all instruments right or left except for the kick drum, bass guitar, and vocals - leave those in the center. Find the loudest and quietest parts of the song and create two loop markers. Bring all of your instrument faders down. Next, at the loudest part of the song, set the vocal fader so that it's level dances around -20 dBFS (if using the K-20 system for mixing). Now switch your output from Stereo to Mono, using a plug-in, stereo fader panning, or your console's mono button if you have one. Turn the vocal volume to a \u2018can just make it out\u2019 level and start to bring up the other faders so you can hear each instrument in the song. Do this for the two loops you made earlier and adjust to find a happy balance. Now turn the volume back up to your normal monitoring level and switch back from mono to stereo. You should now have a pretty good initial mix balance. Finish this process off later with some automation when fixed fader levels make one instrument too loud or too soft where they shouldn't be.\n\nHow do you get good at mixing? There is no easy answer to this question. Like getting good at anything, mixing improves with experience and time. Mixing requires the right balance of study, copying other peoples processes and adapting them for yourself, practice, comparison, more practice, and refining your own tastes, workflow and processes, and technical know-how. Keep reading, practicing, and learning - you will get good!"}, {"url": "https://www.loopmasters.com/genres/158-Liquid", "page_content": "Liquid\n\nfilter sort results Choose content type DAW Presets Loops MIDI One Shots Sampler Patches Synth Presets Choose genre Ambient Artist Series Chillout Downtempo Drum and Bass Halftime Jungle Liquid Series Vocals Choose format Ableton Live Pack Ableton Live Presets Acid Apple Loops Audio Battery EXS Halion Kontakt Loopcloud Maschine Maschine Expansion Massive Presets MIDI Files NNXT Reason Refill Rex2 SFZ Stylus RMX Synth Presets Wav Choose label 5Pin Media Apollo Sound Blind Audio DABRO Music EST Studios Famous Audio Freaky Loops Ghost Syndicate House Of Loop Industrial Strength IQ Samples Loopmasters Looptone Monster Sounds Niche Audio Producer Loops Production Master Rankin Audio Sample Diggers Singomakers Soul Rush Records THICK SOUNDS Vocal Roads Zenhiser Sort by Popularity Rating Date released \u25ba \u25ba AWARD \u25ba \u25ba AWARD \u25ba \u25ba \u25ba \u25ba \u25ba AWARD \u25ba \u25ba \u25ba \u25ba \u25ba \u25ba \u25ba \u25ba \u25ba AWARD \u25ba \u25ba AWARDSALE \u25ba SALE \u25ba \u25ba \u25ba \u25ba AWARD \u25ba \u25ba \u25ba \u25ba AWARD \u25ba \u25ba AWARDSALE \u25ba AWARDSALE \u25ba AWARD \u25ba \u25ba \u25ba AWARD \u25ba AWARD \u25ba \u25ba AWARD \u25ba \u25ba \u25ba\n\n"}, {"url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2021.769663/full", "page_content": "and Kai Siedenburg1\n\nListeners can attend to and track instruments or singing voices in complex musical mixtures, even though the acoustical energy of sounds from individual instruments may overlap in time and frequency. In popular music, lead vocals are often accompanied by sound mixtures from a variety of instruments, such as drums, bass, keyboards, and guitars. However, little is known about how the perceptual organization of such musical scenes is affected by selective attention, and which acoustic features play the most important role. To investigate these questions, we explored the role of auditory attention in a realistic musical scenario. We conducted three online experiments in which participants detected single cued instruments or voices in multi-track musical mixtures. Stimuli consisted of 2-s multi-track excerpts of popular music. In one condition, the target cue preceded the mixture, allowing listeners to selectively attend to the target. In another condition, the target was presented after the mixture, requiring a more \u201cglobal\u201d mode of listening. Performance differences between these two conditions were interpreted as effects of selective attention. In Experiment 1, results showed that detection performance was generally dependent on the target\u2019s instrument category, but listeners were more accurate when the target was presented prior to the mixture rather than the opposite. Lead vocals appeared to be nearly unaffected by this change in presentation order and achieved the highest accuracy compared with the other instruments, which suggested a particular salience of vocal signals in musical mixtures. In Experiment 2, filtering was used to avoid potential spectral masking of target sounds. Although detection accuracy increased for all instruments, a similar pattern of results was observed regarding the instrument-specific differences between presentation orders. In Experiment 3, adjusting the sound level differences between the targets reduced the effect of presentation order, but did not affect the differences between instruments. While both acoustic manipulations facilitated the detection of targets, vocal signals remained particularly salient, which suggest that the manipulated features did not contribute to vocal salience. These findings demonstrate that lead vocals serve as robust attractor points of auditory attention regardless of the manipulation of low-level acoustical cues.\n\nIntroduction\n\nIn everyday life, our sense of hearing is exposed to complex acoustical scenes that need to be analyzed and interpreted. The ability to segregate an acoustic scene into a mental representation of individual streams is known as auditory scene analysis (ASA; Bregman and McAdams, 1994). A prime example of this is listening to music with multiple instruments playing at once. Human listeners can focus and track a single instrument remarkably well, even though the acoustic signal is a potentially ambiguous clutter of diverse instrument signals.\n\nTwo interwoven analytical processes are used in ASA: endogenous top-down and exogenous bottom-up processes. Endogenous processes are based on cortical functions, such as expectations, learned patterns, and volition. Exogenous processes are driven by pre-attentive processes based on the temporal and spectral properties of a sound, from which auditory attributes, such as duration, pitch, or timbre are computed, and which are pivotal for grouping auditory information into separate sound events. Timbre, often simply described as \u201ctexture\u201d or \u201ctone color\u201d (Helmholtz, 1885), is a multidimensional attribute (Siedenburg and McAdams, 2017) that enables the discrimination of sound sources (e.g., sounds from a keyboard vs. a guitar), even though they may match in other acoustic cues such as loudness and pitch.\n\nA well-established approach to the study of ASA and auditory attention is the use elementary auditory tasks, such as the presentation of sequential or simultaneous streams of tones (for a review, see Alain and Bernstein, 2015). Bey and McAdams (2002) investigated the influence of selective attention in ASA using two-tone sequences, one of which was interleaved with distractor tones. The semitone spacing between the distractor tones and the target sequence was varied from 0 to 24 semitones, thereby varying the strength of exogenous cues that allow for bottom-up stream segregation. Participants had to judge whether the sequences were different or identical and had to ignore the distractors. To vary the dependency on selective attention, in one condition, the stream with distractor tones was presented first, followed by the melody without distractors; in a second condition, selective attention was facilitated by presenting the melodies without distractors first, thus providing a pattern that could be compared with the following mixture. The results showed that participants achieved higher recognition rates when the melodies without distractors were presented first, thus being able to selectively attend to the target melody.\n\nAnother more ecological approach uses polyphonic music to study ASA. In polyphonic music, multiple relatively independent melodies (also referred to as voices) are played or sung simultaneously. Behavioral studies showed that when listening to polyphonic music a superior perception of timing and meter is found in the lower voices (Hove et al., 2014), whereas tonal and melodic perception is facilitated in the highest voice (Crawley et al., 2002). Accordingly, the so-called high-voice superiority effect states that the voice with the highest pitch trajectory is most salient in polyphonic mixtures (Fujioka et al., 2005). It has been shown that this effect is present in infants (Marie and Trainor, 2013) and that it can be enhanced by musical training (Marie et al., 2012). Using a model of peripheral auditory processing, results by Trainor et al. (2014) suggest that the origin of high-voice superiority may be based on physiological factors such as cochlear filtering and masking patterns.\n\nAnother factor that has been shown to affect musical scene perception and the specific trajectory of auditory attention is related to the repetitiveness of musical voices. Taher et al. (2016) found that when a repetitive and non-repetitive voice is playing simultaneously, attention is drawn to the non-repetitive voice. Barrett et al. (2021) investigated whether the coherent timings between instruments in a piece of music facilitate stream segregation. The authors either slowed down one instrument or recomposed an instrumental line so that it no longer matched with the other lines. The results suggested that, when instruments are temporally coherent, attention is not directed to a particular instrument, and therefore instruments are integrated into one percept. For incoherent musical lines, attention was drawn toward one instrument while the other instrument was ignored. A study by Disbergen et al. (2018) focused on the effect of timbre dissimilarity for distinguishing between two melodic voices in polyphonic music. Although no clear effect for a modification of timbral dissimilarity could be observed, the results implied a trend that a reduction of timbral dissimilarity and thus a reduction of acoustical cues lead to a deterioration of stream segregation, further suggesting that a minimum of exogenous cues is necessary to track and separate single streams. In Siedenburg et al. (2020), listeners had to hear out instruments and melodies of varying sound level masked by a simultaneously playing instrument. It was found that participants were able to exploit dips in the masker signal, allowing them to hear the target instrument at lower levels than with a masker that did not contain these dips.\n\nSeveral of the aforementioned studies used (simplified or stylized) excerpts of Western classical instrumental music. In Western popular music, the lead melody and thus the centerpiece of a song is sung by a human voice (lead vocals), which is accompanied by a variety of instruments and, at times, background vocals. Recent studies have shown that the voice occupies a unique role among other sound sources (e.g., Belin et al., 2000; Levy et al., 2001; Agus et al., 2012; Suied et al., 2014; Isnard et al., 2019). In a neurophysiological study, Belin et al. (2000) examined the response to speech, vocal non-speech sounds, and non-vocal environmental sounds. The data implied not only that cortical activity to vocal speech and non-speech sounds were higher than to non-vocal environmental sounds but also that specific regions in the human cortex responded more strongly to vocal sounds, suggesting a specialized processing of speech sounds. Levy et al. (2001) measured neurophysiological data from participants in an oddball task in which single instruments and singing voice were presented sequentially. A piano sound was used as a target, while other sounds were used as distractors. The results showed a stronger response to the presentation of the human voice, termed the \u201cvoice-specific response.\u201d The authors hypothesized that this response represented a gating mechanism in which the auditory system allocates the input to be processed phonologically. In Agus et al. (2012), accuracy and reaction times were investigated in a sound classification task. Single notes were played by instruments, sung by voices, or played by interpolations between instruments and voices (i.e., chimeras). Accuracy for voices was higher and reaction times were faster than for all other target categories, indicating an advantage in processing voices. Studies by Suied et al. (2014) and Isnard et al. (2019) focused on the recognition of timbre in short glimpses of recorded sounds that differed only in timbre. Again, singing voices stood out by achieving recognition above chance level with a sound duration of only 4 ms, while all other instrument categories required 8 ms durations.\n\nIn the present study, we aim to investigate auditory attention in an instrument and singing voice detection task inspired by everyday music listening of popular music. To study how the detection of different instruments is modulated by auditory attention, we vary the presentation order of mixture and target cue. In one order of presentation, a cue from a target vocals or instrument is presented first, followed by the mixture, such that the cue can be used to search the mixture for the target. In the reverse presentation order, the mixture is presented first followed by the target cue. Based on experiments such as Bey and McAdams (2002), we expect that the order in which a cue is presented first facilitates detection of the target. Motivated by the distinct role of singing voices that has been reported in the literature, we investigated whether the lead vocals in popular music would play a special role in auditory scene analysis (ASA) and selective listening. Based on this assumption, we hypothesize that lead vocals achieve distinctly higher accuracies in both presentation orders.\n\nGeneral Methods\n\nFor our experiment, we used short excerpts of popular music in which either a cued target instrument or target vocal was present or absent in a mixture of multiple instruments (see Figure 1B). To test the effects of selective auditory attention, we interchanged the presentation order of the cue and mixture (Bey and McAdams, 2002). This yielded two different listening scenarios: one requiring selective listening, and the other requiring a rather global mode of listening. When the target was presented prior to the mixture, selective attention could be used to detect the target in the mixture. When the target was presented after the mixture, listeners had to be aware of possibly all components of the mixture and hence listen more globally to the excerpts. In that case, attention could be affected by exogenous factors, for instance the salience of individual sounds in the musical scene. We conducted three experiments aimed to study the role of attention in the processing of popular music mixtures and whether acoustic modifications of the excerpts would manipulate the detection of instruments or vocals. For the first experiment, we left the excerpts unmodified and investigated the detection accuracy in the complex musical scene and how it was affected by the presentation order and different instruments. In the second experiment, we aimed to suppress energetic masking of the target by means of bandpass/bandstop-filtering. To control the influence of instrument dependent sound levels, we equalized the sound levels ratios between the different targets in the third experiment. A schematic overview of the experiments is shown in Figure 1C. The same general methods were applied in all three experiments. Specific modifications of the methods are described in detail in the respective experiments (see Unmodified Excerpts, Spectral Unmasking Equalization, and Sound Level Equalization).\n\nFIGURE 1\n\nFigure 1. Schematic overview of the experiments. (A) Procedure: The experiment started with a headphone screening task, followed by a subjective sound level calibration, a training section where participants were familiarized with the instrument detection task and finally the main experimental section. (B) Task: An instrument detection task was used in the experiments: Participants either took part in an experiment where the targets were preceding the mixtures or where the mixtures were preceding the targets. (C) Stimuli modification: In the first experiment, excerpts unmodified from their original state were used. In the second experiment, the targets were filtered in an octave band to create a spectral region in which the target could pass without being spectrally masked. In the third experiment, the individual sound level differences between the diverse vocals and instruments were adjusted to one of three possible level ratios.\n\nParticipants\n\nAll participants were students recruited via an online call for participation at the e-learning platform of the University of Oldenburg. General information about the experiment and exclusions criteria were given. The criteria included the use of headphones, a stable internet connection, and self-reported normal hearing. Participants could start the online experiment at any time via a link that was provided in a personalized email. Participation was compensated monetarily. We acquired information about the participants musical training using five questions: Number of instruments played, hours practiced during the period of greatest musical interest, years of lessons in music theory, years of lessons for an instrument, and self-designation (non-musician, amateur musician, and professional musician).\n\nStimuli and Task\n\nAn illustration of the stimuli extraction is shown in Figure 2. Stimuli were generated using a Matlab script (MathWorks Inc., Natick, MA, United States) that extracted 2-s excerpts from a multitrack music database. The database was created by Tency Music and is used within the Musiclarity web-app (Eastgate et al., 2016). It consists of sound alike reproductions of well-known popular music with English lyrics and individual audio files for each instrument. The Instruments in the database were coarsely categorized as: Backing Vocals, Bass, Drums, Guitars, Lead Vocals, Piano, Percussion, Strings, Synthesizer, and Winds. For each excerpt, one to-be attended instrument was chosen (target). Other instruments in the excerpt that were not from the same category as the target served as maskers (mixture). Instruments from the same category that were not used as a target were excluded from the mixture. When lead vocals were assigned as the target, all backing vocals were also excluded. Songs were drawn pseudo-randomly, with the same song chosen as infrequent as possible. To investigate which instruments were audible at any given time, the sound level of each instrument was analyzed using a 500 ms sliding window. In each window, the root-mean-squared (RMS) sound level was calculated. Windows were qualified as potential candidates for the excerpt extraction if one instrument in the target category and 6\u20139 additional instruments had sound levels above \u221220 dB relative to the instrument\u2019s maximum sound level across the full song. A previously unused 2,000 ms time slice containing four qualified adjacent 500 ms windows was randomly drawn. Three monophonic signals were compiled from each 2-s excerpts: (1) a signal only containing the target, (2) a signal containing a mixture of 5\u20138 instruments from non-target categories plus the target. (3) A signal containing a mixture of 6\u20139 instruments without the target. For mixtures, the full number of instruments was used, which were also present in the original excerpt of the song. A logarithmic fade-in and fade-out with duration of 200 ms was applied to the beginning and end of all extracted signals. For half of the trials, the mixture signals were arranged to contain the target signals, and for the other half, the mixture did not contain the target signal. From these signal combinations, two stimuli with duration of 4,500 ms were created using different presentation orders for the target and mixture signal. In the \u201cTarget-Mixture\u201d condition, the target signal was followed by a 500 ms pause and the mixture signal; in the \u201cMixture-Target\u201d condition, the presentation order was reversed. For the use on the online platform, the stimuli were converted from WAV format to MP3 with a bit rate of 320 kbit/s. Example stimuli are provided on our website: https://uol.de/en/musik-wahrnehmung/sound-examples/listening-in-the-mix\n\nFIGURE 2\n\nFigure 2. Stimuli extraction. Short excerpts from a multitrack database containing reproductions of popular music were used as stimuli. The schematic shows the workflow of the stimulus construction. For details, see the text.\n\nProcedure\n\nThe experiments were approved by the ethics board of the University of Oldenburg and carried out online via the web platform www.testable.org. Participants were divided into one of two groups. For group 1, all stimuli had the presentation order \u201cTarget-Mixture,\u201d whereas for group 2 the presentation order was reversed (see Figure 1B). The same excerpts were used for both groups, thus the only differences were in the order of presentation. Each experiment was further divided into four consecutive segments (see Figure 1A).\n\nAt the beginning of the experiment, participants had to fill in a form regarding personal data (see General Methods). To get an indication of whether participants were using headphones, a headphone screening task was performed at the beginning of the experiments. For Experiment 1 headphone screening was based on Woods et al. (2017), employing a sequential presentation of three pure tones, where one of the tones was quieter. The tones were phase shifted on the left side by 180\u00b0 and therefore appeared attenuated when listening over loudspeakers but not attenuated when headphones were worn. Therefore, a matching volume judgment should only be achieved by wearing headphones. Listeners had to detect the quiet tone and passed the test if five out of six detections were correct. For Experiments 2 and 3, the headphone screening was based on Milne et al. (2020), which provides a higher selectivity for headphone users than the headphone screening used in Experiment 1. Here, a sequence of three white noise signals were presented, where one of the noise signals was phase shifted by 180 degrees in a narrow frequency band at around 600 Hz on the left headphone channel. When headphones were worn, the phase shift was perceived as a narrow tone embedded in the broadband noise. Listeners had to detect the tone and passed the test if five out of six detections were correct. Participants who failed the headphone screening were removed from the data analysis.\n\nAfter the headphone screening, three song excerpts were presented aiming to provide an impression of the dynamic range of the stimuli. During the presentation, participants were instructed to adjust the sound to a comfortable level. This was followed by a training phase, where participants were familiarized with the detection task. Participants listened to stimuli akin to those used in the main experiment and were asked whether the target was present or absent in the mixture. For each category, one stimulus with and without target was presented. To help participants understand the task and to make them more sensitized for the acoustic scene, feedback was given after each answer. This was followed by the main experiment where the same procedure was used but no feedback was given. Stimuli presented in the training segment were not reused in the experiment segment. All stimuli were presented in a random sequence that intermixed all conditions (except for the between-subjects factor of presentation order). The number of stimuli, the conditions, and the target categories differed from experiment to experiment and are therefore described in the sections on the individual experiments below.\n\nData Analysis\n\nFollowing the methodology recommended by the American Statistical Association (Wasserstein et al., 2019), we refrain from the assignment of binary labels of significance or non-significance depending on an immutable probability threshold. We provide mean detection accuracies, followed by a square bracket containing the 95% CIs computed by means of bootstrapping and round brackets containing the decrease or increase through a change in presentation order.\n\nA generalized binominal mixed-effect model (West et al., 2014) was used for the statistical analysis. All mixed-effects analyses were computed with the software R (R Core Team, 2014) using the packages lme4 (Bates et al., 2015), which was also used to estimate marginal means and CIs. Our model included random intercepts for each participant and item (i.e., stimulus). All binary categorical predictors were sum-coded. The correlation coefficients of the model are given as standardized coefficients (\u03c72) and probability (p). To summarize the main effects and interactions, results are presented in the form of an ANOVA table, derived from the GLME models via the anova function from the car package (Fox and Weisberg, 2019). A detailed view of the behavioral results, models and statistic evaluations for each experiment are presented in the supplementary material (see Supplementary Tables 1\u20136).\n\nMethod Validation\n\nSince the experiment was conducted online, and therefore did not undergo the strict controls of a laboratory experiment, we compared results for using calibrated laboratory equipment and consumer devices. In order to achieve this, a pilot experiment that was very similar to Experiment 1 was completed by the members of the Oldenburg research lab. In one condition, participants used their own computer and headphones. In another condition, they used calibrated audio equipment, and the presentation order of these two conditions was counterbalanced across participants. The calibrated equipment consisted of a laptop, RME Babyface soundcard, and Sennheiser HD650 headphones. The long-term sound level was set to 75 dB SPL (A), measured with Norsonic Nor140 sound-level meter using music-shaped noise as the excitation signal. Results showed very similar data for both types of equipment (for details, see Supplementary Figure 1), which did not indicate any systematic problem in conducting the present study via online experiments.\n\nExperiment 1: Unmodified Excerpts\n\nThe first experiment was our starting point to investigate selective auditory attention in musical scenes. We left the excerpts in their original state (as described in General Methods). As target categories besides the lead vocals, we chose four instrument categories that had shown rather diverse results in a pilot experiment.\n\nParticipants\n\nA total of 84 participants with a mean age of 25.1 years (SD = 4.5, range = 19\u201344) were tested in the experiment. A total of 25 out of 42 participants passed the headphone screening for the Target-Mixture condition and 22 out of 42 for the Mixture-Target condition (age = 25.3, SD = 5, range = 19\u201344). Only participants passing the headphone screening were included in further analysis. Eleven participants in the Target-Mixture condition and 10 participants in the Mixture-Target condition described themselves as either amateur or professional musicians.\n\nStimuli and Procedure\n\nFor the first experiment, the following five target categories were selected: lead vocals, bass, synthesizer, piano, and drums. Headphone screening was based on Woods et al. (2017). In the training phase of the main experiment, one excerpt with a target and one excerpt without a target were presented for each of the five target categories, summing up to 10 stimuli in total. In the experimental phase, 150 stimuli were presented, divided into 30 stimuli for each of the five target categories. The average duration of the experiment was 25 min.\n\nResults and Discussion\n\nFigure 3 displays the average results of the first experiment for each instrument and presentation order (for numerical values, see Supplementary Tables 1, 2). Detection accuracy differed depending on the target category and order of presentation, which was also evident in our model (Instrument: \u03c72 = 97.881, p < 0.001, Order: \u03c72 = 38.878, p < 0.001.). Averaged across target categories, the Target-Mixture condition yielded the highest accuracy of 84% (70\u201397%), which deteriorated in the Mixture-Target condition to 72% (55\u201388%; \u221212%). This decline was strongest for the bass category in which the mean accuracy dropped by \u221219% from the Target-Mixture to the Mixture-Target condition. A nearly identical decrease was found for the synthesizers (\u221211%), piano (\u22128%), and drums (\u221211%). The lead vocals had the best performance overall and were least affected by a change in the presentation order (\u22122%). This resulted in an interaction effect between the instrument factor and the presentation order (\u03c72 = 13.059, p = 0.011).\n\nFIGURE 3\n\nFigure 3. Detection accuracy in Experiment 1. Five instrument and vocal categories were used as targets (lead vocals, drums, synthesizer, piano, and bass). The Square marks the mean detection accuracy for a given target category. Error bars indicate 95% CIs. Asterisks represent the average accuracy of an individual participant for the given target category. \u201cTAR\u201d denotes the presentation order \u201cTarget-Mixture\u201d where the target cue was presented followed by a mixture. \u201cMIX\u201d denotes the presentation order \u201cMixture-Target\u201d where a mixture was presented followed by the target cue.\n\nAll instruments except the lead vocals showed degraded detection accuracy when listeners were required to listen to the musical scenes without a cue. While the degradation of detection accuracy in a global listening scenario was to be expected (Bey and McAdams, 2002; Janata et al., 2002; Richards and Neff, 2004), the specific attentional bias towards lead vocals is, to our knowledge, a novel finding. We will refer to this unique characteristic as \u201clead vocal salience\u201d in the following. This finding is in line with the unique role of singing voices documented in previous experiments, where voices were processed faster and more accurately in comparison to other musical instruments (e.g., Agus et al., 2012; Suied et al., 2014; Isnard et al., 2019) and were shown to have a unique cortical voice-specific-response indicating a specialized processing for human voices (e.g., Levy et al., 2001).\n\nThe bass was found to be most strongly affected by a change in presentation order, having medial detection accuracy in the Target-Mixture condition that, however, decreased almost 2-fold compared to the other instruments. One explanation for this could be tied to the spectral characteristics of the bass. The bass mostly occurs in a rather narrow band in the low frequencies, whereas other instruments cover a wider frequency range. When a cue is given, attention may be focused selectively toward that frequency band, and thus narrow signals like the bass can be reliably perceived. Another explanation could be derived from the high-voice superiority effect that has been observed in polyphonic music. The effect describes a pre-attentive attentional bias (Trainor et al., 2014), which, in the presence of multiple voices, draws attention toward the highest voices. In the current experiment, bass signals naturally correspond to low voices, and hence high-voice superiority may come into play.\n\nIt is to be noted, that our analysis revealed no systematic differences between participants who declared themselves as musician and those who did not. This held true across all three experiments, even though in previous studies, musicians showed improved results in ASA tasks (e.g., Ba\u015fkent et al., 2018; Madsen et al., 2019; Siedenburg et al., 2020). The most likely reason to explain this may be that we did not specifically control for an equal number of musicians and non-musicians in a large sample; thus, the proportion of participants considered musicians were only a fraction of the total participants, and therefore the sample size may be too small for an adequate statistical comparison. We further analyzed how performance was affected by possible fatigue over the course of the experiment. Considering performance over the duration of the experiment averaged across subjects suggested that the difference between performance at the beginning and end of the experiment was negligible (for details, see Supplementary Figure 2).\n\nTo further evaluate the acoustic origins of the lead vocal salience, we analyzed the music database in terms of spectral features and sound levels features. For each song and target category, we evaluated the broadband sound level as well as the sound level on an ERB-scale between all instruments and voices in a category and all other instruments and voices. We used a sliding window of 500 ms moving over the duration of a song and discarded all windows in which the sound level was less than 20 dB below the maximum sound level of the instruments, voices, or mixtures. The results of the time windows were then averaged for each song and are displayed in Figure 4.\n\nFIGURE 4\n\nFigure 4. Database feature analysis. We analyzed the average sound level in ERB-bands (A) and broadband sound level (B) between each voice or instrument and the remaining mixture for each song. (A) Each colored line represents the average sound level for the given center frequency. The filled area represents the 95% CIs for the lead vocals. (B) The circle marks the mean detection accuracy for a given target category. Error bars indicate 95% CIs. Crosses represent the average level of an individual song for the given target category.\n\nThe spectral analysis revealed a frequency region from 0.5 to 4 kHz, where the difference between the lead vocals and remaining mixtures had a positive level ratio (up to 2.5 dB), meaning that the lead vocals exhibited higher levels than the sum of accompaniment instruments and were therefore released from energetic masking in those spectral regions. While the lead vocals had a relative sound level of more than 0 dB in such a broad spectral region, only the bass and drums showed similar levels in either low or high frequencies. The other instruments did not have such a differentiated spectral range and their level was substantially below the level of the lead vocals. This was also evident in the broadband level analysis, in which the lead vocals had a significantly higher level than the other instruments. Accordingly, two acoustically-based explanations for the superior detection accuracy of the lead vocals could be (a) less susceptibility to masking by other instruments or (b) higher loudness levels of lead vocals. To scrutinize these two hypotheses, we conducted a second experiment where the vocals and the instruments were released from masking in the same frequency band, and a third experiment equalizing the sound level differences between lead vocals and instruments.\n\nExperiment 2: Spectral Unmasking Equalization\n\nTo investigate whether the observed lead vocal saliency was due to spectral masking, we here examined the spectral regions where vocals tended to be unmasked and applied the same unmasking to different target instruments. For this purpose, we analyzed the database for spectral regions in which the lead vocals exhibited particularly high sound levels. A broad spectral region from about 0.5 to 5 kHz was found. To provide equal masking and unmasking for all vocals and instruments, we used octave bands adjacent to the center of this region (1\u20132 and 2\u20134 kHz) and designed filters to pass signals only into one of the two bands (bandpass) or to suppress signals only into this range (bandstop). To compensate for level-dependent differences, the sound levels of all target instruments were adjusted identically. Only instruments with relevant intensity in the selected frequency bands were considered as targets for the experiment. Therefore, lead vocals, guitars, and piano were used as target categories. To avoid listeners focusing only on the octave bands, a randomly drawn accompaniment instrument was passed through the octave band for one third of trials, whereas the target category sound was attenuated in the octave band.\n\nParticipants\n\nA total of 49 participants with a mean age of 25.6 years (SD = 4.2, range: 20\u201339) were tested in the experiment. A total of 20 out of 25 participants passed the headphone screening for the Target-Mixture condition and 20 out of 24 or the Mixture-Target condition (age = 23.5, SD = 2.9, range: 20\u201329). Only participants passing the headphone screening were included in the analysis. Among these, 12 participants in the Target-Mixture condition and four participants in the Mixture-Target condition described themselves as either amateur or professional musicians.\n\nStimuli and Procedure\n\nIn two out of three excerpts, the target was filtered through a passband either from 1 to 2 kHz or from 2 to 4 kHz, while the mixture was filtered through a bandstop in the same octave band. Excerpts filtered in this way are referred to as \u201cTBP\u201d in the following. To prevent participants to focus on only one of the two octave bands, in one third of the excerpts, a randomly drawn accompanying instrument was filtered through a passband of either 1\u20132 or 2\u20134 kHz, while the other accompaniment instruments and the target were filtered through a bandstop in the same octave band. Excerpts filtered this way are referred to as \u201cTBS\u201d in further analysis. Bandpass and bandstop filters were designed and applied using the corresponding Matlab functions bandpass and bandstop (Signal Processing Toolbox Release 8.3, MathWorks Inc., Natick, MA, United States). The filtered target signal was used both during the presentation of the cue and when it was presented in the mix. The signal components in the stopband were attenuated to \u221280 dB FS (decibels relative to full scale). Sound levels ratios between targets and mixtures were adjusted for all targets to \u221210 dB. In a final step, the average sound level of each stimulus was normalized to \u221215 dB FS. As target categories, lead vocals, guitar, and piano were chosen.\n\nThe headphone screening test was based on Milne et al. (2020). In the training phase of the main experiment, one stimulus with and one without a target were presented for each of the three target categories, each of the two octave bands and one additional stimulus for each target category and octave band where the target was filtered by a bandstop and an accompaniment instrument was filtered by a bandpass, summing up to 18 stimuli in total. In the experimental phase of the main experiment, 180 stimuli were presented, divided into groups of 60 for each of the three target categories and further subdivided into 20 stimuli for each octave band where the target was filtered by a bandpass plus 10 for each octave band where the target was filtered by a bandstop. The average duration of the experiment was 35 min.\n\nResults and Discussion\n\nResults are displayed in Figure 5 (for details, see Supplementary Tables 3, 4). Detection accuracy was affected by the filter type (TBP = target is filtered with a bandpass, TBS = target is filtered with bandstop), presentation order, and instrument type. While we used two different adjacent octave bands to filter the signals (1\u20132 and 2\u20134 kHz), results for both frequency bands showed nearly identical results with no systematic differences (differences for all conditions between both octave bands: Difference_ MEAN = 2.5%, Difference_ MIN = 1.5%, and Difference_ MAX = 3.5%). This finding was underpinned by the GLME model, which revealed no effect for the usage of different octave bands (Octave: \u03c72 = 0.002, p = 0.963).\n\nFIGURE 5\n\nFigure 5. Detection accuracy in Experiment 2. Three instrument and vocal categories were used as targets (lead vocals, guitar, and piano). Either a bandpass or bandstop was applied to the filter and the mixture. The target filter type is listed in the upper area of the figure with T BP indicating a bandpass was used and T BS indicating a bandstop was used. The Square marks the mean detection accuracy for a given target category. Error bars indicate 95% CIs. Asterisks represent the average accuracy of an individual participant (n = 40) for the given target category. \u201cTAR\u201d denotes the presentation order \u201cTarget-Mixture\u201d where the target cue was presented followed by a mixture. \u201cMIX\u201d denotes the presentation order \u201cMixture-Target\u201d where a mixture was presented followed by the target cue.\n\nAs in Experiment 1, the detection accuracy was better in the Target-Mixture condition and best when the target signal was filtered by a bandpass. The influence of both the order and the filter was reflected in our model (Order: \u03c72 = 3.547, p = 0.06, Filter: \u03c72 = 18.657, p < 0.001). For the Target-Mixture TBP condition, an average accuracy of 96% (95\u201397%) was observed compared to the 85% (82\u201389%; \u221211%) in the Mixture-Target condition. For the Target-Mixture TBS condition, an average accuracy of 85% (82\u201388%) was achieved compared to the Mixture-Target condition 76% (72\u201380%; \u22129%).\n\nLead vocals performed best with an accuracy of 96% (93\u201399%) and showed the smallest decrease by changing the order (TBP: \u22121%, TBS: \u22122%) or removing the isolation by changing the filtering (Target-Mixture: \u22124%, Mixture-Target: \u22125%). This was followed by the guitar with an accuracy of 84% (80\u201386%), which in contrast to the vocals and pianos, achieved higher accuracies in the Target-Mixture TBS than in the Mixture-Target TBP condition and almost as well in the Mixture-Target TBP and the Mixture-Target TBS conditions (difference by order TBP: \u221217%, TBS: \u22127%. Difference by filter Target-Mixture: \u221211%, Mixture-Target: \u22121%). The piano with an accuracy of 79% (75\u201383%), showed a similar pattern as for the lead vocals and was generally better when it was isolated than when the isolation was lifted (difference by order TBP: \u221215%, TBS: \u221215%. Difference by filter Target-Mixture: \u221218%, Mixture-Target: \u221218%). This dependence on instruments was also corroborated by our model (Instrument: \u03c72 = 42.177, p < 0.001).\n\nCompared to the unmodified stimuli in the first experiment, applying a bandpass filter to the target improved the detection of instruments for both presentation orders by up to 16%. Specifically, this improvement raised the accuracies in the Target-Mixture condition to 99% (Experiment 1: 88%) for the lead vocals, 95% for the guitar, and 95% for the piano (Experiment 1: 79%). This indicates that whereas the frequency content of the instrument signals was narrowed down to an octave band and isolated, the additional selective attention in the Target-Mixture condition may have acted as searchlight, allowing for the detection of the target with an improved accuracy. However, whereas the overall accuracy was generally higher compared to the first experiment, the gaps between the accuracy in the Target-Mixture and the Mixture-Target conditions were larger than before for all instruments except the lead vocals. This gap was smallest and almost non-existent for the lead vocals (Experiment 1: \u22122%, Experiment 2 TBS: \u22122%), and enhanced for the guitar (in comparison to the average of non-bass instruments in Experiment 1: \u221210%, Experiment 2 TBS: \u221217%) and the piano (Experiment 1: \u22128%, Experiment 2 TBS: \u221215%). An instrument specific deterioration was underpinned by our model, which revealed a notable smaller contribution of the order alone (Order: \u03c72 = 3.5474, p = 0.060) and a much stronger contribution for the interaction between instruments and presentation order (Interaction: \u03c72 = 8.3447, p < 0.015).\n\nRelative to Experiment 1, the increased effect of presentation order in Experiment 2 could be interpreted as related to the narrowband nature of the target signals, as it was already discussed for the bass in the first experiment. In a global mode of listening, listeners are required to distribute attention across the whole musical scene, which may make it easier to miss narrowband signals in a mixture of wideband signals, or not to perceive them as individual signals. In contrast to the bass in Experiment 1, instruments in Experiment 2 occurred in frequency ranges in which the human hearing is particularly sensitive, which in turn still led to generally high detection accuracy. Here, the lead vocals also showed advantages over other instruments, which suggest that other characteristics of the lead vocals can be detected within the narrow band, leading to better detection accuracy.\n\nDetection accuracies additionally dropped in all target categories and for both presentation orders when the passband-filter was applied to an accompaniment instrument rather than the target. Again, the lead vocals were by far the least affected target category, showing that the lead vocal salience remains prominent even when the voice is suppressed in frequency regions where it is usually mixed louder than the mix. The general deterioration for all instruments and orders could be the by-product of a strategy in which participants listened primarily to the octave bands in which two-thirds of the targets appeared. Another reason for this pattern of results could be that the target did no longer occur in single octave band and thus targets were again subject to masking. A further possibility would be that the passband used here covers a particularly sensitive frequency region of human hearing, so that sound events in this area may be particularly salient.\n\nTaken together, the spectral filtering guaranteed that the target stood out from the mixture, hence resulting in high detection accuracies in the Target-Mixture condition. In the Mixture-Target condition, accuracies were distinctly lower. As in the first experiment, the lead voice showed by far the smallest difference between the different orders of presentation. When the isolation of the target was removed, all instruments showed a deterioration of detection accuracies. Again, the lead vocals achieved the highest accuracy compared to the other instruments, yet with a smaller deterioration across presentation orders. Thus, an explanation of the lead vocal salience does not seem to be due to less susceptibility to masking in frequency regions in which the vocals are mixed with higher levels than the sum of the accompanying instruments.\n\nExperiment 3: Sound Level Equalization\n\nMotivated by the relatively high sound levels of the lead vocals, here, we aimed to manipulate the level ratios of targets relative to the accompaniment to investigate whether this manipulation would affect detection performance and the observed differences between the presentation orders. As target categories, we selected the bass and lead vocals categories, because both were shown to be the conditions with lowest and highest performance in the first experiment, respectively. Since both target categories differ greatly in their spectral components, with bass being present mainly in the low frequencies and lead vocals in the mid and high frequencies, listeners could adopt a strategy where they would only listen to one of the distinct spectral regions. To avoid this, we added an additional experimental condition that contained instruments from all other target categories.\n\nParticipants\n\nA total of 55 participants with a mean age of 24.4 years (SD = 5.1, range = 18\u201333) were tested in the experiment. A total of 20 out of 27 participants passed the headphone screening for the Target-Mixture condition and 20 out of 28 for the Mixture-Target condition (age = 24, SD = 3.5, range: 19\u201333). Nine participants in the Target-Mixture condition and seven participants in the Mixture-Target condition described themselves as either amateur or professional musicians. Only participants passing the headphone screening were included in the analysis.\n\nStimuli and Procedure\n\nThe target categories lead vocals, bass, and individual instruments from the categories drums, guitar, piano, synthesizer, strings, and winds were chosen as targets for the instrument conditions. Excerpts with targets from lead vocals, bass, and the mixed category appeared equally often. The sound level ratio between the targets and mixtures was set to one of three possible levels where the broadband level of the target was either 5, 10, or 15 dB below the level of the mixture (referred here as \u22125, \u221210, and \u221215 dB condition). To accomplish this, the 2-s instrument and mixture signal were separately analyzed using a 100 ms sliding window. For every window, the A-weighted sound level was computed using the weightingFilter function in Matlab (Audio Toolbox Version 2.1, MathWorks Inc., Natick, MA, United States) followed by a sound level estimation via RMS calculation. We normalized the average sound levels of each stimulus to \u221215 dBFS.\n\nThe headphone screening test was based on Milne et al. (2020). In the training phase of the main experiment, one excerpt with and without a target were presented for each of the three target categories and for each of the three sound level ratio conditions, summing up to 18 stimuli in total. In the experimental phase of the main experiment, 180 stimuli were presented, divided into 60 stimuli for each of the three target categories and further subdivided into 20 stimuli for each sound level conditions. The average duration of the experiment was 35 min.\n\nResults and Discussion\n\nResults for the third experiment are shown in Figure 6 (for details, see Supplementary Tables 5, 6). Changes in the sound level ratio, presentation order, and target category affected the detection accuracy. The best performing condition was the lead vocal target category with a level ratio of \u22125 dB with an averaged accuracy of 99% (98\u2013100%) in the Target-Mixture condition and 100% (100\u2013100%) in the Mixture-Target order. Lowest detection accuracy was achieved by the lowest level ratio of \u221215 dB in the bass category ranging from 60% (57\u201364%) in the Target-Mixture condition to 58% (53\u201363%) in the Mixture-Target order. Within the same presentation order, the Target-Mixture condition achieved a generally higher accuracy, whereas the mean accuracy of all categories in the Mixture-Target condition deteriorated from 83% (82\u201384%) to 78% (75\u201380%; \u22125%).\n\nFIGURE 6\n\nFigure 6. Detection accuracy in Experiment 3. Three instrument and vocal categories were used as targets (lead vocals, bass, and others = drums, guitar, piano, strings, synthesizer, and winds). The sound level ratio between the target and mixture was adjusted to either \u22125, \u221210, and \u221215 dB and is listed in the upper area of the figure, decreasing from right to left. The Square marks the mean detection accuracy for a given target category. Error bars indicate 95% CIs. Asterisks represent data from individual participants for the given target category. \u201cTAR\u201d denotes the presentation order \u201cTarget-Mixture\u201d where the target cue was presented followed by a mixture. \u201cMIX\u201d denotes the presentation order \u201cMixture-Target\u201d where a mixture was presented followed by the target cue. The green cross above the lead voice in the \u221215 dB condition marks the averaged detection accuracy when all stimuli that were consistently answered incorrectly were excluded (for details, see Results).\n\nAveraged across all sound level ratios, the lead vocals showed the highest detection accuracy and was most unaffected by a change in presentation order showing a slightly better accuracy of 97% (96\u201398%) in the Target-Mixture condition compared to the Mixture-Target condition with an accuracy of 95% (93\u201396%; \u22122%). In the category of multiple instruments, the detection accuracy deteriorated from 84% (81\u201386%) in the Target-Mixture condition to 79% (76\u201382%; \u22125%) in the Mixture-Target condition. The bass achieved the overall lowest accuracy dropping from 71% (68\u201374%) in the Target-Mixture condition to 63% (60\u201366%; \u22128%) in the Mixture-Target condition. These results are similar to the results of Experiment 1, where the lead vocals performed best while the bass performed worst.\n\nIn summary, contrary to our assumptions using higher sound levels and equalizing the levels had not resulted in a cancellation of the order influence, as it was still present in most conditions but not present for the lead vocals. This reasoning was further supported by the statistical model, which showed relevant interaction between the instruments and presentation orders for the third experiment (Interaction: \u03c72 = 1.257, p < 0.001). However, if we draw a comparison between Experiments 1 and 3, the effect of presentation order was reduced considerably (Bass: Exp.1 = \u221219%, Exp.3 = \u221210%. Other instruments: Exp.1 = \u221212%, Exp.3 = \u22125%).\n\nSimilar to the first and second experiment, the lead vocals stood out and achieved the highest accuracy. A decline in accuracy of 7% in the Target-Mixture and a considerably larger decline of 15% in the Mixture-Target conditions could only be observed in the lowest sound level condition. An observation of the individual sound levels shows a clear difference between both presentation orders in the lowest level condition: \u22120% (\u22125 dB), \u22120% (\u221210 dB), and \u22128% (\u221215 dB). Yet, a closer look at individual stimuli revealed that this decrease was based on a few distinct stimuli that achieved low detection accuracies (for a detailed view, see Supplementary Figures 3, 4). In the Target-Mixture condition, 17 out of 20 stimuli exceeded 90% detection accuracies, while one stimulus was close to chance level at 56%, whereas two stimuli were almost collectively answered incorrectly, achieving an accuracy of only 15%. This agreement was even stronger in the Mixture-Target condition where 15 out of 20 stimuli achieved 100% accuracy, one stimulus achieved 95% accuracy, and the last four stimuli achieved an accuracy of less than 16%. When we excluded all stimuli that were consistently answered incorrectly (detection accuracy of 0%), the results remained identical for all conditions except for the lead vocals in the lowest level ratio. Here, accuracy in the Target-Mixture condition remained at 92% (+0%) and in the Mixture-Target condition from 86 to 91% (+5%), almost closing the gap between the two presentation orders that arose in the \u221215 dB condition (from an order effect of 6\u20133%), although we only conservatively screened out stimuli that were consistently answered incorrectly by all participants (for a detailed view, see Supplementary Table 5). For these reasons, a generalization of the results of the lead vocals at the lowest level ratios seems questionable, because accuracies here seem to be mainly driven by a few stimuli rather than the systematic change in level ratio.\n\nThe target category \u201cothers\u201d was most affected by a level decrease, declining by 18% in both presentation orders. Differences between presentation orders in this target category varied at different levels: \u22127% (\u22125 dB), \u22121% (\u221210 dB), and \u22128% (\u221215 dB). At the \u221210 dB condition revealed an ambiguous result, where the difference between the two presentation orders is only marginal. Considering all seven remaining conditions, which show clear effects, we interpret the present pattern of results as indication that the adjustment of the sound level ratios did not eliminate the order effect for the instruments and did not cause any robust order effect for the lead vocals.\n\nThe bass was slightly less affected by a decrease in level, achieving 18% in the Target-Mixture and 9% in the Mixture-Target conditions. With decreasing level, a consistently deteriorating detection of the mixture-target condition could be observed: \u221212% (\u22125 dB), \u221214% (\u221210 dB), and \u22123% (\u221215 dB).\n\nIn summary, by varying the target-to-accompaniment level ratio, we here observed effects of presentation order at different level ratios for a mixed category of instruments and for the bass instrument but no notable effect for the lead vocals. This once more confirmed the inherent salience of lead vocals in musical mixtures, which seems to be stable across sound levels.\n\nGeneral Discussion\n\nIn this study, we aimed to investigate ASA for musical instruments and singing voices and its modulation by selective auditory attention. Excerpts of popular music were presented in an instrument and singing voice detection task. Participants listened to a 2-s except either globally with a mixture preceding a target cue or selectively with a target cue preceding the mixture. We hypothesized that listeners\u2019 performance would be facilitated when a target cue is presented prior to the presentation of the mixture. In addition, we suspected a detection advantage of lead vocals relative to other instruments.\n\nIn line with our assumptions regarding the presentation order and previous studies (e.g., Bey and McAdams, 2002; Janata et al., 2002), detection performance was best when the target cue was presented before the mixture, highlighting the role of endogenous top-down processing to direct selective auditory attention. Accuracy worsened when listeners were presented the target after the mixture. This was the case for all target categories, apart from the lead vocals, which not only achieved the best detection accuracies among all target categories, but also showed no (or clearly much smaller) decreases of detection accuracies across the two orders of presentation. Although we initially assumed higher detection accuracy for the lead vocals, the latter finding exceeded our expectations about vocal salience in musical mixtures.\n\nIn a second and third experiment, we investigated how manipulations of acoustical features would affect lead vocal salience by eliminating differences between the target categories in relative sound level or release from spectral masking. However, contrary to our hypothesis, even when targets were completely unmasked from the mixture, or when the same sound levels were applied, lead vocals retained a unique role and robustly achieved the highest detection accuracies results across all manipulations, with a clear advantage over all other instruments. These findings support a unique role of the lead vocals in musical scene perception. More generally, this pattern of results is consistent with previous work in which singing voices have been shown to be perceptually privileged compared to other musical instruments by yielding faster processing (Agus et al., 2012) and more precise recognition rates (e.g., Suied et al., 2014; Isnard et al., 2019) as well as a stronger cortical representation (e.g., Levy et al., 2001) compared to other instruments. Our results demonstrate that auditory attention is drawn to the lead vocals in a mix, which complements knowledge about pre-attentive perceptual biases in musical scene analysis such as the high-voices superiority effect (Trainor et al., 2014).\n\nFrom a music production point of view, it may be argued that the facilitated detection of lead vocals could be a result of acoustic cues that arise from common tools such as compression and notch filtering, which allow the vocals to \u201ccome through\u201d and be perceived as the most prominent sound \u201cin front of\u201d the mixture. The results of Experiments 2 and 3 render this hypothesis unlikely, however. Despite complete unmasking of target categories in Experiment 2 and drastic changes of level in Experiment 3, the lead vocals remained the only target category that did not show an order effect and hence may be interpreted as the only target category with specific auditory salience.\n\nIn several recent studies, Weiss and colleagues provided evidence for a memory advantage of vocal melodies compared to melodies played by non-vocal musical instruments. Analyses of the recognition ratings for old and new melodies revealed that listeners more confidently and correctly recognized vocal compared to instrumental melodies (Weiss et al., 2012). It was further shown that the presentation of vocal melodies, as well as previously encountered melodies, was accompanied by an increase in pupil dilation (Weiss et al., 2016), often indirectly interpreted as indicator of raised engagement and recruitment of attentional resources. Our results directly highlight that those vocal melodies appear to act as a type of robust attentional attractors in musical mixtures, hence providing converging evidence for a privileged role of voices in ASA.\n\nThe lead vocal salience observed here could be due to a human specialization to process speech sounds. Therefore, lead vocals may have benefited from their speech features. Previous studies have demonstrated that phonological sounds, such as words and pseudo-words, are easier to detect than non-phonological complex sounds (Signoret et al., 2011). Therefore, an idea worth exploring is whether the advantage of lead vocals is still present when the vocal melody is sung with non-phonological speech, sung by humming, or played by an instrument. Another speech-like aspect that could make vocals more salient is the semantic content of the lyrics. Trying to grasp the meaning behind the lyrics could therefore draw attention to the vocals. Although test participants were not English native speakers, in Germany, it is common to listen to songs with English lyrics.\n\nAnother origin for the lead vocal salience could lie on a compositional level. In the used excerpts of popular music, lead voices certainly acted as the melodic center of the songs. The resulting melodic salience is known to dominate the perception of a musical scene (Ragert et al., 2014). A question which would be interesting to examine is whether the vocal salience found here would also be found if the main melody were played by another instrument and whether in this case the instrument would show enhanced auditory salience.\n\nConclusion\n\nWe used short excerpts of popular music in a detection task to investigate the influence of selective auditory attention in the perception of instruments and singing voices. Participants were either directed to a cued target vocal or instrument in a musical scene or had to listen globally in the scene before the cued target was presented. As expected, in the presentation order where no cue was given before the mixture and thus no additional support for endogenous top-down processing was provided, detection accuracy deteriorated. Whereas all instruments were affected by a change in the presentation order, the lead vocals were robustly detected and achieved the best detection accuracies among all target categories. To control for potential spectral and level effects, we filtered the target signals so that they were unmasked in a particular frequency band and eliminated sound levels differences between the targets. This facilitated instrument detection for the presentation order where the target was presented first, but not for the order where the mixture was presented first. These results indicate that the observed lead vocal salience is not based on acoustic cues in frequency region where the lead vocals are mixed at higher levels than the sum of accompanying instruments. It was further found that higher sound levels resulted in more similar scores across the presentations orders, but there remained clear order effect for all instruments except for the lead vocals, suggesting that the higher level-ratios of vocals are not the origin of the lead vocal salience. This confirms previous studies on vocal significance in ASA. Further research is needed to assess whether these features are based on its unique vocal qualities, semantic aspects of the vocal signal, or on the role of the center melody in musical mixtures.\n\nData Availability Statement\n\nThe raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.\n\nEthics Statement\n\nThe studies involving human participants were reviewed and approved by Kommission f\u00fcr Forschungsfolgenabsch\u00e4tzung und Ethik. The patients/participants provided their written informed consent to participate in this study.\n\nAuthor Contributions\n\nMB and KS designed the study. LP provided the stimuli. MB collected and analyzed the data and wrote a first draft of the manuscript. KS and LP revised the manuscript. All authors contributed to the article and approved the submitted version.\n\nFunding\n\nThis research was supported by a Freigeist Fellowship of the Volkswagen Foundation to KS.\n\nConflict of Interest\n\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\n\nPublisher\u2019s Note\n\nAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\n\nAcknowledgments\n\nWe thank Tency Music for allowing us to use excerpts from the musiclarity database for our experiments. We also express our gratitude to the reviewers whose constructive feedback improved the paper.\n\nSupplementary Material\n\nThe Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fpsyg.2021.769663/full#supplementary-material\n\nReferences\n\nEastgate, R., Picinali, L., Patel, H., and D'Cruz, M. (2016). 3d games for tuning and learning about hearing aids. Hear. J. 69, 30\u201332. doi: 10.1109/VR.2018.8446298 CrossRef Full Text | Google Scholar\n\nFox, J., and Weisberg, S. (2019). An R Companion to Applied Regression, 3rd Edn. Thousand Oaks CA: Sage Google Scholar\n\nHelmholtz, H. (1885). On the Sensations of Tone as a Physiological Basis for the Theory of Music. New York, NY: Dover Google Scholar\n\nHove, M. J., Marie, C., Bruce, I. C., and Trainor, L. J. (2014). Superior time perception for lower musical pitch explains why bass-ranged instruments lay down musical rhythms. Proc. Natl. Acad. Sci. U. S. A. 111, 10383\u201310388. doi: 10.1073/pnas.1402039111 PubMed Abstract | CrossRef Full Text | Google Scholar\n\nIsnard, V., Chastres, V., Viaud-Delmon, I., and Suied, C. (2019). The time course of auditory recognition measured with rapid sequences of short natural sounds. Sci. Rep. 9:8005. doi: 10.1038/s41598-019-43126-5 PubMed Abstract | CrossRef Full Text | Google Scholar\n\nMarie, C., Fujioka, T., Herrington, L., and Trainor, L. J. (2012). The high-voice superiority effect in polyphonic music is influenced by experience: A comparison of musicians who play soprano-range compared with bass-range instruments. Psychomusicology 22, 97\u2013104. doi: 10.1037/a0030858 CrossRef Full Text | Google Scholar\n\nR Core Team (2014). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. Available at: https://www.R-project.org/ Google Scholar\n\nSiedenburg, K., R\u00f6ttges, S., Wagener, K. C., and Hohmann, V. (2020). Can you hear Out the melody? Testing musical scene perception in young Normal-hearing and older hearing-impaired listeners. Trends Hear. 24:2331216520945826. doi: 10.1177/2331216520945826 PubMed Abstract | CrossRef Full Text | Google Scholar\n\nTrainor, L. J., Marie, C., Bruce, I. C., and Bidelman, G. M. (2014). Explaining the high voice superiority effect in polyphonic music: evidence from cortical evoked potentials and peripheral auditory models. Hear. Res. 308, 60\u201370. doi: 10.1016/j.heares.2013.07.014 PubMed Abstract | CrossRef Full Text | Google Scholar"}, {"url": "https://www.allmusic.com/artist/dan-auerbach-mn0000189446/credits", "page_content": "Credits Dan Auerbach Follow Artist +\n\nBlack Keys frontman who branched out on his own by adding singer/songwriter sensibilities to his trademark bluesy squall.\n\nRead Full Biography"}, {"url": "https://www.britannica.com/art/percussion-instrument", "page_content": "Other categories include scraped idiophones, comprising scrapers and cog rattles; split idiophones made of split hollow cane, including the Southeast Asian \u201ctuning fork\u201d idiophones and the chopstick; plucked idiophones, such as the jew\u2019s harp , mbira , and music box ; friction idiophones, including friction sticks, simple or combined, and musical glasses; and blown idiophones, such as the 19th-century \u00c4olsklavier and piano chanteur.\n\nShaken idiophones, or rattles , include vessels filled with rattling material, such as gourd, basketry, and hollow-ring rattles, as well as pellet bells; strung rattles, such as dancers\u2019 leg rattles or anklets; stick rattles, including the sistrum , originally a forked stick with crossbars on which rattling shells, etc., have been strung; pendant rattles with suspended rattling objects; and sliding rattles.\n\nIdiophones form a diverse and disparate group. Concussion instruments, consisting of two similar components struck together, include clappers, concussion stones, castanets , and cymbals . Percussion idiophones, instruments struck by a nonsonorous striker, form a large subgroup, including triangles and simple percussion sticks; percussion beams, such as the semanterion ; percussion disks and plaques, single and in sets; xylophones , lithophones (sonorous stones), and metallophones (sets of tuned metal bars); percussion tubes, such as stamping tubes, slit drums, and tubular chimes; and percussion vessels varying from struck gourds and pots to gongs , kettle gongs, steel drums , bells , and musical cups.\n\nMusical instruments in which the sound-producing medium is a vibrating membrane fall into four main groups: kettledrums and bowl-shaped drums; tubular drums\u2014whether cylindrical, barrel, conical, double conical, hourglass, goblet, or shallow\u2014and rattle drums, the membranes of which are set in motion by enclosed pellets or by knotted ends of a thong or cord; friction drums, with membranes caused to vibrate by friction; and mirlitons, whose membranes are set in motion by the sound of an instrument or the human voice. Strictly speaking, mirlitons are voice modifiers rather than true musical instruments inasmuch as they have no pitch of their own.\n\nGet a Britannica Premium subscription and gain access to exclusive content. Subscribe Now\n\nKettledrums and tubular drums occur in both tunable and nontunable forms; friction drums and mirlitons are not tunable. The membranes of the first two groups are either glued, nailed, lapped, or laced to the body, or shell; if they are glued or nailed, the pitch can be modified by exposure to heat. Lapped and laced heads are readily tunable by tightening the lacings or screws, and wooden wedges may be inserted between the shell and lacings to further increase the membrane\u2019s tension and thus raise the pitch. The membranes of such instruments and of friction drums are set in vibration by percussion, while those of mirlitons vibrate by impact of sound waves. In all groups the shell plays a subordinate acoustical part, acting as resonator only\u2014the greater the diameter of a head, the deeper its sound; and the greater its tension, the higher the pitch. In Western culture the only drums tuned to a definite pitch are kettledrums (the orchestral timpani).\n\nKettledrums and tubular drums may be struck with the hands, with beaters, or with both combined or with the knotted ends of a thong or cord. Beaters can be cylindrical, club-shaped, straight, curved, or angled, with or without knobs or padding, or may take the form of a switch or wire brush. Friction drums are sounded by rubbing the membrane with a piece of hide or by the more usual method of working an inserted friction stick or cord up and down or by rubbing the membrane with a player\u2019s wet fingers. Acoustically, they are subject to the same laws as other membranophones, but the speed of friction is an influencing factor. They occur in Africa, the Americas, Europe, Asia (India and Japan), and Hawaii. Mirlitons are sounded by directing against the membrane the vibrating air column of a voice, be it human (as in a kazoo) or instrumental (as when affixed, for example, to African xylophone resonators), or by holding the membrane against the player\u2019s vibrating vocal cords.\n\nIn addition to the four major categories of membranophones, a small group composed of ground drums and pot drums can also be distinguished. Ground drums, consisting in their simplest form of an animal skin stretched over the opening of a pit, are found in many parts of the world. The skin may also be held taut by several players, each beating it with a stick. These and similar ground drums are played by women in Africa and Australia, and in North America usually by men. By their very nature ground drums are nonportable; a similar type of instrument was made by stretching a skin over the opening of a gourd, clay pot, or other object. Among the Swazi of southern Africa such skins are not attached but held taut. Pot drums are found in Asia, Africa, and the Americas\u2014in Africa and the Americas often in connection with exorcism."}, {"url": "https://www.scaruffi.com/music/picbest.html", "page_content": "1) Captain Beefheart: Trout Mask Replica\n\nFrownland 1.39 The Dust Blows Forward 'N' The Dust Blows Back 1.53 Dachau Blues 2.21 Elle Guru 2.23 Hair Pie: Bake 1 4.57 Moonlight On Vermont 3.55 Pachuco Cadaver 4.37 Bill Corpse 1.47 Sweet Sweet Bulbs 1.47 Neon Meate Dream Of A Octafish 2.25 China Pig 3.56 My Human Gets Me Blues 2.42 Dali's Car 1.25 Hair Pie: Bake 2 2.23 Pena 2.31 Well 2.05 When Big Joan Sets Up 5.19 Fallin' Ditch 2.03 Sugar 'N' Spikes 2.29 Ant Man Bee 3.55 Orange Claw Hammer 3.35 Wild Life 3.07 She's Too Much For My Mirror 1.42 Hobo Chang Ba 2.01 The Blimp 2.04 Steal Softly Thru Snow 2.13 Old Fart At Play 1.54 Veteran's Day Poppy 4.30 First release: Straight, 1969. CD reissue: Reprise, 1990. Personnel and instruments:\n\nCaptain Beefheart (Donald Van Vliet): bass clarinet, harmonica, soprano & tenor Sax, vocals, musette\n\nDrumbo (John French): drums\n\nDouglas Moon: guitar\n\nMascara Snake (Victor Hayden): bass clarinet, vocals\n\nZoot Horn Rollo (Bill Harkleroad): flute, guitar\n\nAntennae Jimmy Semens (Jeff Cotton): steel guitar, vocals, winds\n\nMark Boston (Rockette Morton): bass\n\nand as guests:\n\nIan Underwood: alto sax\n\nDon Preston: piano\n\nRoy Estrada: bass\n\nArtie Tripp: drums\n\nFrank Zappa: vocals\n\n\n\n2) Robert Wyatt: Rock Bottom\n\nSea Song 6.31 A Last Straw 5.46 Little Red Riding Hood Hit The Road 7.38 Alifib 6.55 Alife 6.31 Little Red Robin Hood Hit The Road 6.08 First release: Virgin, 1974. CD reissue: Thirsty Ear, 1998. Personnel and instruments:\n\nRobert Wyatt: guitar, drums, keyboards, vocals\n\nRichard Sinclair & Hugh Hopper: bass\n\nLaurie Allan: drums\n\nGary Windo: clarinet & bass clarinet, tenor sax, winds, vocals\n\nFred Frith: piano, viola\n\nIvor Cutler: vocals, keyboards, concertina\n\nMongezi Feza: trumpet\n\nAlfreda Benge: vocals\n\nMike Oldfield: guitar\n\n\n\n3) Faust: Faust\n\nWhy Don't You Eat Carrots? 9.35 Meadow Meal 8.05 Miss Fortune (live) 16.36 First release: Polydor, 1971. CD reissue: Polygram, 2001. Personnel and instruments:\n\nWerner Diermaier: drums\n\nHans Joachim Irmler: organ\n\nJean Herv\u00e9 Peron: bass, guitar, trumpet, vocals\n\nRudolf Sossna: guitar\n\nGunther Wusthoff: keyboards, winds\n\n\n\n\n\n4) Velvet Underground: VU and Nico\n\nSunday Morning 2.54 I'm Waiting For The Man 4.37 Femme Fatale 2.37 Venus in Furs 5.10 Run Run Run 4.20 All Tomorrow's Parties 5.58 Heroin 7.10 There She Goes Again 2.38 I'll Be Your Mirror 2.12 The Black Angel's Death Song 3.12 European Son 7.46 First release: Verve, 1967. CD reissue: Polygram, 1996. Personnel and instruments:\n\nJohn Cale: bass, piano, guitar, keyboards, viola & electric viola\n\nNico: vocals\n\nLou Reed: acoustic & electric guitar, keyboards, vocals\n\nMaureen Tucker: bass, drums, percussions\n\nSterling Morrison: bass, guitar\n\n\n\n\n\n5) Doors: The Doors\n\nBreak on Through (To the Other Side) 2.25 Soul Kitchen 3.30 Crystal Ship 2.30 Twentieth Century Fox 2.30 Alabama Song (Whiskey Bar) 3.15 Light My Fire 6.50 Back Door Man 3.30 I Looked at You 2.18 End of the Night 2.49 Take It as It Comes 2.13 The End 11.35 First release: Elektra, 1967. CD reissue: id, 1988. Personnel and instruments:\n\nJim Morrison: vocals\n\nRay Manzarek: keyboards\n\nRobby Krieger: guitar\n\nJohn Densmore: drums\n\n\n\n6) Tim Buckley: Lorca\n\nLorca 9.53 Anonymous Proposition 7.43 I Had A Talk With My Woman 5.55 Driftin' 8.10 Nobody Walkin' 7.30 First release: Elektra, 1970. CD reissue: Asylum, 1992. Personnel and instruments:\n\nTim Buckley: guitars, vocals\n\nJohn Balkin: bass & electric bass, pipe organ\n\nLee Underwood: acoustic & electric guitar, keyboards, electric piano\n\nCarter C. C. Collins: congas\n\n\n\n7) Pere Ubu: The Modern Dance\n\nNon alignment pact 3.18 Modern dance 3.28 Laughing 4.35 Street waves 3.04 Chinese radiation 3.27 Life stinks 1.52 Real world 3.59 Over my head 3.48 Sentimental journey 6.05 Humour me 2.44 First release: Blank, 1978. CD reissue: Geffen, 1998. Personnel and instruments:\n\nDavid Thomas: percussion, vocals, musette\n\nTom Herman: guitar\n\nScott Krauss: drums\n\nTony Maimone: bass\n\nAllen Ravenstine: synthesizer, keyboards\n\n\n\n8) Royal Trux: Twin Infinitives\n\nSolid Gold Tooth 2.02 Ice Cream 3.38 Jet Pet 4.28 RTX-USA 2.22 Kool Down Wheels 2.18 Chances Are The Comets In Our Future 6.25 Yin Jim Versus The Vomit Creature 5.30 Osiris 3.51 (Edge Of The) Ape Oven 14.35 Florida Avenue Theme 1.05 Lick My Boots 4.19 Glitterbust 3.43 Funky Son 2.48 Ratcreeps 2.51 New York Avenue Bridge 3.42 First release: Drag City, 1990. CD reissue: id, 1994. Personnel and instruments:\n\nNeil Hagerty: guitar, vocals, percussions\n\nJennifer Herrema: organ, vocals, percussions\n\n\n\n9) John Fahey: Fare Forward Voyagers\n\nFirst release: Takoma, 1973, CD reissue: Shanachie, 1992. Personnel and instruments:\n\nJohn Fahey: guitar\n\nWhen the Fire and the Rose Are One (13:55) Thus Krishna On The Battlefield (6:36) Fare Forward Voyagers (23:42)\n\n10) Nico: Desert Shore\n\nJanitor of Lunacy 4.01 The Falconer 5.39 My Only Child 3.27 Le Petit Chevalier 1.12 Abschied 3.02 Afraid 3.27 Mutterlein 4.38 All That Is My Own 3.54 First release: Reprise, january 1971. CD reissue: Warner Bros, 1993. Personnel and instruments:\n\nNico: vocals, harmonium, marimba\n\nJohn Cale: vocals, bass, guitar, keyboards, multi instruments\n\nAdam Miller, Annagh & Vicki Wood: vocals\n\n\n\n\n\n11) Popol Vuh: Hosianna Mantra\n\nAh! 4.43 Kyrie 5.20 Hosianna - Mantra 10.15 Abschied 3.10 Segnung 6.00 Andacht 0.40 Nicht Hoch Im Himmel 6.17 Andacht 0.35 First release: Pilz, 1973. CD reissue: High Tide, 1994. Personnel and instruments:\n\nFlorian Fricke: piano, harpsichord\n\nKlaus Wiese: tambura\n\nConny Veit: guitar\n\nRobert Eliscu: oboe\n\nDjong Yun: vocals\n\nFritz Sonnleitner: violin\n\n\n\n12) Red Crayola: Parable Of Arable Land\n\nFree Form Freak-out 1.31 Hurricane Fighter Plane 3.31 Free Form Freak-out 2.23 Transparent Radiation 2.33 Free Form Freak-out 4.18 War Sucks 3.53 Free Form Freak-out 1.51 Pink Stainless Tail 3.13 Free Form Freak-out 3.02 Parable Of Arable Land 3.01 Free Form Freak-out 4.11 Former Reflections Enduring Doubt 4.59 First release: International Artist, 1967. CD reissue: Spalax, 1994. Personnel and instruments:\n\nMayo Thompson: vocals, guitar?\n\nRick Barthelme: bass, drums?\n\nSteve Cunningham: guitar?\n\nRick McCollum: keyboards?\n\nTom Smith: drums ?\n\n\n\n13) Klaus Schulze: Irrlicht\n\nEbene 23.23 Gewitter (energy rise - energy collaps) 5.40 Exil Sils Maria 21.26 First release: Brain, 1972. CD reissue: Spalax, 1995. Personnel and instruments:\n\nKlaus Schulze: organ, synthesizer, guitar, keyboards, percussions, vocals\n\nColloquium Musica Orchestra\n\n\n\n14) Nick Cave: The Good Son\n\nFoi Na Cruz 5.39 The Good Son 6.01 Sorrow's Child 4.36 The Weeping Song 4.21 The Ship Song 5.14 The Hammer Song 4.16 Lament 4.51 The Witness Song 5.57 Lucy 4.17 First release: Mute, 1990. Personnel and instruments:\n\nNick Cave: vocals, piano, organ, harmonica\n\nMick Harvey: guitars, bass, percussions, background vocals, vibraphone, string arrangements\n\nBlixa Bargeld: guitar, background vocals\n\nKid Congo Powers: guitar\n\nClovis Trindade, Rubinho: vocals\n\nThomas Wydler: drums, percussion\n\nRoland Wolf: piano\n\nAlexandre Ramirez, Altamir T\u00e9a Bueno Salinas, Helena Akiku Imasato, L\u00e9a Kalil Sadi: violin\n\nAkira Terakazi, Glauco Masahiru Imasato: viola\n\nBraulio Marques Lima, Cristina Manescu: cello\n\nBill McGee: string arrangements\n\n\n\n15) Lisa Germano: Geek The Girl\n\nMy Secret Reason 4.32 Trouble 2.20 Geek The Girl 3.40 Just Geek 2.43 Cry Wolf 4.59 ...A Psychopath 4.36 Sexy Little Girl Princess 3.38 Phantom Love 3.21 Cancer Of Everything 4.00 A Guy Like You 3.18 ...Of Love And Colors 3.54 Stars 2.31 First release: 4AD, 1994. Personnel and instruments:\n\nLisa Germano: vocals\n\nKenny Aronoff: drums\n\nMalcolm Burn: dulcimer, piano. guitar, drums\n\n\n\n16) Morphine: Good\n\nGood 2.36 The Saddest Song 2.50 Claire 3.07 Have A Lucky Day 3.24 You Speak My Language 3.25 You Look Like Rain 3.42 Do Not Go Quietly Unto Your Grave 3.21 Lisa 0.43 The Only One 2.42 Test-Tube Baby/Shoot'm Down 3.11 The Other Side 3.50 I Know You (Part I) 2.17 I Know You (Part II) 2.45 First release: Accurate, 1992. Personnel and instruments:\n\nMark Sandman: organ, bass, guitar, vocals\n\nDana Colley: saxes, vocals\n\nBilly Conway & Jerry Deupree: drums\n\n\n\n17) Bob Dylan: Blonde On Blonde\n\nRainy day women nos 12 & 15 4.33 Pledging my time 3.42 Visions of Johanna 7.27 One of us must know (sooner of later) 4.53 I want you 3.06 Stuck inside of mobile with the Memphis blues again 7.04 Leopard-skin-pill-box hat 3.50 Just like a woman 4.39 Most likely you go tour way and I'll go mine 3.22 Temporary like Achilles 5.03 Absolutely sweet Marie 4.46 4th Time around 4.26 Obviously 5 believers 3.30 Sad eyed lady of the lowlands 11.19 First release: CBS, 1966. CD reissue: Columbia, 1992. Personnel and instruments:\n\nBob Dylan: vocals, harmonica, piano, guitar, keyboards\n\nAl Kooper: organ, guitar, horn, keyboards\n\nRobbie Robertson & Wayne Moss: guitar, vocals\n\nJoe South & Jerry Kennedy: guitar\n\nRick Danko: bass, violin, vocals\n\nBill Atkins: keyboards\n\nWayne Butler: trombone\n\nKenneth Buttrey & Sanford Konikoff: drums\n\nPaul Griffin: piano\n\nGarth Hudson: keyboards, sax\n\nRichard Manuel: drums, keyboards, vocals\n\nHargus Robbins: piano, keyboards\n\nHenry Strzelecki: bass\n\nCharlie McCoy: bass, guitar, harmonica, trumpet\n\n\n\n18) Neu!: Neu!\n\nHallogallo 10.07 Sanderouwgebut 4.51 Weinensee 6.46 In Gl\ufffdck 6.53 Negativland 9.47 Lieber Honig 7.18 First release: UA, 1973. CD reissue: Astralwerks, 2001. Personnel and instruments:\n\nMichael Rother: bass, guitar, keyboards\n\nKlaus Dinger: bass, guitar, keyboards, vocals\n\n\n\n19) Foetus: Nail\n\nTheme From Pigdome Come 1.52 The Throne Of Agony 5.18 ! 0.04 Pigswill 6.13 Descent Into The Inferno 6.17 Enter The Exterminator 4.43 DI-1-9026 4.40 The Overture From Pigdome Come 3.01 Private War 1.06 Anything (Viva!) 6.50 First release: SomeBizarre, 1985. CD reissue: Homestead, 1995. Personnel and instruments:\n\nFoetus (Jim Thirlwell): multi instruments, vocals\n\n\n\n20) Suicide: Suicide\n\nGhost Rider 2.34 Rocket U.S.A. 4.16 Cheree 3.42 Johnny 2.10 Girl 4.05 Frankie Teardrop 10.27 Che 4.52 First release: Bronze, 1977. Second release, with additional tracks: ZE, 1980. CD reissue: Demon, 1995. Personnel and instruments:\n\nAlan Vega: vocals\n\nMartin Rev: synthesizer, multi instruments, electronics\n\n\n\n21) Van Morrison: Astral Weeks\n\nAstral weeks 7.00 Beside you 5.10 Sweet thing 4.10 Cyprus Avenue 6.50 The way young lovers do 3.10 Madame George 9.25 Ballerina 7.00 Slim slow rider 3.20 First release: Warner Bros, 1968. CD reissue: id, 1987. Personnel and instruments:\n\nVan Morrison: guitar, vocals, sax, keyboards\n\nRichard Davis: bass\n\nJay Berliner: guitar\n\nConnie Kay: drums\n\nWarren Smith: percussion, vibraphone\n\nJohn Payne: flute, sax soprano\n\n\n\n\n\n22) Residents: Not Available\n\nEdweena 9.25 Making of a Soul 9.56 Ship's A'going Down 6.30 Never Known Questions 7.05 Epilogue 2.25 First release: Ralph, 1978. CD reissue: Mute, 2000. Personnel and instruments (not surely identified):\n\nHomer Flynn & Hardy Fox: multi instruments\n\nPhilip Lithnam: guitar\n\n\n\n23) Pop Group: Y\n\nThief Of Fire 4.35 Snowgirl 3.21 Blood Money 2.57 We Are Time 6.29 Savage Sea 3.02 Words Disobey Me 3.26 Dont Call Me Pain 5.35 The Boys From Brazil 4.16 Don't Sell Your Dreams 6.40 First release: Radar, 1979. CD reissue: id, 1996. Personnel and instruments:\n\nMark Stewart: guitar, vocals\n\nGareth Sager: sax\n\nSimon Underwood: bass\n\nJohn Waddington: guitar\n\nBruce Smith: drums\n\n\n\n24) Vampire Rodents: Lullaby Land\n\nTrilobite 4.43 Catacomb 4.03 Crib Death 4.22 Dogchild 3.24 Gargoyles 3.51 Grace 4.34 Tremulous 0.53 Glow Worm 2.23 Lullaby Land 3.04 Dervish 3.55 Scavenger 5.06 Exuviate 5.11 Akrotiri 4.10 Toten Faschist 2.25 Nosedive 3.07 Bosch Erotique 2.17 Hubba Hubba 1.46 Cartouche 1.52 Awaken 1.43 Raga Rodentia 5.42 Passage 3.39 First release: Re-Constriction, 1993. Personnel and instruments:\n\nDaniel Vahnke: vocals, guitar, orchestration, electronics\n\nVictor Wulf: synthesizers\n\nAndrea Akastia: violin, cello\n\nJing Laoshu: percussion\n\nMarc Bennett: guitar\n\nThe Sandman & Jared Hendrickson: vocals\n\n\n\n25) Bruce Springsteen: The River First release: Columbia, 1980: CD reissue: id, 1995. Personnel and instruments:\n\nBruce Springsteen: bass, guitar, harmonica, vocals\n\nRoy Bittan: piano, keyboards\n\nDanny Federici: organ, vocals\n\nClarence Clemons: sax, vocals\n\nGarry Tallent: bass, horn\n\nSteve Van Zandt: guitar\n\nMax Weinberg: drums\n\nHoward Kaylan, Mark Volman: vocals\n\n25) Hash Jar Tempo: Well Oiled First release: Drunken Fish, 1997. Personnel and instruments:\n\nRoy Montgomery, John & Michael Gibbons: guitar\n\nJoe Culver: drums\n\nClint Takeda: bass\n\n\n\n25+) Butthole Surfers: Psychic Powerless\n\nFirst release: Touch&Go, 1985. CD reissue: id, 1990. Personnel and instruments:\n\nGibby Haynes: vocals\n\nPaul Leary: guitar\n\nKing Coffey: drums\n\n...and others musicians, not identified yet!\n\n\n\n25+) Soft Machine: 3 First release: Columbia, 1970. CD reissue: id, 1991. Personnel and instruments:\n\nMike Ratledge: piano, organ\n\nRobert Wyatt: drums, vocals\n\nHugh Hopper: bass\n\nElton Dean: alto sax, saxello\n\nand, as guests:\n\nRab Spall: violin\n\nLyn Dobson: flute, sax soprano\n\nNick Evans: trombone\n\nJimmy Hastings: flute, bass clarinet\n\n\n\n25+) Type O Negative: Slow Deep And Hard\n\nFirst release: Roadrunner, 1991. Personnel and instruments:\n\nKenny Hickey: guitar, vocals\n\nJosh Silver: keyboards, vocals\n\nPete Steele: bass, vocals\n\nSal Abruscato: drums\n\n\n\n25+) Frank Zappa: Uncle Meat\n\nFirst release: Bizarre, 1969. CD reissue: Barking Pumpkin, 1986 (with excerpts from movie). Personnel and instruments:\n\nFrank Zappa: guitar, vocals, percussion, keyboards\n\nRay Collins, Nelcy Walker: vocals\n\nJimmy Carl Black, Billy Mundi: drums\n\nRoy Estrada: electric bass, vocals\n\nDon Preston: electric piano\n\nBunk Gardner: piccolo, flute, clarinet, bass clarinet, alto, soprano & tenor sax, bassoon\n\nIan Underwood: organ, piano, harpsichord, celeste, alto & baritone sax, flute, clarinet\n\nArtie Tripp: drums, marimba, vibraphone, tympani, several percussion\n\nEuclid James Sherwood: tenor sax, tambourine\n\nRuth Komanoff: marimba, vibraphone\n\n\n\n25+) Minutemen: Double Nickels On The Dime First release: SST, 1984. CD reissue: id, 1995. Personnel and instruments:\n\nGeorge Hurley: percussion, drums\n\nMike Watt: bass, vocals\n\nDennes Boon: guitar, vocals\n\n\n\n25+) Fugazi: Repeater\n\nFirst release: Dischord, 1990. Personnel and instruments:\n\nIan MacKaye, Guy Picciotto : guitar, vocals\n\nBrendan Canty: drums\n\nJoe Lally: bass\n\n\n\n\n\n25+) Mercury Rev: Yerself Is Steam\n\nFirst release: Beggars Banquet, 1991. CD reissue: Columbia, 1992. Personnel and instruments:\n\nJimmy Chambers: drums\n\nJonathan Donahue: guitar, vocals\n\nDave Fridmann: bass\n\nSean Mackiowiak (Grasshopper): guitar\n\nSuzanne Thorpe: flute\n\nDavid Baker:vocals\n\nand, as guest:\n\nC. Gavazzi: trumpet\n\n\n\n25+) Gun Club: Fire Of Love\n\nFirst release: Slash, 1981. CD reissue: id, 1993. Personnel and instruments:\n\nJerry Lee Pierce: lead & background vocals, slide guitar\n\nWard Dotson: guitar & slide guitar, background vocals\n\nRob Ritter: bass\n\nTerry Graham: drums\n\nTito Larriva: violin\n\nChris D., Lois Graham: background vocals\n\n25+) Red House Painters: Down Colorful Hill\n\nFirst release: 4AD, 1992. Personnel and instruments:\n\nMark Kozelek: vocals\n\nGordon Mack: guitar\n\nAnthony Koutsos: drums\n\nJerry Vessel: bass\n\n\n\n25+) Slint: Spiderland\n\nFirst release: Touch&Go, 1991. Personnel and instruments:\n\nBritt Walford: drums\n\nBrian MacMahan: vocals, electric guitar\n\nDavid Pajo: guitar\n\nTodd Brashear: bass\n\n\n\n25+) My Bloody Valentine: Loveless\n\nFirst release: Sire, 1991. Personnel and instruments:\n\nBilinda Butcher: guitar, vocals\n\nKevin Shields: guitar, vocals, sampling\n\nColm O'Ciosoig: drums, sampling\n\nDeb Googe: bass\n\n\n\n25+) Velvet Underground: White Light White Heat\n\nFirst release: Verve, 1967. CD reissue: Polygram, 1996. Personnel and instruments:\n\nLou Reed: vocals, guitar, piano\n\nJohn Cale: vocals, electric viola, organ, bass, multi instruments\n\nMaureen Tucker: drums, percussion\n\nSterling Morrison: vocals, guitar, bass\n\n\n\n25+) Captain Beefheart: Safe As Milk\n\nFirst release: Buddah, 1967. CD reissues: Castle, 1988; Buddah, 1999 (with 7 bonus tracks). Personnel and instruments:\n\nCaptain Beefheart (Donald Van Vliet): vocals, guitar, marimba, harmonica\n\nTaj Mahal, Milt Holland: percussion\n\nRy Cooder: bass, guitar\n\nJohn French: drums\n\nJerry Handley: bass\n\nAntennae Jimmy Semens, Alex St. Clair, Russ Titelman: guitar\n\n\n\n\n\n25) Husker Du: Zen Arcade\n\nSomething I Learned Today - 1:58 Broken Home, Broken Heart - 2:01 Never Talking to You Again - 1:39 Chartered Trips - 3:33 Dreams Reoccurring - 1:40 Indecision Time - 2:07 Hare Krsna - 3:33 Beyond the Threshold - 1:35 Pride - 1:45 I'll Never Forget You - 2:06 The Biggest Lie - 1:58 What's Going On? - 4:23 Masochism World - 2:43 Standing by the Sea - 3:12 Somewhere - 2:30 One Step at a Time - :45 Pink Turns to Blue - 2:39 Newest Industry - 3:02 Monday Will Never Be the Same - 1:10 Whatever - 3:50 The Tooth Fairy and the Princess - 2:43 Turn on the News - 4:21 Reoccurring Dreams - 13:47 First release: SST, 1984. CD reissue: ???, ???. Personnel and instruments:\n\nGrant Hart: drums, vocals\n\nBob Mould: guitar, vocals Greg Norton: bass\n\n\n\n25) Third Ear Band: Thrid Ear Band\n\nFirst release: Harvest, 1970. CD reissue: ???, ???. Personnel and instruments:\n\nGlen Sweeney: percussions\n\nPaul Minns: oboe, recorder\n\nRichard Coff: violin, viola\n\nUrsula Smith: cello\n\n"}]}